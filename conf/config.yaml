defaults:
  - _self_

# Model configuration
reynolds_number: 5000
alpha_evm: 0.05  # Initial value, will be updated during training
alpha_boundary: 10.0
alpha_equation: 1.0
alpha_evm_constraint: 1.0

# 6-stage training configuration (integrated from ev-NSFnet)
training_stages:
  - stage_name: "Stage 1 - Initial"
    alpha_evm: 0.05
    epochs: 500000
    learning_rate: 0.001
  - stage_name: "Stage 2 - Reduce EVM"
    alpha_evm: 0.03
    epochs: 500000
    learning_rate: 0.0002
  - stage_name: "Stage 3 - Fine-tune"
    alpha_evm: 0.01
    epochs: 500000
    learning_rate: 0.00004
  - stage_name: "Stage 4 - Precision"
    alpha_evm: 0.005
    epochs: 500000
    learning_rate: 0.00001
  - stage_name: "Stage 5 - Final adjust"
    alpha_evm: 0.002
    epochs: 500000
    learning_rate: 0.000002
  - stage_name: "Stage 6 - Converge"
    alpha_evm: 0.002
    epochs: 500000
    learning_rate: 0.000002

# Network architectures
main_net:
  nr_layers: 6
  layer_size: 80
  activation_fn: "tanh"

evm_net:
  nr_layers: 6  # Updated to match ev-NSFnet
  layer_size: 40
  activation_fn: "tanh"

# Data configuration
data_dir: "./data"
num_interior_points: 120000  # Full batch training
num_boundary_points: 4000    # Increased for better boundary handling

# Training configuration
optimizer:
  lr: 1e-3
  weight_decay: 0.0

scheduler:
  enabled: true  # Enable for 6-stage training
  gamma: 0.95

# Logging and checkpointing
log_freq: 100
checkpoint_freq: 2000  # More frequent checkpoints
checkpoint_dir: "./checkpoints"
checkpoint_path: null

# Distributed training
distributed:
  backend: "nccl"

# Hydra configuration
hydra:
  run:
    dir: ./outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}
  job:
    chdir: false