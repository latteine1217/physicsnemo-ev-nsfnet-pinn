# Copyright (c) 2025 NVIDIA Corporation. All Rights Reserved.
import os
import torch
import hydra
from omegaconf import DictConfig
from physicsnemo.distributed import DistributedManager
from physicsnemo.utils.loggers import get_logger
from physicsnemo.trainer import Trainer
from physicsnemo.optimizers import get_optimizer
from physicsnemo.schedulers import get_scheduler

from physicsnemo_solver import PhysicsNeMoPINNSolver


class PhysicsNeMoPINNTrainer(Trainer):
    """
    PhysicsNeMo æ¨™æº– PINN è¨“ç·´å™¨
    
    å¯¦ä½œ 6 éšæ®µæ¼¸é€²å¼è¨“ç·´ç­–ç•¥ï¼Œç”¨æ–¼ EV-NSFnet æ¨¡å‹
    """
    
    def __init__(self, cfg: DictConfig):
        super().__init__(cfg)
        
        # ç²å–æ—¥èªŒå™¨
        self.logger = get_logger(__name__)
        
        # åˆå§‹åŒ–åˆ†æ•£å¼ç’°å¢ƒ
        self.dist = DistributedManager()
        
        # åˆå§‹åŒ–æ±‚è§£å™¨
        self.solver = PhysicsNeMoPINNSolver(cfg)
        
        # è¨­å®šå„ªåŒ–å™¨å’Œæ’ç¨‹å™¨
        self._setup_optimizer_and_scheduler()
        
        # è¨“ç·´ç‹€æ…‹
        self.current_stage = 0
        self.stage_epoch = 0
        self.total_epoch = 0
        
        # ç²å–è¨“ç·´éšæ®µé…ç½®
        self.training_stages = cfg.training_stages
        
        self.logger.info(
            f"PhysicsNeMo PINN è¨“ç·´å™¨åˆå§‹åŒ–å®Œæˆ - "
            f"ç¸½éšæ®µæ•¸: {len(self.training_stages)}"
        )
    
    def _setup_optimizer_and_scheduler(self):
        """è¨­å®šå„ªåŒ–å™¨å’Œå­¸ç¿’ç‡æ’ç¨‹å™¨"""
        
        # ä½¿ç”¨ PhysicsNeMo å„ªåŒ–å™¨
        self.optimizer = get_optimizer(
            model_parameters=self.solver.model.parameters(),
            optimizer_config=self.cfg.optimizer
        )
        
        # ä½¿ç”¨ PhysicsNeMo æ’ç¨‹å™¨
        if self.cfg.scheduler.enabled:
            self.scheduler = get_scheduler(
                optimizer=self.optimizer,
                scheduler_config=self.cfg.scheduler
            )
        else:
            self.scheduler = None
    
    def training_step(self, batch: dict) -> dict:
        """PhysicsNeMo æ¨™æº–è¨“ç·´æ­¥é©Ÿ"""
        
        # å‰å‘å‚³æ’­ä¸¦è¨ˆç®—æå¤±
        losses = self.solver.training_step(batch)
        
        # åå‘å‚³æ’­
        losses["total_loss"].backward()
        
        return losses
    
    def validation_step(self, batch: dict) -> dict:
        """PhysicsNeMo æ¨™æº–é©—è­‰æ­¥é©Ÿ"""
        
        losses = self.solver.validation_step(batch)
        
        return losses
    
    def configure_optimizers(self):
        """é…ç½®å„ªåŒ–å™¨ (PhysicsNeMo æ¨™æº–ä»‹é¢)"""
        
        if self.scheduler is not None:
            return {
                "optimizer": self.optimizer,
                "lr_scheduler": {
                    "scheduler": self.scheduler,
                    "monitor": "total_loss"
                }
            }
        else:
            return {"optimizer": self.optimizer}
    
    def train_single_stage(self, stage_idx: int):
        """è¨“ç·´å–®ä¸€éšæ®µ"""
        
        stage_config = self.training_stages[stage_idx]
        stage_name = stage_config["stage_name"]
        epochs = stage_config["epochs"]
        alpha_evm = stage_config["alpha_evm"]
        learning_rate = stage_config["learning_rate"]
        
        self.logger.info(f"é–‹å§‹ {stage_name} - Alpha_EVM={alpha_evm}, LR={learning_rate}")
        
        # è¨­å®šéšæ®µåƒæ•¸
        self.solver.set_training_stage(stage_name, alpha_evm)
        
        # æ›´æ–°å­¸ç¿’ç‡
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = learning_rate
        
        # ç²å–è¨“ç·´è³‡æ–™
        interior_data = self.solver.dataset.get_interior_data()
        boundary_data = self.solver.dataset.get_boundary_data()
        
        # éšæ®µè¨“ç·´è¿´åœˆ
        for epoch in range(epochs):
            self.solver.model.train()
            
            # åˆä½µå…§éƒ¨å’Œé‚Šç•Œè³‡æ–™
            combined_data = {**interior_data, **boundary_data}
            
            # è¨“ç·´æ­¥é©Ÿ
            self.optimizer.zero_grad()
            losses = self.training_step(combined_data)
            self.optimizer.step()
            
            # æ›´æ–°å…¨åŸŸè¨ˆæ•¸å™¨
            self.stage_epoch = epoch
            self.total_epoch += 1
            
            # è¨˜éŒ„æå¤±
            if epoch % self.cfg.log_freq == 0 and self.dist.rank == 0:
                self.logger.info(
                    f"{stage_name} Epoch {epoch}/{epochs} - "
                    f"Total Loss: {losses['total_loss'].item():.6f}, "
                    f"NS Loss: {losses['ns_loss'].item():.6f}, "
                    f"EVM Loss: {losses['evm_loss'].item():.6f}, "
                    f"BC Loss: {losses['boundary_loss'].item():.6f}"
                )
            
            # ä¿å­˜æª¢æŸ¥é»
            if epoch % self.cfg.checkpoint_freq == 0 and self.dist.rank == 0:
                self._save_checkpoint(stage_idx, epoch)
            
            # å­¸ç¿’ç‡æ’ç¨‹
            if self.scheduler is not None:
                self.scheduler.step()
        
        self.logger.info(f"å®Œæˆ {stage_name}")
    
    def train_all_stages(self):
        """åŸ·è¡Œå®Œæ•´çš„ 6 éšæ®µè¨“ç·´"""
        
        self.logger.info("é–‹å§‹ PhysicsNeMo EV-NSFnet 6 éšæ®µè¨“ç·´")
        
        for stage_idx in range(len(self.training_stages)):
            
            # EVM ç¶²è·¯å‡çµ/è§£å‡ç­–ç•¥ (æ¯å…©å€‹éšæ®µåˆ‡æ›)
            if stage_idx % 2 == 0:
                self.solver.freeze_evm_network()
            else:
                self.solver.unfreeze_evm_network()
            
            # è¨“ç·´ç•¶å‰éšæ®µ
            self.train_single_stage(stage_idx)
            
            # éšæ®µé–“é©—è­‰
            if hasattr(self.solver.dataset, 'reference_data') and self.solver.dataset.reference_data:
                self._validate_stage(stage_idx)
        
        self.logger.info("å®Œæˆæ‰€æœ‰ 6 éšæ®µè¨“ç·´ï¼")
    
    def _validate_stage(self, stage_idx: int):
        """éšæ®µé©—è­‰"""
        
        ref_data = self.solver.dataset.get_reference_data()
        
        if ref_data:
            errors = self.solver.evaluate(
                ref_data["x"], ref_data["y"],
                ref_data["u"], ref_data["v"], ref_data["p"]
            )
            
            if self.dist.rank == 0:
                stage_name = self.training_stages[stage_idx]["stage_name"]
                self.logger.info(
                    f"{stage_name} é©—è­‰çµæœ - "
                    f"U èª¤å·®: {errors['error_u']:.3f}%, "
                    f"V èª¤å·®: {errors['error_v']:.3f}%, "
                    f"P èª¤å·®: {errors['error_p']:.3f}%"
                )
    
    def _save_checkpoint(self, stage_idx: int, epoch: int):
        """ä¿å­˜æª¢æŸ¥é»"""
        
        checkpoint_path = os.path.join(
            self.cfg.checkpoint_dir,
            f"model_stage_{stage_idx}_epoch_{epoch}.pth"
        )
        
        checkpoint = {
            'model_state_dict': self.solver.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'stage_idx': stage_idx,
            'epoch': epoch,
            'total_epoch': self.total_epoch,
            'cfg': self.cfg
        }
        
        if self.scheduler is not None:
            checkpoint['scheduler_state_dict'] = self.scheduler.state_dict()
        
        torch.save(checkpoint, checkpoint_path)
        self.logger.info(f"æª¢æŸ¥é»å·²ä¿å­˜: {checkpoint_path}")


@hydra.main(version_base="1.3", config_path="conf", config_name="config")
def main(cfg: DictConfig) -> None:
    """PhysicsNeMo PINN ä¸»è¨“ç·´å‡½æ•¸"""
    
    # åˆå§‹åŒ–åˆ†æ•£å¼ç’°å¢ƒ
    DistributedManager.initialize()
    dist = DistributedManager()
    
    # ç²å–æ—¥èªŒå™¨
    logger = get_logger(__name__)
    
    if dist.rank == 0:
        logger.info("ğŸŒŠ å•Ÿå‹• PhysicsNeMo EV-NSFnet PINN è¨“ç·´")
        logger.info(f"é›·è«¾æ•¸: {cfg.reynolds_number}")
        logger.info(f"GPU æ•¸é‡: {dist.world_size}")
    
    # å‰µå»ºè¼¸å‡ºç›®éŒ„
    os.makedirs(cfg.checkpoint_dir, exist_ok=True)
    
    try:
        # åˆå§‹åŒ–è¨“ç·´å™¨
        trainer = PhysicsNeMoPINNTrainer(cfg)
        
        # åŸ·è¡Œå®Œæ•´è¨“ç·´
        trainer.train_all_stages()
        
        if dist.rank == 0:
            logger.info("ğŸ‰ PhysicsNeMo EV-NSFnet PINN è¨“ç·´å®Œæˆï¼")
            
    except Exception as e:
        logger.error(f"è¨“ç·´éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        raise


if __name__ == "__main__":
    main()