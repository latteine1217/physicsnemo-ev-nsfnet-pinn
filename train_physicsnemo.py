# SPDX-FileCopyrightText: Copyright (c) 2024 LDC-PINNs
# SPDX-License-Identifier: Apache-2.0
#
# ==============================================================================
# LDC-PINNs: Physics-Informed Neural Networks for Lid-Driven Cavity Flow
# åŸºæ–¼ NVIDIA PhysicsNeMo æ¡†æ¶å¯¦ç¾
# ==============================================================================

import os
import hydra
import matplotlib.pyplot as plt
import numpy as np
import torch
from omegaconf import DictConfig

# æœ¬å°ˆæ¡ˆçš„ç‰©ç†æ¨¡çµ„
from src.physics.equations import PhysicsEquations, EntropyViscosityMethod
from src.models.activations import TSAActivation, get_activation_function, compute_activation_regularization

# PyTorch optimization
from torch.optim import Adam, LBFGS
from torch.optim import lr_scheduler

# ç°¡å–®çš„æ—¥èªŒæ¨¡çµ„
import logging

# è¨­å®šæ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def create_output_directory(cfg: DictConfig) -> str:
    """å‰µå»ºè¼¸å‡ºç›®éŒ„"""
    output_dir = cfg.outputs.save_dir
    os.makedirs(output_dir, exist_ok=True)
    return output_dir


def setup_model(cfg: DictConfig, device: torch.device) -> torch.nn.Module:
    """è¨­ç½®ç¥ç¶“ç¶²è·¯æ¨¡å‹"""
    class FullyConnectedNetwork(torch.nn.Module):
        def __init__(self, in_features, out_features, num_layers, layer_size):
            super().__init__()
            layers = []
            prev_size = in_features
            
            for _ in range(num_layers):
                layers.append(torch.nn.Linear(prev_size, layer_size))
                
                # ä½¿ç”¨é€²éšæ¿€æ´»å‡½æ•¸ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
                if cfg.model.get('advanced_activation', {}).get('enabled', False):
                    activation_config = cfg.model.advanced_activation.copy()
                    activation_config['num_neurons'] = layer_size
                    activation = get_activation_function(activation_config)
                else:
                    activation = torch.nn.SiLU()
                
                layers.append(activation)
                prev_size = layer_size
            
            # è¼¸å‡ºå±¤
            layers.append(torch.nn.Linear(prev_size, out_features))
            
            self.network = torch.nn.Sequential(*layers)
            
            # Xavieråˆå§‹åŒ–
            self.apply(self._init_weights)
        
        def _init_weights(self, module):
            if isinstance(module, torch.nn.Linear):
                torch.nn.init.xavier_normal_(module.weight)
                if module.bias is not None:
                    torch.nn.init.zeros_(module.bias)
        
        def forward(self, x):
            return self.network(x)
    
    # ä¸»ç¶²è·¯ (u, v, p)
    main_network = FullyConnectedNetwork(
        in_features=cfg.model.in_features,
        out_features=cfg.model.out_features,
        num_layers=cfg.model.num_layers,
        layer_size=cfg.model.layer_size
    ).to(device)
    
    return main_network


def setup_physics(cfg: DictConfig, device: torch.device) -> PhysicsInformer:
    """è¨­ç½®ç‰©ç†æ–¹ç¨‹å¼å’ŒPhysicsInformer"""
    # å‰µå»ºNavier-Stokesæ–¹ç¨‹å¼
    ns = NavierStokes(
        nu=cfg.physics.nu,
        rho=cfg.physics.rho,
        dim=cfg.physics.dim,
        time=cfg.physics.time
    )
    
    # å‰µå»ºPhysicsInformer
    phy_inf = PhysicsInformer(
        required_outputs=["continuity", "momentum_x", "momentum_y"],
        equations=ns,
        grad_method="autodiff",
        device=device,
    )
    
    return phy_inf


def setup_geometry_dataloaders(cfg: DictConfig, device: torch.device):
    """è¨­ç½®å¹¾ä½•å’Œè³‡æ–™è¼‰å…¥å™¨"""
    # å‰µå»ºå¹¾ä½•å°è±¡ (æ­£æ–¹å½¢cavity)
    bounds = cfg.geometry.domain.bounds
    rec = Rectangle(
        (bounds[0][0], bounds[0][1]),  # lower left
        (bounds[1][0], bounds[1][1])   # upper right  
    )
    
    # é‚Šç•Œæ¢ä»¶è³‡æ–™è¼‰å…¥å™¨
    bc_dataloader = GeometryDatapipe(
        geom_objects=[rec],
        batch_size=cfg.geometry.sampling.batch_size,
        num_points=cfg.geometry.sampling.boundary_points,
        sample_type="surface",
        device=device,
        num_workers=cfg.geometry.sampling.num_workers,
        requested_vars=["x", "y"],
    )
    
    # å…§éƒ¨é»è³‡æ–™è¼‰å…¥å™¨
    interior_dataloader = GeometryDatapipe(
        geom_objects=[rec],
        batch_size=cfg.geometry.sampling.batch_size,
        num_points=cfg.geometry.sampling.interior_points,
        sample_type="volume",
        device=device,
        num_workers=cfg.geometry.sampling.num_workers,
        requested_vars=["x", "y", "sdf"],
    )
    
    return bc_dataloader, interior_dataloader, rec


def setup_optimizer_and_scheduler(cfg: DictConfig, model: torch.nn.Module):
    """è¨­ç½®å„ªåŒ–å™¨å’Œå­¸ç¿’ç‡èª¿åº¦å™¨"""
    optimizer = Adam(
        model.parameters(), 
        lr=cfg.training.learning_rate,
        weight_decay=cfg.training.weight_decay
    )
    
    scheduler = lr_scheduler.LambdaLR(
        optimizer, 
        lr_lambda=lambda step: cfg.training.scheduler.decay_rate ** step
    )
    
    return optimizer, scheduler


def create_inference_grid(cfg: DictConfig, device: torch.device):
    """å‰µå»ºæ¨ç†ç¶²æ ¼"""
    resolution = cfg.inference.grid_resolution
    domain = cfg.inference.domain
    
    x = np.linspace(domain[0][0], domain[0][1], resolution)
    y = np.linspace(domain[1][0], domain[1][1], resolution)
    xx, yy = np.meshgrid(x, y, indexing="xy")
    
    xx = torch.from_numpy(xx).to(torch.float).to(device)
    yy = torch.from_numpy(yy).to(torch.float).to(device)
    
    return xx, yy


def apply_boundary_conditions(cfg: DictConfig, bc_data: dict, model: torch.nn.Module):
    """æ‡‰ç”¨é‚Šç•Œæ¢ä»¶ä¸¦è¨ˆç®—é‚Šç•Œæå¤±"""
    cavity_size = cfg.physics.cavity_size
    height = cavity_size
    
    y_vals = bc_data[0]["y"]
    
    # åˆ†é›¢ä¸åŒçš„é‚Šç•Œ
    mask_no_slip = y_vals < height / 2  # bottom, left, right walls
    mask_top_wall = y_vals == height / 2  # moving top wall
    
    # æå–é‚Šç•Œé»
    no_slip = {}
    top_wall = {}
    
    for k in bc_data[0].keys():
        no_slip[k] = (bc_data[0][k][mask_no_slip]).reshape(-1, 1)
        top_wall[k] = (bc_data[0][k][mask_top_wall]).reshape(-1, 1)
    
    # æ¨¡å‹æ¨ç†
    no_slip_coords = torch.cat([no_slip["x"], no_slip["y"]], dim=1)
    top_wall_coords = torch.cat([top_wall["x"], top_wall["y"]], dim=1)
    
    no_slip_out = model(no_slip_coords)
    top_wall_out = model(top_wall_coords)
    
    # è¨ˆç®—é‚Šç•Œæ¢ä»¶æå¤±
    bc_cfg = cfg.boundary_conditions
    
    # ç„¡æ»‘ç§»é‚Šç•Œ (u=0, v=0)
    v_no_slip = torch.mean(no_slip_out[:, 1:2] ** 2) * bc_cfg.no_slip_walls.weight
    u_no_slip = torch.mean(no_slip_out[:, 0:1] ** 2) * bc_cfg.no_slip_walls.weight
    
    # ç§»å‹•é ‚å£ (u=1, v=0) 
    u_slip = torch.mean(
        ((top_wall_out[:, 0:1] - bc_cfg.moving_lid.u) ** 2) * 
        (1 - bc_cfg.moving_lid.edge_attenuation * torch.abs(top_wall["x"]))
    ) * bc_cfg.moving_lid.weight
    
    v_slip = torch.mean(
        (top_wall_out[:, 1:2] - bc_cfg.moving_lid.v) ** 2
    ) * bc_cfg.moving_lid.weight
    
    return u_no_slip + v_no_slip + u_slip + v_slip


def apply_physics_constraints(cfg: DictConfig, int_data: dict, model: torch.nn.Module, 
                            phy_inf: PhysicsInformer):
    """æ‡‰ç”¨ç‰©ç†ç´„æŸä¸¦è¨ˆç®—ç‰©ç†æå¤±"""
    interior = {}
    
    # æº–å‚™å…§éƒ¨é»è³‡æ–™
    for k, v in int_data[0].items():
        if k in ["x", "y"]:
            requires_grad = True
        else:
            requires_grad = False
        interior[k] = v.reshape(-1, 1).requires_grad_(requires_grad)
    
    # æ¨¡å‹å‰å‘å‚³æ’­
    coords = torch.cat([interior["x"], interior["y"]], dim=1)
    interior_out = model(coords)
    
    # ä½¿ç”¨PhysicsInformerè¨ˆç®—ç‰©ç†æ®˜å·®
    phy_loss_dict = phy_inf.forward({
        "coordinates": coords,
        "u": interior_out[:, 0:1],
        "v": interior_out[:, 1:2],
        "p": interior_out[:, 2:3],
    })
    
    # ä½¿ç”¨SDFæ¬Šé‡ç‰©ç†æå¤± (åƒ…åœ¨åŸŸå…§éƒ¨æ‡‰ç”¨ç´„æŸ)
    sdf = interior["sdf"]
    cont = phy_loss_dict["continuity"] * sdf
    mom_x = phy_loss_dict["momentum_x"] * sdf
    mom_y = phy_loss_dict["momentum_y"] * sdf
    
    # åŠ æ¬Šç‰©ç†æå¤±
    weights = cfg.loss_weights
    physics_loss = (
        weights.continuity * torch.mean(cont ** 2) +
        weights.momentum_x * torch.mean(mom_x ** 2) +
        weights.momentum_y * torch.mean(mom_y ** 2)
    )
    
    return physics_loss


def plot_results(cfg: DictConfig, model: torch.nn.Module, xx: torch.Tensor, 
                yy: torch.Tensor, epoch: int, loss_value: float, output_dir: str):
    """ç¹ªè£½çµæœ"""
    if not cfg.outputs.save_plots:
        return
        
    resolution = cfg.inference.grid_resolution
    
    with torch.no_grad():
        # æ¨ç†
        coords = torch.cat([xx.reshape(-1, 1), yy.reshape(-1, 1)], dim=1)
        inf_out = model(coords)
        out_np = inf_out.detach().cpu().numpy()
        
        # å‰µå»ºåœ–åƒ
        fig, axes = plt.subplots(1, 4, figsize=(16, 4))
        
        # ué€Ÿåº¦
        im = axes[0].imshow(
            out_np[:, 0].reshape(resolution, resolution), 
            origin="lower", extent=cfg.inference.domain[0] + cfg.inference.domain[1]
        )
        fig.colorbar(im, ax=axes[0])
        axes[0].set_title(f"u velocity (epoch {epoch})")
        axes[0].set_xlabel("x")
        axes[0].set_ylabel("y")
        
        # vé€Ÿåº¦
        im = axes[1].imshow(
            out_np[:, 1].reshape(resolution, resolution), 
            origin="lower", extent=cfg.inference.domain[0] + cfg.inference.domain[1]
        )
        fig.colorbar(im, ax=axes[1])
        axes[1].set_title(f"v velocity (epoch {epoch})")
        axes[1].set_xlabel("x")
        axes[1].set_ylabel("y")
        
        # å£“åŠ›
        im = axes[2].imshow(
            out_np[:, 2].reshape(resolution, resolution), 
            origin="lower", extent=cfg.inference.domain[0] + cfg.inference.domain[1]
        )
        fig.colorbar(im, ax=axes[2])
        axes[2].set_title(f"Pressure (epoch {epoch})")
        axes[2].set_xlabel("x")
        axes[2].set_ylabel("y")
        
        # é€Ÿåº¦å¹…åº¦
        u_mag = ((out_np[:, 0] ** 2 + out_np[:, 1] ** 2) ** 0.5).reshape(resolution, resolution)
        im = axes[3].imshow(
            u_mag, origin="lower", 
            extent=cfg.inference.domain[0] + cfg.inference.domain[1]
        )
        fig.colorbar(im, ax=axes[3])
        axes[3].set_title(f"Velocity Magnitude (epoch {epoch})")
        axes[3].set_xlabel("x")
        axes[3].set_ylabel("y")
        
        # æ·»åŠ æ¨™é¡Œ
        fig.suptitle(f"LDC Flow (Re={cfg.physics.Re}) - Loss: {loss_value:.2e}", fontsize=14)
        
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, f"ldc_results_epoch_{epoch:06d}.png"), 
                   dpi=150, bbox_inches='tight')
        plt.close()


@hydra.main(version_base="1.3", config_path="configs", config_name="ldc_pinn")
def ldc_trainer(cfg: DictConfig) -> None:
    """ä¸»è¨“ç·´å‡½æ•¸"""
    # åˆå§‹åŒ–åˆ†å¸ƒå¼ç®¡ç†å™¨
    DistributedManager.initialize()
    dist = DistributedManager()
    
    # è¨­ç½®æ—¥èªŒ
    log = PythonLogger(name="ldc-physicsnemo")
    if cfg.logging.file_logging:
        log.file_logging()
    
    if dist.rank == 0:
        log.info("ğŸš€ å•Ÿå‹•PhysicsNeMo LDC-PINNè¨“ç·´")
        log.info(f"ğŸ“Š è¨­å‚™: {dist.device}")
        log.info(f"ğŸ”§ Reynoldsæ•¸: {cfg.physics.Re}")
        log.info(f"ğŸ¯ æœ€å¤§epochs: {cfg.training.max_epochs}")
    
    # å‰µå»ºè¼¸å‡ºç›®éŒ„
    output_dir = create_output_directory(cfg)
    
    # è¨­ç½®æ¨¡å‹
    model = setup_model(cfg, dist.device)
    
    # è¨­ç½®ç‰©ç†æ–¹ç¨‹å¼
    phy_inf = setup_physics(cfg, dist.device)
    
    # è¨­ç½®å¹¾ä½•å’Œè³‡æ–™è¼‰å…¥å™¨
    bc_dataloader, interior_dataloader, geometry = setup_geometry_dataloaders(cfg, dist.device)
    
    # è¨­ç½®å„ªåŒ–å™¨å’Œèª¿åº¦å™¨
    optimizer, scheduler = setup_optimizer_and_scheduler(cfg, model)
    
    # å‰µå»ºæ¨ç†ç¶²æ ¼
    xx, yy = create_inference_grid(cfg, dist.device)
    
    if dist.rank == 0:
        log.info("âœ… åˆå§‹åŒ–å®Œæˆï¼Œé–‹å§‹è¨“ç·´")
    
    # è¨“ç·´å¾ªç’°
    for epoch in range(cfg.training.max_epochs):
        for bc_data, int_data in zip(bc_dataloader, interior_dataloader):
            optimizer.zero_grad()
            
            # è¨ˆç®—é‚Šç•Œæ¢ä»¶æå¤±
            boundary_loss = apply_boundary_conditions(cfg, bc_data, model)
            
            # è¨ˆç®—ç‰©ç†ç´„æŸæå¤±
            physics_loss = apply_physics_constraints(cfg, int_data, model, phy_inf)
            
            # ç¸½æå¤±
            total_loss = boundary_loss + physics_loss
            
            # åå‘å‚³æ’­
            total_loss.backward()
            optimizer.step()
            scheduler.step()
            
        # æ—¥èªŒå’Œç¹ªåœ–
        if dist.rank == 0:
            if epoch % cfg.training.log_freq == 0:
                current_lr = optimizer.param_groups[0]['lr']
                log.info(f"Epoch {epoch:6d}: Loss = {total_loss.item():.2e}, "
                        f"LR = {current_lr:.2e}")
            
            if epoch % cfg.training.plot_freq == 0:
                plot_results(cfg, model, xx, yy, epoch, total_loss.item(), output_dir)
    
    if dist.rank == 0:
        log.info("ğŸ‰ è¨“ç·´å®Œæˆ!")


if __name__ == "__main__":
    ldc_trainer()