import os
import sys
import time
import torch
import torch.distributed as dist
import torch.multiprocessing as mp
from torch.nn.parallel import DistributedDataParallel as DDP
import pinn_solver as psolver
import cavity_data as cavity
from config import ConfigManager
from tools import setup_device, get_cuda_info
import argparse

torch.backends.cudnn.benchmark = True

def display_supervision_setup(config_manager, rank=0):
    """é¡¯ç¤ºç›£ç£è¨­ç½®å’Œç›£ç£é»ä½ç½®"""
    if rank != 0:  # åªåœ¨ä¸»é€²ç¨‹é¡¯ç¤º
        return
        
    supervision_config = config_manager.config.supervision
    
    print("ğŸ¯ ç›£ç£æ•¸æ“šé…ç½®:")
    print(f"   å•Ÿç”¨ç‹€æ…‹: {'âœ… å•Ÿç”¨' if supervision_config.enabled else 'âŒ é—œé–‰'}")
    print(f"   ç›£ç£é»æ•¸: {supervision_config.data_points}")
    print(f"   æ•¸æ“šæ¬Šé‡: {supervision_config.weight}")
    print(f"   éš¨æ©Ÿç¨®å­: {supervision_config.random_seed}")
    print(f"   æ•¸æ“šè·¯å¾‘: {supervision_config.data_path}")
    
    if supervision_config.enabled and supervision_config.data_points > 0:
        print("\n" + "="*50)
        print("ğŸ” ç›£ç£é»ä½ç½®è©³ç´°ä¿¡æ¯ï¼š")
        print("="*50)
        
        # æª¢æŸ¥æ•¸æ“šæ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(supervision_config.data_path):
            print(f"âš ï¸  æ•¸æ“šæ–‡ä»¶ä¸å­˜åœ¨: {supervision_config.data_path}")
            return
            
        from cavity_data import DataLoader
        loader = DataLoader()
        try:
            loader.print_supervision_locations(
                supervision_config.data_path,
                supervision_config.data_points, 
                supervision_config.random_seed
            )
        except Exception as e:
            print(f"âš ï¸  è¼‰å…¥ç›£ç£é»ä½ç½®æ™‚å‡ºéŒ¯: {e}")

def parse_args():
    """è§£æå‘½ä»¤è¡Œåƒæ•¸"""
    parser = argparse.ArgumentParser(description='PINN Training with Configuration Management')
    parser.add_argument('--config', type=str, default='configs/production.yaml',
                       help='é…ç½®æ–‡ä»¶è·¯å¾‘ (default: configs/production.yaml)')
    parser.add_argument('--resume', type=str, default=None,
                       help='å¾æª¢æŸ¥é»æ¢å¾©è¨“ç·´')
    parser.add_argument('--dry-run', action='store_true',
                       help='åªé¡¯ç¤ºé…ç½®ä¸åŸ·è¡Œè¨“ç·´')
    return parser.parse_args()

def setup_distributed():
    """è¨­ç½®åˆ†å¸ƒå¼è¨“ç·´ç’°å¢ƒ"""
    # æª¢æŸ¥åˆ†å¸ƒå¼ç’°å¢ƒè®Šæ•¸
    if 'RANK' not in os.environ:
        print("ğŸ’» å–®GPUæ¨¡å¼")
        os.environ['RANK'] = '0'
        os.environ['LOCAL_RANK'] = '0'
        os.environ['WORLD_SIZE'] = '1'
        return False  # éåˆ†å¸ƒå¼æ¨¡å¼
    
    # åˆå§‹åŒ–åˆ†å¸ƒå¼é€²ç¨‹çµ„
    try:
        if not dist.is_initialized():
            # åªæœ‰åœ¨ rank 0 æ™‚æ‰é¡¯ç¤ºåˆå§‹åŒ–ä¿¡æ¯
            rank = int(os.environ.get('RANK', 0))
            if rank == 0:
                print("ğŸ”— åˆå§‹åŒ–åˆ†å¸ƒå¼è¨“ç·´...")
                
            dist.init_process_group(backend='nccl')
            
            rank = dist.get_rank()
            world_size = dist.get_world_size()
            local_rank = int(os.environ['LOCAL_RANK'])
            
            # åªæœ‰ä¸»é€²ç¨‹é¡¯ç¤ºåˆ†å¸ƒå¼ä¿¡æ¯
            if rank == 0:
                print(f"ğŸ“¡ åˆ†å¸ƒå¼è¨“ç·´è¨­ç½®å®Œæˆ: {world_size} GPUs")
                print(f"   - Backend: NCCL")
                print(f"   - æ¯å€‹é€²ç¨‹è² è²¬ GPU {local_rank}")
            
            # ä½¿ç”¨çµ±ä¸€è¨­å‚™ç®¡ç†å‡½æ•¸
            device = setup_device(local_rank)
                
        return True  # åˆ†å¸ƒå¼æ¨¡å¼
        
    except Exception as e:
        rank = int(os.environ.get('RANK', 0))
        if rank == 0:
            print(f"âŒ åˆ†å¸ƒå¼åˆå§‹åŒ–å¤±æ•—: {e}")
            print("ğŸ’» é€€å›å–®GPUæ¨¡å¼")
        os.environ['RANK'] = '0'
        os.environ['LOCAL_RANK'] = '0'
        os.environ['WORLD_SIZE'] = '1'
        return False

def cleanup_distributed():
    """æ¸…ç†åˆ†å¸ƒå¼è¨“ç·´ç’°å¢ƒ"""
    if dist.is_initialized():
        dist.destroy_process_group()

def main():
    """ä¸»è¨“ç·´å‡½æ•¸ - ä½¿ç”¨é…ç½®ç³»çµ±"""
    args = parse_args()
    
    # è¨­ç½®åˆ†å¸ƒå¼ç’°å¢ƒ
    is_distributed = setup_distributed()
    
    # ç²å–ç•¶å‰é€²ç¨‹çš„rankï¼ˆç”¨æ–¼æ§åˆ¶è¼¸å‡ºï¼‰
    rank = int(os.environ.get('RANK', 0))
    
    try:
        # åªåœ¨ä¸»é€²ç¨‹é¡¯ç¤ºé…ç½®è¼‰å…¥ä¿¡æ¯
        if rank == 0:
            print(f"ğŸ“‚ è¼‰å…¥é…ç½®æ–‡ä»¶: {args.config}")
        
        config_manager = ConfigManager.from_file(args.config)
        config = config_manager.config  # ç²å–é…ç½®å°è±¡
        
        # åªåœ¨ä¸»é€²ç¨‹é¡¯ç¤ºé©—è­‰å’Œé…ç½®ä¿¡æ¯
        if rank == 0:
            # é©—è­‰é…ç½®
            warnings = config_manager.validate_config()
            if warnings:
                print("âš ï¸  é…ç½®è­¦å‘Š:")
                for warning in warnings:
                    print(f"   - {warning}")
            
            # é¡¯ç¤ºé…ç½®
            config_manager.print_config()
            
            # é¡¯ç¤ºç›£ç£æ•¸æ“šè¨­ç½®
            display_supervision_setup(config_manager, rank)
            
            if args.dry_run:
                print("ğŸƒ Dry runæ¨¡å¼ï¼Œä¸åŸ·è¡Œè¨“ç·´")
                return
        
        # Dry runæª¢æŸ¥ï¼ˆæ‰€æœ‰é€²ç¨‹éƒ½éœ€è¦é€€å‡ºï¼‰
        if args.dry_run:
            return

        # Enable anomaly detection to find the operation that failed to compute its gradient
        torch.autograd.set_detect_anomaly(False)
        
        # åªåœ¨ä¸»é€²ç¨‹é¡¯ç¤ºPINNå‰µå»ºä¿¡æ¯
        if rank == 0:
            print("ğŸš€ å‰µå»ºPINNå¯¦ä¾‹...")
        
        PINN = psolver.PysicsInformedNeuralNetwork(
            Re=config.physics.Re,
            layers=config.network.layers,
            layers_1=config.network.layers_1,
            hidden_size=config.network.hidden_size,
            hidden_size_1=config.network.hidden_size_1,
            N_f=config.training.N_f,
            batch_size=config.training.batch_size,
            alpha_evm=config.physics.alpha_evm,
            bc_weight=config.physics.bc_weight,
            eq_weight=config.physics.eq_weight,
            supervised_data_weight=config.supervision.weight if hasattr(config, 'supervision') else 1.0,
            supervision_data_points=config.supervision.data_points if hasattr(config, 'supervision') else 0,
            supervision_data_path=config.supervision.data_path if hasattr(config, 'supervision') else None,
            supervision_random_seed=config.supervision.random_seed if hasattr(config, 'supervision') else 42,
            checkpoint_freq=config.training.checkpoint_freq,
            config=config
        )
        # config å·²æ–¼æ§‹é€ æ™‚æ³¨å…¥ä¸¦åœ¨DDPå‰å®Œæˆç¸®æ”¾
        
        # åªåœ¨ä¸»é€²ç¨‹é¡¯ç¤ºæ•¸æ“šè¼‰å…¥ä¿¡æ¯
        if rank == 0:
            print("ğŸ“ è¼‰å…¥è¨“ç·´æ•¸æ“š...")
        
        path = './data/'
        dataloader = cavity.DataLoader(
            path=path,
            N_f=config.training.N_f,
            N_b=1000,
            sort_by_boundary_distance=getattr(config.training, 'sort_by_boundary_distance', True)
        )

        # Set boundary data, | u, v, x, y
        boundary_np = dataloader.loading_boundary_data()
        xb_cpu = torch.as_tensor(boundary_np[0], dtype=torch.float32).contiguous()
        yb_cpu = torch.as_tensor(boundary_np[1], dtype=torch.float32).contiguous()
        ub_cpu = torch.as_tensor(boundary_np[2], dtype=torch.float32).contiguous()
        vb_cpu = torch.as_tensor(boundary_np[3], dtype=torch.float32).contiguous()
        total_b = xb_cpu.shape[0]
        world_size = int(os.environ.get('WORLD_SIZE', '1'))
        r = rank
        if total_b < world_size:
            if r < total_b:
                b_start, b_end = r, r+1
            else:
                b_start, b_end = 0, 0
        else:
            per = total_b // world_size
            b_start = r * per
            b_end = b_start + per if r < world_size - 1 else total_b
        if dist.is_initialized():
            idx = [b_start, b_end]
            dist.broadcast_object_list(idx, src=0)
            b_start, b_end = idx
        xb = xb_cpu[b_start:b_end].to(PINN.device)
        yb = yb_cpu[b_start:b_end].to(PINN.device)
        ub = ub_cpu[b_start:b_end].to(PINN.device)
        vb = vb_cpu[b_start:b_end].to(PINN.device)
        PINN.set_boundary_data(X=(xb, yb, ub, vb))

        # Set training data, | x, y
        eq_np = dataloader.loading_training_data()
        xf_cpu = torch.as_tensor(eq_np[0], dtype=torch.float32).contiguous()
        yf_cpu = torch.as_tensor(eq_np[1], dtype=torch.float32).contiguous()
        total_f = xf_cpu.shape[0]
        if total_f < world_size:
            if r < total_f:
                f_start, f_end = r, r+1
            else:
                f_start, f_end = 0, 1
        else:
            per_f = total_f // world_size
            f_start = r * per_f
            f_end = f_start + per_f if r < world_size - 1 else total_f
        if dist.is_initialized():
            idxf = [f_start, f_end]
            dist.broadcast_object_list(idxf, src=0)
            f_start, f_end = idxf
        xf = xf_cpu[f_start:f_end].to(PINN.device).contiguous().requires_grad_(True)
        yf = yf_cpu[f_start:f_end].to(PINN.device).contiguous().requires_grad_(True)
        PINN.set_eq_training_data(X=(xf, yf))

        # åŠ è½½ç›‘ç£æ•°æ®ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if hasattr(config, 'supervision') and config.supervision.enabled and config.supervision.data_points > 0:
            if rank == 0:
                print(f"ğŸ“Š è½½å…¥ç›‘ç£æ•°æ®: {config.supervision.data_points} ä¸ªæ•°æ®ç‚¹...")
            
            # åŠ è½½ç›‘ç£æ•°æ®
            x_sup, y_sup, u_sup, v_sup, p_sup = dataloader.loading_supervision_data(
                config.supervision.data_path, 
                config.supervision.data_points,
                config.supervision.random_seed
            )
            
            # è½¬æ¢ä¸ºtensorå¹¶ç§»åˆ°GPU
            if x_sup.shape[0] > 0:  # ç¡®ä¿æœ‰æ•°æ®ç‚¹
                # ç›£ç£åº§æ¨™ä¸éœ€è¦æ¢¯åº¦ï¼Œåªéœ€è®“ç¶²è·¯æ¬Šé‡åå‘å³å¯
                x_sup_tensor = torch.as_tensor(x_sup, dtype=torch.float32).to(PINN.device)
                y_sup_tensor = torch.as_tensor(y_sup, dtype=torch.float32).to(PINN.device)
                u_sup_tensor = torch.as_tensor(u_sup, dtype=torch.float32).to(PINN.device)
                v_sup_tensor = torch.as_tensor(v_sup, dtype=torch.float32).to(PINN.device)
                p_sup_tensor = torch.as_tensor(p_sup, dtype=torch.float32).to(PINN.device)
                
                # è®¾ç½®ç›‘ç£æ•°æ®åˆ°PINN
                PINN.x_sup = x_sup_tensor
                PINN.y_sup = y_sup_tensor
                PINN.u_sup = u_sup_tensor
                PINN.v_sup = v_sup_tensor
                PINN.p_sup = p_sup_tensor
                
                if rank == 0:
                    print(f"âœ… ç›‘ç£æ•°æ®åŠ è½½å®Œæˆï¼Œç›‘ç£ç‚¹åæ ‡: ({x_sup[0,0]:.4f}, {y_sup[0,0]:.4f})")
        else:
            if rank == 0:
                print("ğŸ“Š æœªå¯ç”¨ç›‘ç£æ•°æ®æˆ–æ•°æ®ç‚¹æ•°é‡ä¸º0")

        filename = f'./data/cavity_Re{config.physics.Re}_256_Uniform.mat'
        x_star, y_star, u_star, v_star, p_star = dataloader.loading_evaluate_data(filename)

        # ä½¿ç”¨é…ç½®ä¸­çš„è¨“ç·´éšæ®µ
        training_stages = []
        for i, stage in enumerate(config.training.training_stages):
            alpha, epochs, lr = stage[0], stage[1], stage[2]
            sched = stage[3] if len(stage) > 3 else 'Constant'
            stage_name = f"Stage {i+1}"
            training_stages.append((alpha, epochs, lr, sched, stage_name))
        
        total_epochs = sum([stage[1] for stage in training_stages])
        
        if not is_distributed or PINN.rank == 0:
            print(f"ğŸš€ é–‹å§‹å®Œæ•´è¨“ç·´ï¼šç¸½å…± {total_epochs:,} epochsï¼Œåˆ† {len(training_stages)} å€‹éšæ®µ")
            print(f"   é ä¼°å®Œæˆæ™‚é–“å°‡åœ¨è¨“ç·´é–‹å§‹å¾Œè¨ˆç®—...")
            print("=" * 60)

        # åˆå§‹åŒ– AdamWï¼ˆStage 0 placeholderï¼Œå¯¦éš›æ¯Stageé‡å»ºï¼‰
        PINN.build_adamw_optimizer = getattr(PINN, 'build_adamw_optimizer')  # ä¿éšªå¼•ç”¨
        first_lr = training_stages[0][2]
        if getattr(config.training, 'weight_decay_stages', None) is not None:
            first_wd = config.training.weight_decay_stages[0]
        else:
            first_wd = getattr(config.training, 'weight_decay', 0.0)
        PINN.build_adamw_optimizer(first_lr, first_wd)

        # æ¢å¾©è¨“ç·´ç‹€æ…‹
        start_epoch = 0
        if args.resume:
            if rank == 0:
                print(f"ğŸ”„ æ­£åœ¨å¾æª¢æŸ¥é»æ¢å¾©: {args.resume}")
            start_epoch = PINN.load_checkpoint(args.resume, PINN.opt)
            if rank == 0:
                if start_epoch > 0:
                    print(f"âœ… æˆåŠŸæ¢å¾©ï¼Œå°‡å¾ epoch {start_epoch} é–‹å§‹")
                else:
                    print("âš ï¸ ç„¡æ³•è¼‰å…¥æª¢æŸ¥é»ï¼Œå°‡å¾é ­é–‹å§‹è¨“ç·´")

        # åŸ·è¡Œåˆ†éšæ®µè¨“ç·´
        # Note: When resuming, training will continue from the next epoch in the sequence,
        # but will start with the stage configuration determined by the current logic.
        # This means if you resume in what was originally stage 2, it will still follow the
        # sequence from stage 1 as defined in the config.
        # A more advanced implementation might save and restore the stage index.
        for stage_idx, (alpha_evm, num_epochs, learning_rate, sched_name, stage_name) in enumerate(training_stages):
            # Skip epochs that are already completed if resuming
            if start_epoch >= num_epochs:
                if rank == 0:
                    print(f"â­ï¸ è·³é {stage_name} (å·²å®Œæˆ {num_epochs} epochsï¼Œå¾ {start_epoch} æ¢å¾©)")
                start_epoch -= num_epochs  # Decrement for next stage
                continue
            
            epochs_to_run = num_epochs - start_epoch

            if not is_distributed or PINN.rank == 0:
                print(f"ğŸ”„ {stage_name}: alpha_evm={alpha_evm}, epochs={epochs_to_run}/{num_epochs}, lr={learning_rate:.2e}")
            
            # è¨­ç½®éšæ®µåç¨±å’Œåƒæ•¸
            PINN.current_stage = stage_name
            PINN.set_alpha_evm(alpha_evm)
            
            # é‡å»º AdamW ä»¥å¥—ç”¨è©²éšæ®µå­¸ç¿’ç‡èˆ‡ weight decay
            if getattr(config.training, 'weight_decay_stages', None) is not None:
                stage_wd = config.training.weight_decay_stages[stage_idx]
            else:
                stage_wd = getattr(config.training, 'weight_decay', 0.0)
            PINN.build_adamw_optimizer(learning_rate, stage_wd)
            if rank == 0:
                print(f"   - Optimizer: AdamW (lr={learning_rate:.2e}, wd={stage_wd})")

            # æ ¹æ“šç­–ç•¥æ±ºå®šèª¿åº¦å™¨ï¼ˆç”±é…ç½®æŒ‡å®šï¼‰
            stage_scheduler = None
            if sched_name not in ['Constant','MultiStepLR','CosineAnnealingLR','CosineAnnealingWarmRestarts','SGDR']:
                if not is_distributed or PINN.rank == 0:
                    print(f"   - æœªçŸ¥èª¿åº¦å™¨ {sched_name}ï¼Œå›é€€ Constant")
                sched_name = 'Constant'
            if sched_name == 'MultiStepLR':
                import math
                m1 = math.ceil(num_epochs/2)
                m2 = math.ceil(4*num_epochs/5)
                if not is_distributed or PINN.rank == 0:
                    print(f"   - å•Ÿç”¨ MultiStepLR é‡Œç¨‹ç¢‘: {m1}, {m2}")
                stage_scheduler = torch.optim.lr_scheduler.MultiStepLR(PINN.opt, milestones=[m1, m2], gamma=0.5)
            elif sched_name == 'CosineAnnealingLR':
                if stage_idx < len(training_stages) - 1:
                    eta_min = training_stages[stage_idx + 1][2]
                else:
                    eta_min = max(learning_rate * 0.1, 1e-8)
                if not is_distributed or PINN.rank == 0:
                    print(f"   - å•Ÿç”¨ CosineAnnealingLR: T_max={num_epochs}, eta_min={eta_min:.2e}")
                stage_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(PINN.opt, T_max=num_epochs, eta_min=eta_min)
            elif sched_name in ['CosineAnnealingWarmRestarts', 'SGDR']:
                # SGDR: CosineAnnealingWarmRestarts + Warmup (LinearLR)
                sgdr_cfg = getattr(config.training, 'sgdr', None)
                # é è¨­æš–å•Ÿå‹•æ­¥æ•¸ç‚ºéšæ®µçš„5%ï¼Œé™åˆ¶åœ¨[500, 10000]
                default_warmup = max(500, int(0.05 * num_epochs))
                default_warmup = min(default_warmup, 10000)
                warmup_epochs = int(getattr(sgdr_cfg, 'warmup_epochs', default_warmup) if sgdr_cfg else default_warmup)
                # é è¨­ç¬¬ä¸€å€‹é€±æœŸé•·åº¦ï¼ˆä¸å«æš–å•Ÿå‹•ï¼‰
                remain = max(1, num_epochs - warmup_epochs)
                default_T0 = max(1000, int(0.25 * remain))
                T_0 = int(getattr(sgdr_cfg, 'T_0', default_T0) if sgdr_cfg else default_T0)
                T_mult = int(getattr(sgdr_cfg, 'T_mult', 2) if sgdr_cfg else 2)
                # eta_min ä½¿ç”¨ä¸‹ä¸€éšæ®µlræˆ–ç•¶å‰lrçš„10%
                if stage_idx < len(training_stages) - 1:
                    eta_min = training_stages[stage_idx + 1][2]
                else:
                    eta_min = max(learning_rate * 0.1, 1e-8)
                eta_min = float(getattr(sgdr_cfg, 'eta_min', eta_min) if sgdr_cfg else eta_min)
                start_factor = float(getattr(sgdr_cfg, 'start_factor', 0.1) if sgdr_cfg else 0.1)
                end_factor = float(getattr(sgdr_cfg, 'end_factor', 1.0) if sgdr_cfg else 1.0)
                
                if not is_distributed or PINN.rank == 0:
                    print(f"   - å•Ÿç”¨ SGDR: warmup={warmup_epochs}, T_0={T_0}, T_mult={T_mult}, eta_min={eta_min:.2e}")
                    print(f"     * åŸºç¤å­¸ç¿’ç‡: {learning_rate:.2e} (warmup: {start_factor} -> {end_factor})")
                
                # å»ºç«‹ SequentialLR: LinearLR (warmup) -> CosineAnnealingWarmRestarts
                warmup_sched = torch.optim.lr_scheduler.LinearLR(
                    PINN.opt,
                    start_factor=start_factor,
                    end_factor=end_factor,
                    total_iters=warmup_epochs
                )
                cawr_sched = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
                    PINN.opt,
                    T_0=T_0,
                    T_mult=T_mult,
                    eta_min=eta_min
                )
                stage_scheduler = torch.optim.lr_scheduler.SequentialLR(
                    PINN.opt,
                    schedulers=[warmup_sched, cawr_sched],
                    milestones=[warmup_epochs]
                )

            # è¨“ç·´ç•¶å‰éšæ®µ
            start_time = time.time()

            # è¨­ç½® Profiler
            profiler_log_dir = f"runs/profiler/{stage_name}"
            if rank == 0:
                os.makedirs(profiler_log_dir, exist_ok=True)
            do_profile = (start_epoch % 20000 == 0)
            class _Noop:
                def step(self):
                    pass
            if False:  # åŸå…ˆStage 3æ··åˆå„ªåŒ–é—œé–‰ï¼Œæ”¹ç”±æ»‘çª—è§¸ç™¼ L-BFGS
                switch_epoch = int(num_epochs * 0.6)
                if start_epoch < switch_epoch:
                    run_epochs = min(epochs_to_run, switch_epoch - start_epoch)
                    PINN.train(num_epoch=run_epochs, lr=learning_rate, scheduler=stage_scheduler, profiler=_Noop(), start_epoch=start_epoch)
                    start_epoch += run_epochs
                    epochs_to_run -= run_epochs
                if epochs_to_run > 0:
                    if not is_distributed or PINN.rank == 0:
                        print(f"ğŸ” {stage_name}: åˆ‡æ›è‡³ L-BFGS (å¾Œ40%)")
                    lbfgs_cfg = {
                        'max_iter': 50,
                        'history_size': 20,
                        'tolerance_grad': 1e-8,
                        'tolerance_change': 1e-9,
                        'line_search_fn': 'strong_wolfe'
                    }
                    PINN.train_with_lbfgs_segment(max_outer_steps=2000, lbfgs_params=lbfgs_cfg, log_interval=200)
                    if not is_distributed or PINN.rank == 0:
                        print(f"âœ… {stage_name}: L-BFGS æ®µå®Œæˆï¼Œæ¢å¾© Adam")
                remaining = num_epochs - start_epoch
                if remaining > 0:
                    PINN.train(num_epoch=remaining, lr=learning_rate, scheduler=stage_scheduler, profiler=_Noop(), start_epoch=start_epoch)
            else:
                if do_profile:
                    with torch.profiler.profile(
                        schedule=torch.profiler.schedule(wait=1, warmup=1, active=2, repeat=1),
                        on_trace_ready=torch.profiler.tensorboard_trace_handler(profiler_log_dir),
                        record_shapes=True,
                        with_stack=True,
                        profile_memory=True
                    ) as prof:
                        PINN.train(num_epoch=epochs_to_run, lr=learning_rate, scheduler=stage_scheduler, profiler=prof, start_epoch=start_epoch)
                        if rank == 0:
                            print(f"ğŸ”§ Profileræ•¸æ“šå·²ä¿å­˜è‡³: {profiler_log_dir}")
                else:
                    PINN.train(num_epoch=epochs_to_run, lr=learning_rate, scheduler=stage_scheduler, profiler=_Noop(), start_epoch=start_epoch)

            stage_time = time.time() - start_time
            
            if not is_distributed or PINN.rank == 0:
                print(f"âœ… {stage_name} å®Œæˆï¼è€—æ™‚: {stage_time/3600:.2f} å°æ™‚")
                
                # è©•ä¼°ç•¶å‰éšæ®µçµæœ
                PINN.test(x_star, y_star, u_star, v_star, p_star, loop=stage_idx)
                print("-" * 60)

        if not is_distributed or PINN.rank == 0:
            print("ğŸ‰ æ‰€æœ‰è¨“ç·´éšæ®µå®Œæˆï¼")
            print("=" * 60)

    except Exception as e:
        print(f"âŒ è¨“ç·´éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        raise e
    
    finally:
        # æ¸…ç†åˆ†å¸ƒå¼è¨“ç·´ç’°å¢ƒ
        cleanup_distributed()

if __name__ == "__main__":
    main()
