# Copyright (c) 2023 scien42.tech, Se42 Authors. All Rights Reserved.
#
# PINN æª¢æŸ¥é»ç®¡ç†æ¨¡çµ„

import os
import torch
from torch.nn.parallel import DistributedDataParallel as DDP
from typing import Dict, Any, Optional, Union
from torch.nn import Module

class CheckpointManager:
    """
    PINN æª¢æŸ¥é»ç®¡ç†å™¨
    è™•ç†æ¨¡å‹çš„ä¿å­˜å’Œè¼‰å…¥åŠŸèƒ½
    """
    
    def __init__(self, model, evm_model=None, rank=0, logger=None):
        self.model = model
        self.evm_model = evm_model
        self.rank = rank
        self.logger = logger
    
    def _param_name_map(self, model: Union[Module, DDP]) -> Dict[int, str]:
        """æ˜ å°„åƒæ•¸IDåˆ°åƒæ•¸åç¨±"""
        return {id(p): n for n, p in model.named_parameters()}
    
    def _safe_optimizer_state_dict(self, optimizer: torch.optim.Optimizer) -> Dict[str, Any]:
        """
        å®‰å…¨ç²å–optimizer state dictï¼Œé¿å…DDPåƒæ•¸æ˜ å°„å•é¡Œ
        """
        try:
            return optimizer.state_dict()
        except Exception as e:
            if self.logger:
                self.logger.warning(f"Failed to get optimizer state dict: {e}")
            return {}
    
    def _load_optimizer_state_dict_safe(self, optimizer: torch.optim.Optimizer, opt_state: Optional[Dict[str, Any]]) -> None:
        """
        å®‰å…¨è¼‰å…¥optimizer state dict
        """
        if opt_state is None or not opt_state:
            if self.logger:
                self.logger.info("No optimizer state to load")
            return
        
        try:
            optimizer.load_state_dict(opt_state)
            if self.logger:
                self.logger.info("Successfully loaded optimizer state")
        except Exception as e:
            if self.logger:
                self.logger.warning(f"Failed to load optimizer state: {e}")
    
    def get_checkpoint_dir(self):
        """ç²å–æª¢æŸ¥é»ç›®éŒ„"""
        checkpoint_dir = "./checkpoints"
        if not os.path.exists(checkpoint_dir):
            os.makedirs(checkpoint_dir)
        return checkpoint_dir
    
    def save_checkpoint(self, epoch, optimizer):
        """ä¿å­˜æª¢æŸ¥é»"""
        if self.rank != 0:  # åªæœ‰ rank 0 ä¿å­˜
            return
            
        try:
            checkpoint_dir = self.get_checkpoint_dir()
            
            # ç²å–æ¨¡å‹ state_dict
            net_state = self.model.module.state_dict() if isinstance(self.model, DDP) else self.model.state_dict()
            net1_state = None
            if self.evm_model is not None:
                net1_state = self.evm_model.module.state_dict() if isinstance(self.evm_model, DDP) else self.evm_model.state_dict()
            
            # ç²å– optimizer state_dict
            opt_state = self._safe_optimizer_state_dict(optimizer) if optimizer else None
            
            checkpoint = {
                'epoch': epoch,
                'net_state_dict': net_state,
                'net1_state_dict': net1_state,
                'optimizer_state_dict': opt_state,
                'loss': None  # å¯ä»¥åœ¨èª¿ç”¨æ™‚å‚³å…¥
            }
            
            checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch}.pth')
            torch.save(checkpoint, checkpoint_path)
            
            if self.logger:
                self.logger.info(f"ğŸ’¾ Checkpoint saved: {checkpoint_path}")
                
        except Exception as e:
            if self.logger:
                self.logger.error(f"âŒ Failed to save checkpoint: {e}")
            raise
    
    def load_checkpoint(self, checkpoint_path, optimizer):
        """è¼‰å…¥æª¢æŸ¥é»"""
        try:
            if not os.path.exists(checkpoint_path):
                raise FileNotFoundError(f"Checkpoint not found: {checkpoint_path}")
            
            checkpoint = torch.load(checkpoint_path, map_location='cpu')
            
            # è¼‰å…¥æ¨¡å‹ç‹€æ…‹
            net_state = checkpoint.get('net_state_dict')
            net1_state = checkpoint.get('net1_state_dict')
            
            if net_state:
                model_to_load = self.model.module if isinstance(self.model, DDP) else self.model
                model_to_load.load_state_dict(net_state)
            
            if net1_state and self.evm_model is not None:
                evm_to_load = self.evm_model.module if isinstance(self.evm_model, DDP) else self.evm_model
                evm_to_load.load_state_dict(net1_state)
            
            # è¼‰å…¥ optimizer ç‹€æ…‹
            if optimizer:
                opt_state = checkpoint.get('optimizer_state_dict')
                self._load_optimizer_state_dict_safe(optimizer, opt_state)
            
            start_epoch = checkpoint.get('epoch', 0) + 1
            
            if self.logger:
                self.logger.info(f"âœ… Checkpoint loaded: {checkpoint_path}")
                self.logger.info(f"ğŸ“… Resuming from epoch: {start_epoch}")
            
            return start_epoch
            
        except Exception as e:
            if self.logger:
                self.logger.error(f"âŒ Failed to load checkpoint: {e}")
            raise
    
    def save_model(self, filename, directory=None, N_HLayer=None, N_neu=None, N_f=None):
        """ä¿å­˜æ¨¡å‹ (èˆ‡åŸå§‹ save æ–¹æ³•å…¼å®¹)"""
        try:
            if directory is None:
                directory = self.get_checkpoint_dir()
            
            if not os.path.exists(directory):
                os.makedirs(directory)
            
            # ç²å–å¯¦éš›æ¨¡å‹ (è™•ç† DDP åŒ…è£)
            net_to_save = self.model.module if isinstance(self.model, DDP) else self.model
            net1_to_save = None
            if self.evm_model is not None:
                net1_to_save = self.evm_model.module if isinstance(self.evm_model, DDP) else self.evm_model
            
            save_data = {
                'net_state_dict': net_to_save.state_dict(),
                'net1_state_dict': net1_to_save.state_dict() if net1_to_save else None,
                'N_HLayer': N_HLayer,
                'N_neu': N_neu,
                'N_f': N_f
            }
            
            filepath = os.path.join(directory, filename)
            torch.save(save_data, filepath)
            
            if self.logger:
                self.logger.info(f"ğŸ’¾ Model saved: {filepath}")
                
        except Exception as e:
            if self.logger:
                self.logger.error(f"âŒ Failed to save model: {e}")
            raise