# Copyright (c) 2023 scien42.tech, Se42 Authors. All Rights Reserved.
#
# PINN å„ªåŒ–å™¨å’Œæ’ç¨‹å™¨ç®¡ç†æ¨¡çµ„

import torch
import torch.optim as optim
from torch.optim.lr_scheduler import (
    LinearLR, CosineAnnealingLR, CosineAnnealingWarmRestarts, 
    MultiStepLR, SequentialLR
)
from typing import Dict, List, Any, Optional, Union

class OptimizerSchedulerManager:
    """
    PINN å„ªåŒ–å™¨å’Œæ’ç¨‹å™¨ç®¡ç†å™¨
    çµ±ä¸€ç®¡ç† Adamã€L-BFGS å„ªåŒ–å™¨å’Œå„ç¨®å­¸ç¿’ç‡æ’ç¨‹å™¨
    """
    
    def __init__(self, logger=None):
        self.logger = logger
        self.optimizer = None
        self.scheduler = None
        self.scheduler_params = {}
        
    def setup_optimizer(self, parameters, lr=1e-3, betas=(0.9, 0.999)):
        """è¨­ç½® Adam å„ªåŒ–å™¨"""
        self.optimizer = optim.Adam(parameters, lr=lr, betas=betas)
        if self.logger:
            self.logger.info(f"âœ… Adam optimizer initialized with lr={lr}")
        return self.optimizer
    
    def setup_lbfgs_optimizer(self, parameters, lr=1.0, max_iter=20):
        """è¨­ç½® L-BFGS å„ªåŒ–å™¨"""
        lbfgs_optimizer = optim.LBFGS(
            parameters, 
            lr=lr, 
            max_iter=max_iter,
            tolerance_grad=1e-9,
            tolerance_change=1e-12,
            history_size=100
        )
        if self.logger:
            self.logger.info(f"âœ… L-BFGS optimizer initialized with lr={lr}, max_iter={max_iter}")
        return lbfgs_optimizer
    
    def setup_scheduler(self, optimizer, scheduler_type, scheduler_params=None):
        """è¨­ç½®å­¸ç¿’ç‡æ’ç¨‹å™¨"""
        if scheduler_params is None:
            scheduler_params = {}
        
        self.scheduler_params = scheduler_params.copy()
        
        if scheduler_type == 'Constant':
            self.scheduler = None
            if self.logger:
                self.logger.info("ğŸ“ˆ Using constant learning rate")
        
        elif scheduler_type == 'LinearLR':
            params = {
                'start_factor': scheduler_params.get('start_factor', 1.0),
                'end_factor': scheduler_params.get('end_factor', 0.1),
                'total_iters': scheduler_params.get('total_iters', 1000)
            }
            self.scheduler = LinearLR(optimizer, **params)
            if self.logger:
                self.logger.info(f"ğŸ“ˆ LinearLR scheduler setup: {params}")
        
        elif scheduler_type == 'CosineAnnealingWarmRestarts':
            params = {
                'T_0': scheduler_params.get('T_0', 100),
                'T_mult': scheduler_params.get('T_mult', 2),
                'eta_min': scheduler_params.get('eta_min', 1e-8)
            }
            self.scheduler = CosineAnnealingWarmRestarts(optimizer, **params)
            if self.logger:
                self.logger.info(f"ğŸ“ˆ CosineAnnealingWarmRestarts scheduler setup: {params}")
        
        elif scheduler_type == 'CosineAnnealingLR':
            params = {
                'T_max': scheduler_params.get('T_max', 1000),
                'eta_min': scheduler_params.get('eta_min', 1e-8)
            }
            self.scheduler = CosineAnnealingLR(optimizer, **params)
            if self.logger:
                self.logger.info(f"ğŸ“ˆ CosineAnnealingLR scheduler setup: {params}")
        
        elif scheduler_type == 'MultiStepLR':
            params = {
                'milestones': scheduler_params.get('milestones', [100, 200]),
                'gamma': scheduler_params.get('gamma', 0.1)
            }
            self.scheduler = MultiStepLR(optimizer, **params)
            if self.logger:
                self.logger.info(f"ğŸ“ˆ MultiStepLR scheduler setup: {params}")
        
        elif scheduler_type == 'SGDR':
            # SGDR: LinearLR + CosineAnnealingWarmRestarts çµ„åˆ
            linear_params = {
                'start_factor': scheduler_params.get('start_factor', 1.0),
                'end_factor': scheduler_params.get('end_factor', 1.0),
                'total_iters': scheduler_params.get('total_iters', 1000)
            }
            cosine_params = {
                'T_0': scheduler_params.get('T_0', 100),
                'T_mult': scheduler_params.get('T_mult', 2),
                'eta_min': scheduler_params.get('eta_min', 1e-8)
            }
            
            schedulers = [
                LinearLR(optimizer, **linear_params),
                CosineAnnealingWarmRestarts(optimizer, **cosine_params)
            ]
            milestones = [linear_params['total_iters']]
            
            self.scheduler = SequentialLR(optimizer, schedulers, milestones)
            if self.logger:
                self.logger.info(f"ğŸ“ˆ SGDR scheduler setup - Linear: {linear_params}, Cosine: {cosine_params}")
        
        else:
            raise ValueError(f"Unsupported scheduler type: {scheduler_type}")
        
        return self.scheduler
    
    def rebuild_scheduler(self, optimizer):
        """
        é‡å»ºæ’ç¨‹å™¨ - ç”¨æ–¼ EVM ç¶²è·¯å‡çµ/è§£å‡æ™‚é‡å»º
        é€™æ˜¯ä¿®å¾© SequentialLR å•é¡Œçš„æ ¸å¿ƒæ–¹æ³•
        """
        if self.scheduler is None:
            return None
        
        scheduler_class_name = self.scheduler.__class__.__name__
        
        if scheduler_class_name == 'SequentialLR':
            # è™•ç† SequentialLR é‡å»º
            return self._rebuild_sequential_scheduler(optimizer)
        else:
            # è™•ç†å…¶ä»–é¡å‹çš„æ’ç¨‹å™¨
            return self._rebuild_simple_scheduler(optimizer, scheduler_class_name)
    
    def _rebuild_sequential_scheduler(self, optimizer):
        """é‡å»º SequentialLR (SGDR) æ’ç¨‹å™¨"""
        try:
            # æå–åŸå§‹æ’ç¨‹å™¨è³‡è¨Š
            if hasattr(self.scheduler, '_schedulers'):
                original_schedulers = self.scheduler._schedulers
            else:
                # å‚™ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨ä¿å­˜çš„åƒæ•¸é‡å»º
                return self._rebuild_sgdr_from_params(optimizer)
            
            if hasattr(self.scheduler, 'milestones'):
                original_milestones = self.scheduler.milestones
            else:
                original_milestones = []
            
            # é‡å»ºå­æ’ç¨‹å™¨
            new_schedulers = []
            for sched in original_schedulers:
                sched_type = type(sched).__name__
                
                if sched_type == 'LinearLR' and hasattr(sched, 'total_iters'):
                    new_schedulers.append(LinearLR(
                        optimizer,
                        start_factor=getattr(sched, 'start_factor', 1.0),
                        end_factor=getattr(sched, 'end_factor', 1.0),
                        total_iters=sched.total_iters
                    ))
                elif sched_type == 'CosineAnnealingWarmRestarts' and hasattr(sched, 'T_0'):
                    new_schedulers.append(CosineAnnealingWarmRestarts(
                        optimizer,
                        T_0=sched.T_0,
                        T_mult=getattr(sched, 'T_mult', 1),
                        eta_min=getattr(sched, 'eta_min', 0)
                    ))
                elif sched_type == 'CosineAnnealingLR' and hasattr(sched, 'T_max'):
                    new_schedulers.append(CosineAnnealingLR(
                        optimizer,
                        T_max=sched.T_max,
                        eta_min=getattr(sched, 'eta_min', 0)
                    ))
                elif sched_type == 'MultiStepLR' and hasattr(sched, 'milestones'):
                    new_schedulers.append(MultiStepLR(
                        optimizer,
                        milestones=list(sched.milestones),
                        gamma=sched.gamma
                    ))
            
            # æ™ºèƒ½é‡å»º milestones
            if not original_milestones:
                # å¦‚æœåŸå§‹ milestones ç‚ºç©ºï¼Œå¾ LinearLR ä¸­æå– total_iters
                if new_schedulers and hasattr(new_schedulers[0], 'total_iters'):
                    milestones = [new_schedulers[0].total_iters]
                else:
                    milestones = [1000]  # é»˜èªå€¼
                
                if self.logger:
                    self.logger.info(f"ğŸ”„ SequentialLR milestone é‡å»º: {milestones}")
            else:
                milestones = list(original_milestones)
            
            # å‰µå»ºæ–°çš„ SequentialLR
            new_scheduler = SequentialLR(optimizer, new_schedulers, milestones)
            
            if self.logger:
                self.logger.info("âœ… SequentialLR é‡å»ºæˆåŠŸ")
            
            self.scheduler = new_scheduler
            return new_scheduler
            
        except Exception as e:
            if self.logger:
                self.logger.error(f"âŒ SequentialLR é‡å»ºå¤±æ•—: {e}")
            # å˜—è©¦å‚™ç”¨æ–¹æ¡ˆ
            return self._rebuild_sgdr_from_params(optimizer)
    
    def _rebuild_sgdr_from_params(self, optimizer):
        """å¾ä¿å­˜çš„åƒæ•¸é‡å»º SGDR æ’ç¨‹å™¨"""
        try:
            linear_params = {
                'start_factor': self.scheduler_params.get('start_factor', 1.0),
                'end_factor': self.scheduler_params.get('end_factor', 1.0),
                'total_iters': self.scheduler_params.get('total_iters', 1000)
            }
            cosine_params = {
                'T_0': self.scheduler_params.get('T_0', 100),
                'T_mult': self.scheduler_params.get('T_mult', 2),
                'eta_min': self.scheduler_params.get('eta_min', 1e-8)
            }
            
            schedulers = [
                LinearLR(optimizer, **linear_params),
                CosineAnnealingWarmRestarts(optimizer, **cosine_params)
            ]
            milestones = [linear_params['total_iters']]
            
            new_scheduler = SequentialLR(optimizer, schedulers, milestones)
            
            if self.logger:
                self.logger.info("âœ… SGDR å¾åƒæ•¸é‡å»ºæˆåŠŸ")
            
            self.scheduler = new_scheduler
            return new_scheduler
            
        except Exception as e:
            if self.logger:
                self.logger.error(f"âŒ SGDR åƒæ•¸é‡å»ºå¤±æ•—: {e}")
            return None
    
    def _rebuild_simple_scheduler(self, optimizer, scheduler_class_name):
        """é‡å»ºç°¡å–®æ’ç¨‹å™¨ (é SequentialLR)"""
        try:
            if scheduler_class_name == 'LinearLR':
                params = {
                    'start_factor': self.scheduler_params.get('start_factor', 1.0),
                    'end_factor': self.scheduler_params.get('end_factor', 0.1),
                    'total_iters': self.scheduler_params.get('total_iters', 1000)
                }
                new_scheduler = LinearLR(optimizer, **params)
            
            elif scheduler_class_name == 'CosineAnnealingWarmRestarts':
                params = {
                    'T_0': self.scheduler_params.get('T_0', 100),
                    'T_mult': self.scheduler_params.get('T_mult', 2),
                    'eta_min': self.scheduler_params.get('eta_min', 1e-8)
                }
                new_scheduler = CosineAnnealingWarmRestarts(optimizer, **params)
            
            elif scheduler_class_name == 'CosineAnnealingLR':
                params = {
                    'T_max': self.scheduler_params.get('T_max', 1000),
                    'eta_min': self.scheduler_params.get('eta_min', 1e-8)
                }
                new_scheduler = CosineAnnealingLR(optimizer, **params)
            
            elif scheduler_class_name == 'MultiStepLR':
                params = {
                    'milestones': self.scheduler_params.get('milestones', [100, 200]),
                    'gamma': self.scheduler_params.get('gamma', 0.1)
                }
                new_scheduler = MultiStepLR(optimizer, **params)
            
            else:
                if self.logger:
                    self.logger.warning(f"Unknown scheduler type: {scheduler_class_name}")
                return None
            
            if self.logger:
                self.logger.info(f"âœ… {scheduler_class_name} é‡å»ºæˆåŠŸ")
            
            self.scheduler = new_scheduler
            return new_scheduler
            
        except Exception as e:
            if self.logger:
                self.logger.error(f"âŒ {scheduler_class_name} é‡å»ºå¤±æ•—: {e}")
            return None
    
    def step_scheduler(self):
        """åŸ·è¡Œæ’ç¨‹å™¨æ­¥é€²"""
        if self.scheduler is not None:
            self.scheduler.step()
    
    def get_current_lr(self):
        """ç²å–ç•¶å‰å­¸ç¿’ç‡"""
        if self.optimizer is None:
            return None
        return self.optimizer.param_groups[0]['lr']
    
    def zero_grad(self):
        """æ¸…é›¶æ¢¯åº¦"""
        if self.optimizer is not None:
            self.optimizer.zero_grad()
    
    def step_optimizer(self):
        """åŸ·è¡Œå„ªåŒ–å™¨æ­¥é€²"""
        if self.optimizer is not None:
            self.optimizer.step()