# batch_efficiency_test.py
# æ”¯æ´é›™GPUåˆ†å¸ƒå¼è¨“ç·´çš„æ‰¹æ¬¡è™•ç†æ•ˆç‡æ¸¬è©¦è…³æœ¬

import os
import time
import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import TensorDataset, DataLoader, DistributedSampler
import numpy as np
import psutil
import matplotlib.pyplot as plt
import csv
from datetime import datetime

import cavity_data as cavity
import pinn_solver as psolver
from tools import *

class DistributedBatchEfficiencyTester:
    def __init__(self):
        self.results = {}
        self.setup_distributed_environment()
        
    def setup_distributed_environment(self):
        """åˆå§‹åŒ–åˆ†å¸ƒå¼è¨“ç·´ç’°å¢ƒ"""
        # æª¢æŸ¥æ˜¯å¦åœ¨åˆ†å¸ƒå¼ç’°å¢ƒä¸­
        if 'WORLD_SIZE' not in os.environ:
            # å–®GPUæ¸¬è©¦æ¨¡å¼
            self.is_distributed = False
            self.rank = 0
            self.local_rank = 0
            self.world_size = 1
            os.environ['RANK'] = '0'
            os.environ['LOCAL_RANK'] = '0'
            os.environ['WORLD_SIZE'] = '1'
        else:
            # åˆ†å¸ƒå¼è¨“ç·´æ¨¡å¼
            self.is_distributed = True
            try:
                dist.init_process_group(backend='nccl')
                self.rank = dist.get_rank()
                self.world_size = dist.get_world_size()
                self.local_rank = int(os.environ['LOCAL_RANK'])
                torch.cuda.set_device(self.local_rank)
            except Exception as e:
                print(f"åˆ†å¸ƒå¼åˆå§‹åŒ–å¤±æ•—: {e}")
                self.is_distributed = False
                self.rank = 0
                self.local_rank = 0
                self.world_size = 1
        
        if self.rank == 0:
            print(f"ğŸ”§ åˆ†å¸ƒå¼è¨­ç½®:")
            print(f"   æ¨¡å¼: {'åˆ†å¸ƒå¼' if self.is_distributed else 'å–®GPU'}")
            print(f"   World Size: {self.world_size}")
            print(f"   GPUæ•¸é‡: {torch.cuda.device_count()}")
        
        # æ¸…ç©ºGPUç·©å­˜
        torch.cuda.empty_cache()
        
    def create_test_pinn(self, Re=3000):
        """å‰µå»ºæ¸¬è©¦ç”¨çš„PINNæ¨¡å‹ï¼ˆæ”¯æ´åˆ†å¸ƒå¼ï¼‰"""
        N_neu = 80
        N_neu_1 = 40
        lam_bcs = 10
        lam_equ = 1
        alpha_evm = 0.03
        N_HLayer = 6
        N_HLayer_1 = 4
        
        # PINNå·²å…§å»ºDDPæ”¯æ´ï¼Œç„¡éœ€é¡å¤–åŒ…è£
        pinn = psolver.PysicsInformedNeuralNetwork(
            Re=Re,
            layers=N_HLayer,
            layers_1=N_HLayer_1,
            hidden_size=N_neu,
            hidden_size_1=N_neu_1,
            alpha_evm=alpha_evm,
            bc_weight=lam_bcs,
            eq_weight=lam_equ,
            N_f=120000)
        
        return pinn
        
    def prepare_data(self, N_f=120000, N_b=1000):
        """æº–å‚™è¨“ç·´æ•¸æ“š"""
        path = './data/'
        dataloader = cavity.DataLoader(path=path, N_f=N_f, N_b=N_b)
        
        boundary_data = dataloader.loading_boundary_data()
        training_data = dataloader.loading_training_data()
        
        return boundary_data, training_data
        
    def create_distributed_dataloader(self, x_f, y_f, x_b, y_b, u_b, v_b, batch_size):
        """å‰µå»ºåˆ†å¸ƒå¼æ‰¹æ¬¡æ•¸æ“šåŠ è¼‰å™¨"""
        # ç‚ºæ–¹ç¨‹é»å‰µå»ºdataset
        eq_dataset = TensorDataset(
            torch.tensor(x_f, dtype=torch.float32),
            torch.tensor(y_f, dtype=torch.float32)
        )
        
        # ç‚ºé‚Šç•Œé»å‰µå»ºdataset  
        bc_dataset = TensorDataset(
            torch.tensor(x_b, dtype=torch.float32),
            torch.tensor(y_b, dtype=torch.float32),
            torch.tensor(u_b, dtype=torch.float32),
            torch.tensor(v_b, dtype=torch.float32)
        )
        
        if self.is_distributed:
            # åˆ†å¸ƒå¼æ¡æ¨£å™¨
            eq_sampler = DistributedSampler(eq_dataset, shuffle=True)
            bc_sampler = DistributedSampler(bc_dataset, shuffle=True)
            
            eq_loader = DataLoader(eq_dataset, batch_size=batch_size, 
                                 sampler=eq_sampler, num_workers=4, pin_memory=True)
            bc_loader = DataLoader(bc_dataset, batch_size=min(batch_size, len(bc_dataset)), 
                                 sampler=bc_sampler, num_workers=2, pin_memory=True)
        else:
            # å–®GPUæ¨¡å¼
            eq_loader = DataLoader(eq_dataset, batch_size=batch_size, shuffle=True)
            bc_loader = DataLoader(bc_dataset, batch_size=min(batch_size, len(bc_dataset)), shuffle=True)
        
        return eq_loader, bc_loader
        
    def monitor_all_gpus_memory(self):
        """ç›£æ§æ‰€æœ‰GPUçš„è¨˜æ†¶é«”ä½¿ç”¨"""
        gpu_info = []
        for i in range(torch.cuda.device_count()):
            if torch.cuda.is_available():
                torch.cuda.synchronize(device=i)
                allocated = torch.cuda.memory_allocated(device=i) / 1024**3
                reserved = torch.cuda.memory_reserved(device=i) / 1024**3
                gpu_info.append((allocated, reserved))
            else:
                gpu_info.append((0, 0))
        return gpu_info
        
    def monitor_cpu_memory(self):
        """ç›£æ§CPUè¨˜æ†¶é«”ä½¿ç”¨"""
        process = psutil.Process()
        cpu_memory = process.memory_info().rss / 1024**3
        return cpu_memory
        
    def test_batch_size_distributed(self, batch_size, num_steps=1000):
        """æ¸¬è©¦ç‰¹å®šbatch sizeçš„åˆ†å¸ƒå¼æ€§èƒ½"""
        if self.rank == 0:
            print(f"\n{'='*50}")
            print(f"æ¸¬è©¦ Batch Size: {batch_size} ({'åˆ†å¸ƒå¼' if self.is_distributed else 'å–®GPU'})")
            print(f"{'='*50}")
        
        # å‰µå»ºæ¨¡å‹å’Œæ•¸æ“š
        pinn = self.create_test_pinn()
        boundary_data, training_data = self.prepare_data()
        
        x_b, y_b, u_b, v_b = boundary_data
        x_f, y_f = training_data
        
        # è¨­ç½®æ•¸æ“šåˆ°æ¨¡å‹
        pinn.set_boundary_data(X=boundary_data)
        pinn.set_eq_training_data(X=training_data)
        
        # å‰µå»ºåˆ†å¸ƒå¼æ•¸æ“šåŠ è¼‰å™¨
        eq_loader, bc_loader = self.create_distributed_dataloader(
            x_f, y_f, x_b, y_b, u_b, v_b, batch_size
        )
        
        # å‰µå»ºè¿­ä»£å™¨ï¼ˆæå‡æ•ˆç‡ï¼‰
        eq_iter = iter(eq_loader)
        bc_iter = iter(bc_loader)
        
        # æ€§èƒ½ç›£æ§è®Šé‡
        step_times = []
        all_gpu_memory = []
        cpu_memory_usage = []
        losses = []
        
        # åŒæ­¥æ‰€æœ‰é€²ç¨‹
        if self.is_distributed:
            dist.barrier()
        
        # é–‹å§‹æ¸¬è©¦
        start_time = time.time()
        
        if self.rank == 0:
            print(f"é–‹å§‹ {num_steps} æ­¥åˆ†å¸ƒå¼è¨“ç·´æ¸¬è©¦...")
        
        for step in range(num_steps):
            step_start = time.time()
            
            # ç²å–æ‰¹æ¬¡æ•¸æ“š
            try:
                eq_batch = next(eq_iter)
                bc_batch = next(bc_iter)
            except StopIteration:
                # é‡æ–°é–‹å§‹æ–°çš„epoch
                eq_iter = iter(eq_loader)
                bc_iter = iter(bc_loader)
                eq_batch = next(eq_iter)
                bc_batch = next(bc_iter)
            
            # è¨­ç½®æ‰¹æ¬¡æ•¸æ“šåˆ°æ¨¡å‹
            x_f_batch = eq_batch[0].cuda(non_blocking=True)
            y_f_batch = eq_batch[1].cuda(non_blocking=True)
            x_b_batch = bc_batch[0].cuda(non_blocking=True)
            y_b_batch = bc_batch[1].cuda(non_blocking=True)
            u_b_batch = bc_batch[2].cuda(non_blocking=True)
            v_b_batch = bc_batch[3].cuda(non_blocking=True)
            
            # æ›´æ–°æ¨¡å‹æ•¸æ“š
            pinn.x_f = x_f_batch.requires_grad_(True)
            pinn.y_f = y_f_batch.requires_grad_(True)
            pinn.x_b = x_b_batch
            pinn.y_b = y_b_batch
            pinn.u_b = u_b_batch
            pinn.v_b = v_b_batch
            
            # å‰å‘å‚³æ’­å’Œåå‘å‚³æ’­
            pinn.opt.zero_grad()
            loss, loss_components = pinn.fwd_computing_loss_2d()
            loss.backward()  # DDPè‡ªå‹•é€²è¡Œæ¢¯åº¦åŒæ­¥
            pinn.opt.step()
            
            # åªåœ¨å¿…è¦æ™‚åŒæ­¥
            torch.cuda.synchronize()
            
            step_end = time.time()
            step_time = step_end - step_start
            
            # è¨˜éŒ„æ€§èƒ½æ•¸æ“šï¼ˆåªåœ¨rank 0è¨˜éŒ„ï¼‰
            if self.rank == 0:
                step_times.append(step_time)
                losses.append(loss.item())
                
                # è¨˜éŒ„æ‰€æœ‰GPUè¨˜æ†¶é«”ä½¿ç”¨
                gpu_memory = self.monitor_all_gpus_memory()
                all_gpu_memory.append(gpu_memory)
                
                cpu_mem = self.monitor_cpu_memory()
                cpu_memory_usage.append(cpu_mem)
                
                # å®šæœŸè¼¸å‡ºé€²åº¦
                if (step + 1) % 100 == 0:
                    avg_time = np.mean(step_times[-100:])
                    current_loss = loss.item()
                    total_gpu_memory = sum([gpu[0] for gpu in gpu_memory])
                    print(f"Step {step+1}/{num_steps} | "
                          f"Time: {avg_time:.4f}s | "
                          f"Loss: {current_loss:.6e} | "
                          f"Total GPU: {total_gpu_memory:.2f}GB")
        
        total_time = time.time() - start_time
        
        # åªåœ¨rank 0è¨ˆç®—å’Œè¿”å›çµæœ
        if self.rank == 0:
            # è¨ˆç®—GPUè¨˜æ†¶é«”çµ±è¨ˆ
            max_gpu_memory_per_gpu = []
            avg_gpu_memory_per_gpu = []
            for gpu_idx in range(len(all_gpu_memory[0])):
                gpu_usage = [frame[gpu_idx][0] for frame in all_gpu_memory]
                max_gpu_memory_per_gpu.append(max(gpu_usage))
                avg_gpu_memory_per_gpu.append(np.mean(gpu_usage))
            
            result = {
                'batch_size': batch_size,
                'distributed': self.is_distributed,
                'world_size': self.world_size,
                'total_time': total_time,
                'avg_step_time': np.mean(step_times),
                'std_step_time': np.std(step_times),
                'throughput': num_steps / total_time,
                'max_gpu_memory_per_gpu': max_gpu_memory_per_gpu,
                'avg_gpu_memory_per_gpu': avg_gpu_memory_per_gpu,
                'total_max_gpu_memory': sum(max_gpu_memory_per_gpu),
                'total_avg_gpu_memory': sum(avg_gpu_memory_per_gpu),
                'avg_cpu_memory': np.mean(cpu_memory_usage),
                'final_loss': losses[-1],
                'avg_loss': np.mean(losses[-100:]),
                'step_times': step_times,
                'losses': losses,
                'gpu_memory_history': all_gpu_memory
            }
            
            self.results[f"{batch_size}_{'dist' if self.is_distributed else 'single'}"] = result
        else:
            result = None
        
        # æ¸…ç†
        del pinn
        torch.cuda.empty_cache()
        
        return result
        
    def run_distributed_efficiency_tests(self):
        """é‹è¡Œåˆ†å¸ƒå¼æ•ˆç‡æ¸¬è©¦"""
        if self.rank == 0:
            print("ğŸš€ é–‹å§‹åˆ†å¸ƒå¼æ‰¹æ¬¡è™•ç†æ•ˆç‡æ¸¬è©¦")
            print(f"æ¸¬è©¦æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"GPUé…ç½®: {torch.cuda.device_count()} Ã— GPU")
        
        # æ¸¬è©¦ä¸åŒçš„batch size
        batch_sizes = [
            120000,  # åŸå§‹å…¨æ‰¹æ¬¡
            24000,   # é›™GPUæƒ…æ³ä¸‹çš„1/5
            12000,   # é›™GPUæƒ…æ³ä¸‹çš„1/10
            8000,    # ç©æ¥µè¨­ç½®
            4000,    # ä¿å®ˆè¨­ç½®
            2000     # æ›´å°æ‰¹æ¬¡
        ]
        
        for batch_size in batch_sizes:
            try:
                result = self.test_batch_size_distributed(batch_size, num_steps=1000)
                if self.rank == 0 and result:
                    self.print_distributed_batch_summary(result)
            except RuntimeError as e:
                if "out of memory" in str(e):
                    if self.rank == 0:
                        print(f"âŒ Batch size {batch_size} è¨˜æ†¶é«”ä¸è¶³ï¼Œè·³éæ¸¬è©¦")
                    torch.cuda.empty_cache()
                    continue
                else:
                    raise e
            except Exception as e:
                if self.rank == 0:
                    print(f"âŒ Batch size {batch_size} æ¸¬è©¦å¤±æ•—: {e}")
                continue
                
    def print_distributed_batch_summary(self, result):
        """æ‰“å°åˆ†å¸ƒå¼batch sizeæ¸¬è©¦çµæœ"""
        print(f"\nğŸ“Š Batch Size {result['batch_size']} åˆ†å¸ƒå¼æ¸¬è©¦çµæœ:")
        print(f"   æ¨¡å¼: {'åˆ†å¸ƒå¼' if result['distributed'] else 'å–®GPU'} ({result['world_size']} GPU)")
        print(f"   ç¸½æ™‚é–“: {result['total_time']:.2f}s")
        print(f"   å¹³å‡æ¯æ­¥æ™‚é–“: {result['avg_step_time']:.4f}s")
        print(f"   ååé‡: {result['throughput']:.2f} steps/s")
        print(f"   ç¸½GPUè¨˜æ†¶é«”: {result['total_max_gpu_memory']:.2f}GB")
        
        for i, gpu_mem in enumerate(result['max_gpu_memory_per_gpu']):
            print(f"   GPU {i} æœ€å¤§è¨˜æ†¶é«”: {gpu_mem:.2f}GB")
        
        print(f"   æœ€çµ‚æå¤±: {result['final_loss']:.6e}")
        
    def generate_performance_report(self):
        """ç”Ÿæˆæ€§èƒ½åˆ†æå ±å‘Š"""
        if not self.results:
            print("âŒ æ²’æœ‰æ¸¬è©¦çµæœå¯ä¾›åˆ†æ")
            return
            
        print(f"\n{'='*60}")
        print("ğŸ“ˆ åˆ†å¸ƒå¼æ‰¹æ¬¡è™•ç†æ•ˆç‡åˆ†æå ±å‘Š")
        print(f"{'='*60}")
        
        # å‰µå»ºæ¯”è¼ƒè¡¨æ ¼
        print(f"{'Batch Size':<12} {'Mode':<12} {'Time(s)':<10} {'Steps/s':<10} {'Total GPU(GB)':<15} {'Speedup':<10}")
        print("-" * 80)
        
        baseline_time = None
        for key in sorted(self.results.keys()):
            result = self.results[key]
            
            if baseline_time is None:
                baseline_time = result['avg_step_time']
                speedup = 1.0
            else:
                speedup = baseline_time / result['avg_step_time']
                
            mode = "åˆ†å¸ƒå¼" if result['distributed'] else "å–®GPU"
            print(f"{result['batch_size']:<12} {mode:<12} {result['avg_step_time']:<10.4f} "
                  f"{result['throughput']:<10.2f} {result['total_max_gpu_memory']:<15.2f} "
                  f"{speedup:<10.2f}x")
        
        # æ‰¾å‡ºæœ€ä½³batch size
        best_key = min(self.results.keys(), 
                      key=lambda x: self.results[x]['avg_step_time'])
        best_result = self.results[best_key]
        
        print(f"\nğŸ† æœ€ä½³æ€§èƒ½é…ç½®:")
        print(f"   Batch Size: {best_result['batch_size']}")
        print(f"   æ¨¡å¼: {'åˆ†å¸ƒå¼' if best_result['distributed'] else 'å–®GPU'}")
        print(f"   åŠ é€Ÿæ¯”: {baseline_time/best_result['avg_step_time']:.2f}x")
        print(f"   ç¸½GPUè¨˜æ†¶é«”ä½¿ç”¨: {best_result['total_max_gpu_memory']:.2f}GB")
        
        # ä¿å­˜çµæœåˆ°CSV
        self.save_results_csv()
        
    def save_results_csv(self):
        """ä¿å­˜çµæœåˆ°CSVæ–‡ä»¶"""
        filename = f"batch_efficiency_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        
        with open(filename, 'w', newline='') as csvfile:
            fieldnames = ['batch_size', 'distributed', 'world_size', 'total_time', 'avg_step_time', 
                         'throughput', 'total_max_gpu_memory', 'final_loss']
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            
            writer.writeheader()
            for result in self.results.values():
                writer.writerow({k: result[k] for k in fieldnames})
                
        print(f"\nğŸ’¾ çµæœå·²ä¿å­˜åˆ°: {filename}")
        
    def cleanup_distributed(self):
        """æ¸…ç†åˆ†å¸ƒå¼ç’°å¢ƒ"""
        if self.is_distributed:
            dist.destroy_process_group()

def main():
    """ä¸»å‡½æ•¸"""
    tester = DistributedBatchEfficiencyTester()
    
    try:
        # é‹è¡Œåˆ†å¸ƒå¼æ•ˆç‡æ¸¬è©¦
        tester.run_distributed_efficiency_tests()
        
        # åªåœ¨rank 0ç”Ÿæˆå ±å‘Š
        if tester.rank == 0:
            tester.generate_performance_report()
            print(f"\nâœ… åˆ†å¸ƒå¼æ¸¬è©¦å®Œæˆï¼")
            print("å»ºè­°æ ¹æ“šæ¸¬è©¦çµæœé¸æ“‡æœ€ä½³çš„batch sizeé€²è¡Œåˆ†å¸ƒå¼è¨“ç·´ã€‚")
        
    except KeyboardInterrupt:
        if tester.rank == 0:
            print("\nâš ï¸  æ¸¬è©¦è¢«ç”¨æˆ¶ä¸­æ–·")
    except Exception as e:
        if tester.rank == 0:
            print(f"\nâŒ æ¸¬è©¦éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
            import traceback
            traceback.print_exc()
    finally:
        tester.cleanup_distributed()

if __name__ == "__main__":
    main()