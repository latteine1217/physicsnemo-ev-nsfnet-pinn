"""
é…ç½®ç®¡ç†ç³»çµ± - çµ±ä¸€ç®¡ç†PINNè¨“ç·´åƒæ•¸
"""
import os
import json
import yaml
from dataclasses import dataclass, asdict
from typing import List, Tuple, Optional, Dict, Any
import torch

# å¸¸é‡å®šç¾©
RESULTS_PATH = "results"  # çµæœè¼¸å‡ºè·¯å¾‘

@dataclass
class NetworkConfig:
    """ç¥ç¶“ç¶²è·¯æ¶æ§‹é…ç½®"""
    layers: int = 6                    # ä¸»ç¶²è·¯å±¤æ•¸
    layers_1: int = 4                  # EVMç¶²è·¯å±¤æ•¸
    hidden_size: int = 80              # ä¸»ç¶²è·¯ç¥ç¶“å…ƒæ•¸
    hidden_size_1: int = 40            # EVMç¶²è·¯ç¥ç¶“å…ƒæ•¸
    # é¦–/æœ«å±¤ç¸®æ”¾ï¼ˆä¸»ç¶²/EVMï¼‰èˆ‡EVMè¼¸å‡ºæ¿€æ´»
    first_layer_scale_main: float = 2.0
    last_layer_scale_main: float = 0.5
    first_layer_scale_evm: float = 1.2
    last_layer_scale_evm: float = 0.1
    evm_output_activation: str = "softplus_cap"  # å¯é¸ï¼šsoftplus_cap | abs_cap
    # Activation settings
    activation_main: str = "tanh"      # å¯é¸ï¼štanh | laaf
    activation_evm: str = "tanh"       # å¯é¸ï¼štanh | laaf
    laaf_init_scale: float = 1.0       # LAAF åˆå§‹åŒ– a
    laaf_max_scale: float = 20.0       # LAAF ä¸Šé™ï¼ˆé¿å…æš´è¡ï¼‰
    laaf_reg_lambda: float = 0.0       # LAAF æ­£å‰‡æ¬Šé‡ï¼ˆ0é—œé–‰ï¼‰

@dataclass
class LBFGSConfig:
    """L-BFGSå„ªåŒ–å™¨é…ç½®"""
    enabled_in_distributed: bool = True    # åˆ†ä½ˆå¼æ¨¡å¼ä¸‹å•Ÿç”¨L-BFGS
    volatility_threshold: float = 0.01     # ï¼ˆèˆŠï¼‰æ³¢å‹•åº¦é–¾å€¼ï¼ˆä¿ç•™å‘å¾Œç›¸å®¹ï¼‰
    
    # éšæ®µæ§åˆ¶
    enable_from_stage: int = 3              # å¾ç¬¬å¹¾å€‹Stageé–‹å§‹å•Ÿç”¨L-BFGS
    
    # è§¸ç™¼æ¢ä»¶ï¼ˆæ–°ï¼‰ï¼šåˆ†éšæ®µè¦–çª—èˆ‡æ”¹å–„ç‡é–€æª»
    trigger_window_per_stage: Optional[List[int]] = None     # ä¾‹å¦‚ [5000, 7500, 10000]
    min_improve_pct_per_stage: Optional[List[float]] = None  # ä¾‹å¦‚ [0.02, 0.03, 0.015]
    ema_gamma: float = 0.95                        # æ”¹å–„ç‡åˆ†æ¯çš„ EMA å¹³æ»‘ä¿‚æ•¸
    
    # ç°¡åŒ–æ¢¯åº¦æ¢ä»¶
    use_simple_grad_check: bool = True             # ä½¿ç”¨ç°¡åŒ–çš„æ¢¯åº¦æª¢æŸ¥
    grad_median_abs_thresh: float = 2e-3           # æ¢¯åº¦çµ•å°é–€æª»ï¼ˆæ”¾å¯¬ï¼‰
    grad_relative_factor: float = 0.02             # ç›¸å°é–€æª»ï¼š< factor Ã— g_baseï¼ˆæ”¾å¯¬ï¼‰
    grad_cos_ema_thresh: float = 0.9               # æ¢¯åº¦æ–¹å‘ç©©å®šé–¾å€¼
    
    # ç‰©ç†æ¢ä»¶ï¼ˆæ”¾å¯¬ï¼‰
    alpha_evm_threshold: float = 0.02              # alpha_evmé–¾å€¼ï¼ˆæ”¾å¯¬ï¼‰
    cap_ratio_threshold: float = 0.7               # é»æ»¯ä½¿ç”¨ç‡é–¾å€¼ï¼ˆæ”¾å¯¬ï¼‰
    
    cooldown_steps: int = 5000                     # å…©æ®µ L-BFGS ä¹‹é–“å†·å»æ­¥æ•¸
    freeze_evm_during_lbfgs: bool = True           # æ®µå…§å‡çµ EVM
    # æ®µåƒæ•¸ï¼ˆå»ºè­°å€¼ï¼‰ï¼š
    max_outer_steps: int = 200                     # å¤–å¾ªç’°æ­¥æ•¸ä¸Šé™
    timeout_seconds: int = 600                     # åŸ·è¡Œè¶…æ™‚(ç§’)
    max_iter: int = 25                             # L-BFGSå…§éƒ¨è¿­ä»£æ•¸
    history_size: int = 20                         # æ­·å²å¤§å°
    tolerance_grad: float = 1e-6                   # æ¢¯åº¦å®¹å·®ï¼ˆfp32ï¼‰
    tolerance_change: float = 1e-8                 # è®ŠåŒ–å®¹å·®
    line_search_fn: str = "strong_wolfe"           # ç·šæœç´¢æ–¹æ³•
    early_stop_patience: int = 8                   # å…§è¿­ä»£é€£çºŒåœæ»¯æ¬¡æ•¸
    early_stop_min_delta: float = 1e-4             # å…§è¿­ä»£æœ€å°æ”¹å–„
    checkpoint_before_lbfgs: bool = True           # L-BFGSå‰è‡ªå‹•ä¿å­˜

@dataclass
class TrainingConfig:
    """è¨“ç·´åƒæ•¸é…ç½®"""
    N_f: int = 120000                  # æ–¹ç¨‹é»æ•¸é‡
    batch_size: Optional[int] = None   # æ‰¹æ¬¡å¤§å° (None = å…¨æ‰¹æ¬¡)
    checkpoint_freq: int = 5000        # æª¢æŸ¥é»ä¿å­˜é »ç‡
    log_tips: bool = True              # è¨“ç·´éç¨‹æç¤ºè¨Šæ¯
    sort_by_boundary_distance: bool = True  # æ˜¯å¦æŒ‰è·é›¢é‚Šç•Œé è¿‘æ’åºæ–¹ç¨‹é»
    pde_distance_weighting: bool = True     # æ˜¯å¦å•Ÿç”¨PDEè·é›¢æ¬Šé‡ w(d)
    pde_distance_w_min: float = 0.2         # æ¬Šé‡ä¸‹é™ï¼Œé¿å…é å€æ¬Šé‡ç‚º0
    pde_distance_tau: float = 0.1           # æŒ‡æ•¸æ¬Šé‡å°ºåº¦åƒæ•¸ tau

    # AdamW æ¬Šé‡è¡°æ¸›
    weight_decay: float = 0.0                 # å…¨åŸŸ weight decayï¼ˆè‹¥ç„¡åˆ†éšæ®µåˆ—è¡¨å‰‡ä½¿ç”¨ï¼‰
    weight_decay_stages: Optional[List[float]] = None  # åˆ†éšæ®µ weight decayï¼ˆéœ€èˆ‡ training_stages é•·åº¦ä¸€è‡´ï¼‰
    
    # è¨“ç·´éšæ®µé…ç½® (alpha_evm, epochs, learning_rate[, scheduler])
    training_stages: Optional[List[Tuple]] = None
    # SGDRï¼ˆCosineAnnealingWarmRestarts + warmupï¼‰åƒæ•¸ï¼ˆå¯é¸ï¼‰
    # å¯é¸éµï¼šwarmup_epochs, T_0, T_mult, eta_min, start_factor, end_factor
    sgdr: Optional[Dict[str, Any]] = None
    
    # L-BFGSé…ç½®
    lbfgs: Optional[LBFGSConfig] = None
    
    def __post_init__(self):
        if self.training_stages is None:
            # é»˜èª6éšæ®µè¨“ç·´é…ç½®
            self.training_stages = [
                (0.05, 350000, 1e-3),   # Stage 1
                (0.03, 350000, 2e-4),   # Stage 2  
                (0.01, 350000, 4e-5),   # Stage 3
                (0.005, 350000, 1e-5),   # Stage 4
                (0.002, 350000, 2e-6),   # Stage 5
                (0.002, 350000, 2e-6)   # Stage 6
            ]
        # åˆ†éšæ®µ weight decay é•·åº¦é©—è­‰
        if self.weight_decay_stages is not None:
            if len(self.weight_decay_stages) != len(self.training_stages):
                # è‡ªå‹•èª¿æ•´ï¼šé•·åº¦éé•·å‰‡è£å‰ªï¼ŒéçŸ­å‰‡ä»¥æœ€å¾Œä¸€å€‹å€¼å¡«å……
                orig = list(self.weight_decay_stages)
                if len(self.weight_decay_stages) > len(self.training_stages):
                    self.weight_decay_stages = self.weight_decay_stages[:len(self.training_stages)]
                else:
                    if len(self.weight_decay_stages) > 0:
                        last = self.weight_decay_stages[-1]
                    else:
                        last = 0.0
                    self.weight_decay_stages = self.weight_decay_stages + [last] * (len(self.training_stages) - len(self.weight_decay_stages))
                print(f"[Config] âš ï¸ weight_decay_stages é•·åº¦è‡ªå‹•èª¿æ•´: åŸ={orig} -> æ–°={self.weight_decay_stages}")
        if self.lbfgs is None:
            self.lbfgs = LBFGSConfig()
        # é è¨­è§¸ç™¼è¦–çª—èˆ‡é–€æª»ï¼ˆè‹¥æœªè¨­å®šï¼‰
        if self.lbfgs.trigger_window_per_stage is None:
            self.lbfgs.trigger_window_per_stage = [5000, 7500, 10000]
        if self.lbfgs.min_improve_pct_per_stage is None:
            self.lbfgs.min_improve_pct_per_stage = [0.02, 0.01, 0.005]

@dataclass
class PhysicsConfig:
    """ç‰©ç†åƒæ•¸é…ç½®"""
    Re: int = 5000                     # Reynolds number
    alpha_evm: float = 0.03            # åˆå§‹EVMä¿‚æ•¸
    beta: float = 1.0                  # äººå·¥ç²˜æ»¯åº¦ä¸Šé™ä¿‚æ•¸
    bc_weight: float = 10.0            # é‚Šç•Œæ¢ä»¶æ¬Šé‡
    eq_weight: float = 1.0             # æ–¹ç¨‹æ¬Šé‡

@dataclass
class SupervisionConfig:
    """ç›‘ç£æ•°æ®é…ç½®"""
    enabled: bool = True               # æ˜¯å¦å¯ç”¨ç›‘ç£æ•°æ®
    data_points: int = 0               # ç›‘ç£æ•°æ®ç‚¹æ•°é‡ï¼Œ0è¡¨ç¤ºä¸ä½¿ç”¨
    data_path: str = "data/cavity_Re5000_256_Uniform.mat"  # æ•°æ®æ–‡ä»¶è·¯å¾„
    weight: float = 1.0                # ç›‘ç£æ•°æ®æƒé‡
    random_seed: int = 42              # éšæœºç§å­ï¼Œç¡®ä¿å¯é‡ç°æ€§

@dataclass
class SystemConfig:
    """ç³»çµ±é…ç½®"""
    device: str = "auto"               # è¨­å‚™é¸æ“‡ (auto/cpu/cuda)
    precision: str = "float32"         # ç²¾åº¦è¨­ç½®
    tensorboard_enabled: bool = True   # TensorBoardå•Ÿç”¨
    tensorboard_interval: int = 1000   # TensorBoard å¯«å…¥é–“éš”ï¼ˆé™é »I/Oï¼‰
    timing_sync_interval: int = 1000   # GPU åŒæ­¥/æ™‚é–“ä¼°ç®—é–“éš”ï¼ˆé™é »åŒæ­¥ï¼‰
    ddp_broadcast_buffers: bool = False # DDP æ˜¯å¦åŒæ­¥ buffersï¼ˆç„¡BNå»ºè­°é—œé–‰ï¼‰
    log_level: str = "INFO"            # æ—¥èªŒç­‰ç´š
    memory_limit_gb: float = 14.0      # GPUè¨˜æ†¶é«”é™åˆ¶(GB)
    
    # æ€§èƒ½å„ªåŒ–é…ç½®
    gradient_clip_norm: float = 1.0    # æ¢¯åº¦è£å‰ª
    memory_cleanup_freq: int = 100     # è¨˜æ†¶é«”æ¸…ç†é »ç‡
    epoch_times_limit: int = 1000      # epochæ™‚é–“è¨˜éŒ„é™åˆ¶
    # ç›£æ¸¬é »ç‡ï¼ˆæ¢¯åº¦åˆ†ä½ˆã€EVMä¸Šé™å‘½ä¸­ç‡ç­‰ï¼‰
    monitor_interval: int = 1000       # æ¯å¤šå°‘æ­¥åŸ·è¡Œä¸€æ¬¡ç›£æ¸¬

@dataclass
class ExperimentConfig:
    """å®Œæ•´å¯¦é©—é…ç½®"""
    experiment_name: str = "NSFnet_Re5000"
    description: str = "Physics-Informed Neural Network for Lid-Driven Cavity Flow"
    
    # å­é…ç½®
    network: Optional[NetworkConfig] = None
    training: Optional[TrainingConfig] = None  
    physics: Optional[PhysicsConfig] = None
    supervision: Optional[SupervisionConfig] = None
    system: Optional[SystemConfig] = None
    
    def __post_init__(self):
        # åˆå§‹åŒ–å­é…ç½®
        if self.network is None:
            self.network = NetworkConfig()
        if self.training is None:
            self.training = TrainingConfig()
        if self.physics is None:
            self.physics = PhysicsConfig()
        if self.supervision is None:
            self.supervision = SupervisionConfig()
        if self.system is None:
            self.system = SystemConfig()

class ConfigManager:
    """é…ç½®ç®¡ç†å™¨"""
    
    def __init__(self):
        self.config = ExperimentConfig()
        
    @classmethod
    def from_file(cls, config_path: str) -> 'ConfigManager':
        """å¾é…ç½®æ–‡ä»¶è¼‰å…¥"""
        manager = cls()
        
        if config_path.endswith('.yaml') or config_path.endswith('.yml'):
            with open(config_path, 'r', encoding='utf-8') as f:
                config_dict = yaml.safe_load(f)
        elif config_path.endswith('.json'):
            with open(config_path, 'r', encoding='utf-8') as f:
                config_dict = json.load(f)
        else:
            raise ValueError(f"Unsupported config file format: {config_path}")
        
        manager.load_from_dict(config_dict)
        return manager
    
    def load_from_dict(self, config_dict: Dict[str, Any]):
        """å¾å­—å…¸è¼‰å…¥é…ç½®"""
        # æ­£è¦åŒ– training_stagesï¼Œæ”¯æ´ [alpha, epochs, lr, scheduler]
        if 'training' in config_dict and 'training_stages' in config_dict['training']:
            processed_stages = []
            for stage in config_dict['training']['training_stages']:
                alpha = float(stage[0])
                epochs = int(stage[1])
                lr = float(stage[2])
                sched = stage[3] if len(stage) > 3 else 'Constant'
                processed_stages.append((alpha, epochs, lr, str(sched)))
            config_dict['training']['training_stages'] = processed_stages
        
        # æ›´æ–°å„å€‹å­é…ç½®
        if 'network' in config_dict:
            self.config.network = NetworkConfig(**config_dict['network'])
        if 'training' in config_dict:
            training_config = config_dict['training'].copy()
            
            # è™•ç†L-BFGSé…ç½®
            if 'lbfgs' in training_config:
                lbfgs_config = LBFGSConfig(**training_config['lbfgs'])
                training_config.pop('lbfgs')
                self.config.training = TrainingConfig(**training_config)
                self.config.training.lbfgs = lbfgs_config
            else:
                self.config.training = TrainingConfig(**training_config)
        if 'physics' in config_dict:
            self.config.physics = PhysicsConfig(**config_dict['physics'])
        if 'supervision' in config_dict:
            self.config.supervision = SupervisionConfig(**config_dict['supervision'])
        if 'system' in config_dict:
            self.config.system = SystemConfig(**config_dict['system'])
            
        # æ›´æ–°ä¸»é…ç½®
        for key in ['experiment_name', 'description']:
            if key in config_dict:
                setattr(self.config, key, config_dict[key])
    
    def save_to_file(self, config_path: str):
        """ä¿å­˜é…ç½®åˆ°æ–‡ä»¶"""
        config_dict = asdict(self.config)
        
        # å°‡tupleè½‰æ›ç‚ºlist (YAMLåºåˆ—åŒ–å…¼å®¹æ€§)
        if 'training' in config_dict and 'training_stages' in config_dict['training']:
            config_dict['training']['training_stages'] = [
                list(stage) for stage in config_dict['training']['training_stages']
            ]
        
        os.makedirs(os.path.dirname(config_path), exist_ok=True)
        
        if config_path.endswith('.yaml') or config_path.endswith('.yml'):
            with open(config_path, 'w', encoding='utf-8') as f:
                yaml.dump(config_dict, f, default_flow_style=False, 
                         allow_unicode=True, indent=2)
        elif config_path.endswith('.json'):
            with open(config_path, 'w', encoding='utf-8') as f:
                json.dump(config_dict, f, indent=2, ensure_ascii=False)
        else:
            raise ValueError(f"Unsupported config file format: {config_path}")
    
    def get_device(self) -> torch.device:
        """ç²å–è¨­å‚™é…ç½®"""
        if self.config.system.device == "auto":
            if torch.cuda.is_available():
                return torch.device("cuda")
            else:
                return torch.device("cpu")
        else:
            return torch.device(self.config.system.device)
    
    def get_precision_dtype(self) -> torch.dtype:
        """ç²å–ç²¾åº¦é…ç½®"""
        if self.config.system.precision == "float64":
            return torch.float64
        elif self.config.system.precision == "float16":
            return torch.float16
        else:
            return torch.float32
    
    def validate_config(self) -> List[str]:
        """é©—è­‰é…ç½®åˆæ³•æ€§"""
        warnings = []
        
        # æª¢æŸ¥ç¶²è·¯é…ç½®
        if self.config.network.layers < 1:
            warnings.append("Network layers should be >= 1")
        if self.config.network.hidden_size < 1:
            warnings.append("Hidden size should be >= 1")
            
        # æª¢æŸ¥è¨“ç·´é…ç½®  
        if self.config.training.N_f <= 0:
            warnings.append("N_f should be > 0")
        if self.config.training.checkpoint_freq <= 0:
            warnings.append("Checkpoint frequency should be > 0")
            
        # æª¢æŸ¥ç‰©ç†é…ç½®
        if self.config.physics.Re <= 0:
            warnings.append("Reynolds number should be > 0")
        if self.config.physics.alpha_evm <= 0:
            warnings.append("Alpha EVM should be > 0")
            
        return warnings
    
    def print_config(self):
        """æ‰“å°é…ç½®æ‘˜è¦"""
        print("=" * 60)
        print(f"ğŸ”§ å¯¦é©—é…ç½®: {self.config.experiment_name}")
        print(f"ğŸ“ æè¿°: {self.config.description}")
        print("=" * 60)
        
        print(f"ğŸ§  ç¶²è·¯æ¶æ§‹:")
        print(f"   ä¸»ç¶²è·¯: {self.config.network.layers} å±¤ Ã— {self.config.network.hidden_size} ç¥ç¶“å…ƒ")
        print(f"   EVMç¶²è·¯: {self.config.network.layers_1} å±¤ Ã— {self.config.network.hidden_size_1} ç¥ç¶“å…ƒ")
        
        print(f"ğŸ¯ è¨“ç·´è¨­å®š:")
        print(f"   æ–¹ç¨‹é»æ•¸: {self.config.training.N_f:,}")
        print(f"   æ‰¹æ¬¡å¤§å°: {'å…¨æ‰¹æ¬¡' if self.config.training.batch_size is None else self.config.training.batch_size}")
        print(f"   ä¾è·é›¢æ’åº: {'æ˜¯' if self.config.training.sort_by_boundary_distance else 'å¦'}")
        print(f"   PDEè·é›¢æ¬Šé‡: {'å•Ÿç”¨' if self.config.training.pde_distance_weighting else 'é—œé–‰'} (w_min={self.config.training.pde_distance_w_min}, tau={self.config.training.pde_distance_tau})")
        print(f"   ç¸½éšæ®µæ•¸: {len(self.config.training.training_stages)}")
        # é¡¯ç¤º weight decay ç­–ç•¥
        if self.config.training.weight_decay_stages is not None:
            print(f"   Weight Decay(åˆ†éšæ®µ): {self.config.training.weight_decay_stages}")
        else:
            print(f"   Weight Decay(å…¨åŸŸ): {self.config.training.weight_decay}")
        for i, st in enumerate(self.config.training.training_stages):
            try:
                a,e,l,s = st
            except Exception:
                a,e,l = st[:3]
                s = 'Constant'
            if self.config.training.weight_decay_stages is not None:
                wd_val = self.config.training.weight_decay_stages[i]
            else:
                wd_val = self.config.training.weight_decay
            print(f"   - Stage {i+1}: alpha={a}, epochs={e}, lr={l}, wd={wd_val}, sched={s}")
        
        print(f"âš¡ ç‰©ç†åƒæ•¸:")
        print(f"   Reynoldsæ•¸: {self.config.physics.Re}")
        print(f"   åˆå§‹Î±_EVM: {self.config.physics.alpha_evm}")
        
        print(f"ğŸ’» ç³»çµ±é…ç½®:")
        print(f"   è¨­å‚™: {self.config.system.device}")
        print(f"   ç²¾åº¦: {self.config.system.precision}")
        print(f"   TensorBoard: {'å•Ÿç”¨' if self.config.system.tensorboard_enabled else 'é—œé–‰'}")
        print("=" * 60)

# é è¨­é…ç½®å¯¦ä¾‹
default_config = ConfigManager()

# é«˜æ€§èƒ½é…ç½® (é©ç”¨æ–¼æœå‹™å™¨)
def get_server_config() -> ConfigManager:
    """ç²å–æœå‹™å™¨é«˜æ€§èƒ½é…ç½®"""
    config = ConfigManager()
    
    # èª¿æ•´ç‚ºé«˜æ€§èƒ½è¨­ç½®
    config.config.training.N_f = 120000
    config.config.system.memory_limit_gb = 14.0
    config.config.system.tensorboard_enabled = True
    
    return config

# æ¸¬è©¦é…ç½® (é©ç”¨æ–¼å¿«é€Ÿæ¸¬è©¦)
def get_test_config() -> ConfigManager:
    """ç²å–æ¸¬è©¦é…ç½®"""
    config = ConfigManager()
    
    # èª¿æ•´ç‚ºå¿«é€Ÿæ¸¬è©¦è¨­ç½®
    config.config.training.N_f = 1000
    config.config.training.training_stages = [
        (0.05, 10, 1e-3),
        (0.03, 10, 5e-4)
    ]
    config.config.system.memory_limit_gb = 2.0
    
    return config
