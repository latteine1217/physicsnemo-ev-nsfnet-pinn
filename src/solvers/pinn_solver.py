"""
PINNæ±‚è§£å™¨æ ¸å¿ƒæ¨¡çµ„

æ•´åˆConfigManagerã€FCNetã€ç‰©ç†æ–¹ç¨‹ï¼Œå¯¦ç¾å®Œæ•´çš„PINNè¨“ç·´æµç¨‹
åŸºæ–¼ev-NSFnetçš„æˆç†Ÿå¯¦ç¾
"""

import os
import torch
import torch.nn as nn
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.tensorboard import SummaryWriter
import numpy as np
import time
import datetime
from typing import Dict, List, Optional, Tuple, Any, Union
import warnings

from ..config.config_manager import ConfigManager
from ..models.networks import FCNet
from ..models.activations import LAAFActivation
from ..physics.equations import PhysicsEquations
from ..utils.device_utils import setup_device
from ..utils.logger import LoggerFactory, PINNLogger


warnings.filterwarnings("ignore", message=".*c10d::allreduce_.*autograd kernel.*")


class PINNSolver:
    """
    Physics-Informed Neural Network æ±‚è§£å™¨
    
    æ•´åˆæ‰€æœ‰PINNçµ„ä»¶ï¼š
    - ä¸»ç¶²è·¯ï¼ˆu, v, pé æ¸¬ï¼‰
    - EVMç¶²è·¯ï¼ˆentropyé æ¸¬ï¼‰  
    - ç‰©ç†æ–¹ç¨‹è¨ˆç®—
    - è¨“ç·´å¾ªç’°å’Œå„ªåŒ–
    """
    
    def __init__(self, config_path: str):
        """
        åˆå§‹åŒ–PINNæ±‚è§£å™¨
        
        Args:
            config_path: é…ç½®æª”æ¡ˆè·¯å¾‘
        """
        # è¼‰å…¥é…ç½®
        self.config = ConfigManager.load_config(config_path)
        
        # åˆ†æ•£å¼è¨“ç·´è¨­å®š
        self.rank = int(os.environ.get('RANK', 0))
        self.local_rank = int(os.environ.get('LOCAL_RANK', 0))
        self.world_size = int(os.environ.get('WORLD_SIZE', 1))
        
        # åˆå§‹åŒ–logger
        self.logger = LoggerFactory.get_logger(
            name=f"PINN_Re{self.config.physics.reynolds_number}",
            level="INFO",
            rank=self.rank
        )
        
        # è¨­å‚™è¨­å®š
        self.device = setup_device(self.local_rank, self.logger)
        
        # åŸºæœ¬åƒæ•¸
        self.Re = self.config.physics.reynolds_number
        self.alpha_evm = self.config.physics.alpha_evm
        self.beta = getattr(self.config.physics, 'beta', 1.0)
        
        # ç¶²è·¯æ¶æ§‹åƒæ•¸
        self.layers = self.config.network.layers
        self.hidden_size = self.config.network.hidden_size
        self.layers_1 = self.config.network.layers_1
        self.hidden_size_1 = self.config.network.hidden_size_1
        
        # è¨“ç·´åƒæ•¸
        self.N_f = self.config.training.N_f
        self.batch_size = getattr(self.config.training, 'batch_size', self.N_f)
        self.checkpoint_freq = getattr(self.config.training, 'checkpoint_freq', 2000)
        
        # æå¤±æ¬Šé‡
        self.alpha_b = getattr(self.config.training, 'bc_weight', 10.0)
        self.alpha_e = getattr(self.config.training, 'eq_weight', 1.0)
        self.alpha_i = getattr(self.config.training, 'ic_weight', 0.1)
        self.alpha_o = getattr(self.config.training, 'outlet_weight', 1.0)
        self.alpha_s = getattr(self.config.training, 'supervised_data_weight', 1.0)
        
        # åˆå§‹åŒ–æå¤±
        self.loss_i = self.loss_o = self.loss_b = self.loss_e = self.loss_s = 0.0
        
        # TensorBoardè¨­å®š
        self._setup_tensorboard()
        
        # æ™‚é–“è¿½è¹¤
        self.epoch_start_time = None
        self.epoch_times = []
        self.stage_start_time = None
        self.training_start_time = None
        self.global_step_offset = 0
        
        # ç•¶å‰éšæ®µ
        self.current_stage = ' '
        
        # åˆå§‹åŒ–ç¥ç¶“ç¶²è·¯
        self._initialize_networks()
        
        # åˆå§‹åŒ–ç‰©ç†æ–¹ç¨‹
        self.physics_equations = PhysicsEquations(
            reynolds_number=self.Re,
            alpha_evm=self.alpha_evm,
            beta=self.beta
        )
        
        # è¨“ç·´æ•¸æ“šï¼ˆå°‡åœ¨load_training_dataä¸­è¨­å®šï¼‰
        self.x_f = self.y_f = None  # æ–¹ç¨‹é»
        self.x_b = self.y_b = self.u_b = self.v_b = None  # é‚Šç•Œæ¢ä»¶é»
        self.x_i = self.y_i = self.u_i = self.v_i = None  # åˆå§‹æ¢ä»¶é»
        self.x_o = self.y_o = None  # å‡ºå£é»
        
        # å„ªåŒ–å™¨
        self.opt = None
        self.opt_1 = None
        
        # è¼¸å‡ºåˆå§‹åŒ–è¨Šæ¯
        self._log_initialization_info()
    
    def _setup_tensorboard(self):
        """è¨­å®šTensorBoard"""
        tb_enabled = getattr(self.config.system, 'tensorboard_enabled', True)
        self.tb_interval = getattr(self.config.system, 'tensorboard_interval', 1000)
        
        if self.rank == 0 and tb_enabled:
            log_dir = f"runs/PINN_Re{self.Re}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}"
            self.tb_writer = SummaryWriter(log_dir=log_dir)
            self.logger.info(f"ğŸ“Š TensorBoard log directory: {log_dir} (interval={self.tb_interval})")
        else:
            self.tb_writer = None
    
    def _initialize_networks(self):
        """åˆå§‹åŒ–ç¥ç¶“ç¶²è·¯"""
        # ä¸»ç¶²è·¯ï¼ˆu, v, pï¼‰
        self.net = self._create_network(
            num_ins=2, num_outs=3, 
            num_layers=self.layers, 
            hidden_size=self.hidden_size,
            is_evm=False
        ).to(self.device)
        
        # EVMç¶²è·¯ï¼ˆentropyï¼‰
        self.net_1 = self._create_network(
            num_ins=2, num_outs=1,
            num_layers=self.layers_1,
            hidden_size=self.hidden_size_1,
            is_evm=True
        ).to(self.device)
        
        # ç¢ºä¿float32ç²¾åº¦
        self.net = self.net.float()
        self.net_1 = self.net_1.float()
        
        # å¥—ç”¨é…ç½®å¾Œè™•ç†
        self._apply_config_post_init()
        
        # DDPåŒ…è£ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if self.world_size > 1:
            ddp_broadcast_buffers = getattr(self.config.system, 'ddp_broadcast_buffers', False)
            
            self.net = DDP(
                self.net,
                device_ids=[self.local_rank],
                output_device=self.local_rank,
                find_unused_parameters=False,
                broadcast_buffers=ddp_broadcast_buffers,
                gradient_as_bucket_view=True
            )
            
            self.net_1 = DDP(
                self.net_1,
                device_ids=[self.local_rank],
                output_device=self.local_rank,
                find_unused_parameters=False,
                broadcast_buffers=ddp_broadcast_buffers,
                gradient_as_bucket_view=True
            )
    
    def _create_network(self, num_ins: int, num_outs: int, num_layers: int, 
                       hidden_size: int, is_evm: bool = False) -> FCNet:
        """å‰µå»ºç¥ç¶“ç¶²è·¯"""
        # é¸æ“‡æ¿€æ´»å‡½æ•¸
        if is_evm:
            activation_name = getattr(self.config.network, 'activation_evm', 'tanh').lower()
        else:
            activation_name = getattr(self.config.network, 'activation_main', 'laaf').lower()
        
        if activation_name == 'laaf':
            init_scale = getattr(self.config.network, 'laaf_init_scale', 1.0)
            max_scale = getattr(self.config.network, 'laaf_max_scale', 20.0)
            
            def activation_factory():
                return LAAFActivation(init_scale=init_scale, max_scale=max_scale)
        else:
            activation_factory = torch.nn.Tanh
        
        return FCNet(
            num_ins=num_ins,
            num_outs=num_outs,
            num_layers=num_layers,
            hidden_size=hidden_size,
            activation=activation_factory
        )
    
    def _apply_config_post_init(self):
        """å¥—ç”¨é…ç½®ç›¸é—œçš„å¾Œè™•ç†"""
        # å±¤æ¬Šé‡ç¸®æ”¾
        main_first = getattr(self.config.network, 'first_layer_scale_main', 2.0)
        main_last = getattr(self.config.network, 'last_layer_scale_main', 0.5)
        evm_first = getattr(self.config.network, 'first_layer_scale_evm', 1.2)
        evm_last = getattr(self.config.network, 'last_layer_scale_evm', 0.1)
        
        self._apply_layer_scales(self._get_model(self.net), main_first, main_last)
        self._apply_layer_scales(self._get_model(self.net_1), evm_first, evm_last)
    
    def _apply_layer_scales(self, model: nn.Module, first_scale: float, last_scale: float):
        """å¥—ç”¨å±¤æ¬Šé‡ç¸®æ”¾"""
        linear_layers = [m for m in model.modules() if isinstance(m, nn.Linear)]
        if not linear_layers:
            return
        
        # é¦–å±¤ç¸®æ”¾
        linear_layers[0].weight.data.mul_(float(first_scale))
        # æœ«å±¤ç¸®æ”¾
        linear_layers[-1].weight.data.mul_(float(last_scale))
    
    def _get_model(self, model: Union[FCNet, DDP]) -> FCNet:
        """ç²å–åº•å±¤æ¨¡å‹ï¼ˆè™•ç†DDPåŒ…è£ï¼‰"""
        if isinstance(model, DDP):
            return model.module
        else:
            return model
    
    def _log_initialization_info(self):
        """è¨˜éŒ„åˆå§‹åŒ–è¨Šæ¯"""
        config_info = {
            "Reynoldsæ•¸": self.Re,
            "ä¸»ç¶²è·¯": f"{self.layers} å±¤ Ã— {self.hidden_size} ç¥ç¶“å…ƒ",
            "EVMç¶²è·¯": f"{self.layers_1} å±¤ Ã— {self.hidden_size_1} ç¥ç¶“å…ƒ",
            "è¨“ç·´é»æ•¸": f"{self.N_f:,}",
            "è¨­å‚™": str(self.device),
            "æ‰¹æ¬¡å¤§å°": "å…¨æ‰¹æ¬¡" if self.batch_size == self.N_f else str(self.batch_size)
        }
        
        for key, value in config_info.items():
            self.logger.info(f"{key}: {value}")
    
    def load_training_data(self, x_f: torch.Tensor, y_f: torch.Tensor,
                          x_b: torch.Tensor, y_b: torch.Tensor,
                          u_b: torch.Tensor, v_b: torch.Tensor,
                          x_i: Optional[torch.Tensor] = None, y_i: Optional[torch.Tensor] = None,
                          u_i: Optional[torch.Tensor] = None, v_i: Optional[torch.Tensor] = None,
                          x_o: Optional[torch.Tensor] = None, y_o: Optional[torch.Tensor] = None):
        """
        è¼‰å…¥è¨“ç·´æ•¸æ“š
        
        Args:
            x_f, y_f: æ–¹ç¨‹é»åº§æ¨™
            x_b, y_b: é‚Šç•Œé»åº§æ¨™
            u_b, v_b: é‚Šç•Œæ¢ä»¶å€¼
            x_i, y_i: åˆå§‹æ¢ä»¶é»åº§æ¨™ï¼ˆå¯é¸ï¼‰
            u_i, v_i: åˆå§‹æ¢ä»¶å€¼ï¼ˆå¯é¸ï¼‰
            x_o, y_o: å‡ºå£é»åº§æ¨™ï¼ˆå¯é¸ï¼‰
        """
        # æ–¹ç¨‹é»
        self.x_f = x_f.to(self.device).requires_grad_(True)
        self.y_f = y_f.to(self.device).requires_grad_(True)
        
        # é‚Šç•Œæ¢ä»¶
        self.x_b = x_b.to(self.device).requires_grad_(True)
        self.y_b = y_b.to(self.device).requires_grad_(True)
        self.u_b = u_b.to(self.device)
        self.v_b = v_b.to(self.device)
        
        # åˆå§‹æ¢ä»¶ï¼ˆå¦‚æœæä¾›ï¼‰
        if x_i is not None:
            self.x_i = x_i.to(self.device).requires_grad_(True)
            self.y_i = y_i.to(self.device).requires_grad_(True)
            self.u_i = u_i.to(self.device)
            self.v_i = v_i.to(self.device)
        
        # å‡ºå£æ¢ä»¶ï¼ˆå¦‚æœæä¾›ï¼‰
        if x_o is not None:
            self.x_o = x_o.to(self.device).requires_grad_(True)
            self.y_o = y_o.to(self.device).requires_grad_(True)
        
        self.logger.info(f"å·²è¼‰å…¥è¨“ç·´æ•¸æ“š:")
        self.logger.info(f"  æ–¹ç¨‹é»: {self.x_f.shape[0]}")
        self.logger.info(f"  é‚Šç•Œé»: {self.x_b.shape[0]}")
        if self.x_i is not None:
            self.logger.info(f"  åˆå§‹é»: {self.x_i.shape[0]}")
        if self.x_o is not None:
            self.logger.info(f"  å‡ºå£é»: {self.x_o.shape[0]}")
    
    def neural_net_u(self, x: torch.Tensor, y: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        ç¥ç¶“ç¶²è·¯å‰å‘å‚³æ’­
        
        Args:
            x, y: è¼¸å…¥åº§æ¨™
            
        Returns:
            (u, v, p, e): é€Ÿåº¦ã€å£“åŠ›ã€entropyé æ¸¬
        """
        X = torch.cat((x, y), dim=1).to(self.device)
        
        with torch.enable_grad():
            uvp = self.net(X)
            ee = self.net_1(X)
        
        u = uvp[:, 0:1]
        v = uvp[:, 1:2] 
        p = uvp[:, 2:3]
        e = ee[:, 0:1]
        
        return u, v, p, e
    
    def compute_physics_loss(self) -> Tuple[torch.Tensor, List[torch.Tensor]]:
        """
        è¨ˆç®—ç‰©ç†æå¤±
        
        Returns:
            (total_loss, individual_losses): ç¸½æå¤±å’Œå„é …æå¤±åˆ—è¡¨
        """
        losses = []
        
        # é‚Šç•Œæ¢ä»¶æå¤±
        if self.x_b.shape[0] > 0:
            u_pred_b, v_pred_b, _, _ = self.neural_net_u(self.x_b, self.y_b)
            
            u_b_flat = self.u_b.view(-1)
            v_b_flat = self.v_b.view(-1)
            u_pred_b_flat = u_pred_b.view(-1)
            v_pred_b_flat = v_pred_b.view(-1)
            
            self.loss_b = torch.mean(torch.square(u_b_flat - u_pred_b_flat)) + \
                         torch.mean(torch.square(v_b_flat - v_pred_b_flat))
        else:
            self.loss_b = torch.tensor(0.0, device=self.device)
        
        # æ–¹ç¨‹æå¤±
        u_pred_f, v_pred_f, p_pred_f, e_pred_f = self.neural_net_u(self.x_f, self.y_f)
        
        eq1, eq2, eq3, eq4 = self.physics_equations.compute_physics_residuals(
            self.x_f, self.y_f, u_pred_f, v_pred_f, p_pred_f, e_pred_f
        )
        
        # å„æ–¹ç¨‹æå¤±
        self.loss_eq1 = torch.mean(torch.square(eq1))
        self.loss_eq2 = torch.mean(torch.square(eq2))
        self.loss_eq3 = torch.mean(torch.square(eq3))
        self.loss_eq4 = torch.mean(torch.square(eq4))
        
        self.loss_e = self.loss_eq1 + self.loss_eq2 + self.loss_eq3 + 0.1 * self.loss_eq4
        
        # åˆå§‹æ¢ä»¶æå¤±
        if self.x_i is not None and self.x_i.shape[0] > 0:
            u_pred_i, v_pred_i, _, _ = self.neural_net_u(self.x_i, self.y_i)
            self.loss_i = torch.mean(torch.square(self.u_i - u_pred_i)) + \
                         torch.mean(torch.square(self.v_i - v_pred_i))
        else:
            self.loss_i = torch.tensor(0.0, device=self.device)
        
        # å‡ºå£æ¢ä»¶æå¤±
        if self.x_o is not None and self.x_o.shape[0] > 0:
            _, _, p_pred_o, _ = self.neural_net_u(self.x_o, self.y_o)
            self.loss_o = torch.mean(torch.square(p_pred_o))  # å‡ºå£å£“åŠ›=0
        else:
            self.loss_o = torch.tensor(0.0, device=self.device)
        
        # ç›£ç£æ•¸æ“šæå¤±ï¼ˆæš«æ™‚è¨­ç‚º0ï¼‰
        self.loss_s = torch.tensor(0.0, device=self.device)
        
        # çµ„åˆæå¤±
        total_loss = (self.alpha_e * self.loss_e + 
                     self.alpha_b * self.loss_b + 
                     self.alpha_i * self.loss_i + 
                     self.alpha_o * self.loss_o + 
                     self.alpha_s * self.loss_s)
        
        # æå¤±åˆ—è¡¨
        losses = [
            self.loss_e,      # 0: æ–¹ç¨‹æå¤±
            self.loss_b,      # 1: é‚Šç•Œæå¤±
            self.loss_s,      # 2: ç›£ç£æå¤±
            self.loss_eq1,    # 3: Xå‹•é‡æ–¹ç¨‹
            self.loss_eq2,    # 4: Yå‹•é‡æ–¹ç¨‹
            self.loss_eq3,    # 5: é€£çºŒæ€§æ–¹ç¨‹
            self.loss_eq4     # 6: entropy residual
        ]
        
        return total_loss, losses
    
    def setup_optimizer(self, learning_rate: float = 1e-3, weight_decay: float = 0.0):
        """è¨­å®šå„ªåŒ–å™¨"""
        # ç²å–æ¨¡å‹åƒæ•¸
        main_params = list(self._get_model(self.net).parameters())
        evm_params = list(self._get_model(self.net_1).parameters())
        all_params = main_params + evm_params
        
        # å‰µå»ºAdamWå„ªåŒ–å™¨
        if weight_decay > 0:
            # åˆ†çµ„ï¼šæœ‰weight decayå’Œç„¡weight decay
            decay_params = []
            no_decay_params = []
            
            for param in all_params:
                if param.requires_grad:
                    if len(param.shape) >= 2:  # æ¬Šé‡çŸ©é™£
                        decay_params.append(param)
                    else:  # åç½®é …
                        no_decay_params.append(param)
            
            param_groups = [
                {'params': decay_params, 'weight_decay': weight_decay},
                {'params': no_decay_params, 'weight_decay': 0.0}
            ]
        else:
            param_groups = [{'params': all_params, 'weight_decay': 0.0}]
        
        self.opt = torch.optim.AdamW(param_groups, lr=learning_rate)
        
        # è¨­å®šåˆå§‹å­¸ç¿’ç‡
        for group in self.opt.param_groups:
            group['initial_lr'] = learning_rate
        
        self.logger.info(f"å·²è¨­å®šå„ªåŒ–å™¨: lr={learning_rate}, weight_decay={weight_decay}")
    
    def train_epoch(self) -> Tuple[float, List[float]]:
        """
        è¨“ç·´ä¸€å€‹epoch
        
        Returns:
            (loss_value, losses_list): æå¤±å€¼å’Œå„é …æå¤±åˆ—è¡¨
        """
        self.opt.zero_grad()
        
        # è¨ˆç®—æå¤±
        loss, losses = self.compute_physics_loss()
        
        # åå‘å‚³æ’­
        loss.backward()
        
        # æ¢¯åº¦è£å‰ªï¼ˆå¯é¸ï¼‰
        max_grad_norm = getattr(self.config.training, 'max_grad_norm', None)
        if max_grad_norm is not None:
            torch.nn.utils.clip_grad_norm_(
                list(self._get_model(self.net).parameters()) + 
                list(self._get_model(self.net_1).parameters()),
                max_grad_norm
            )
        
        # å„ªåŒ–å™¨æ­¥é€²
        self.opt.step()
        
        return loss.item(), [l.item() for l in losses]
    
    def log_tensorboard(self, epoch: int, loss_value: float, losses: List[float]):
        """è¨˜éŒ„TensorBoard"""
        if self.tb_writer is None or epoch % self.tb_interval != 0:
            return
        
        global_step = self.global_step_offset + epoch
        
        # ä¸»è¦æå¤±
        self.tb_writer.add_scalar('Loss/Total', loss_value, global_step)
        self.tb_writer.add_scalar('Loss/Equation_Combined', losses[0], global_step)
        self.tb_writer.add_scalar('Loss/Boundary', losses[1], global_step)
        self.tb_writer.add_scalar('Loss/Supervised', losses[2], global_step)
        self.tb_writer.add_scalar('Loss/Equation_NS_X', losses[3], global_step)
        self.tb_writer.add_scalar('Loss/Equation_NS_Y', losses[4], global_step)
        self.tb_writer.add_scalar('Loss/Equation_Continuity', losses[5], global_step)
        self.tb_writer.add_scalar('Loss/Equation_EntropyResidual', losses[6], global_step)
        
        # å­¸ç¿’ç‡
        self.tb_writer.add_scalar('Training/LearningRate', self.opt.param_groups[0]['lr'], global_step)
    
    def save_checkpoint(self, epoch: int, checkpoint_dir: str):
        """ä¿å­˜æª¢æŸ¥é»"""
        if self.rank != 0:
            return
        
        os.makedirs(checkpoint_dir, exist_ok=True)
        checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch}.pth')
        
        checkpoint = {
            'epoch': epoch,
            'net_state_dict': self._get_model(self.net).state_dict(),
            'net_1_state_dict': self._get_model(self.net_1).state_dict(),
            'optimizer_state_dict': self.opt.state_dict(),
            'Re': self.Re,
            'alpha_evm': self.alpha_evm,
            'current_stage': self.current_stage,
            'global_step_offset': self.global_step_offset
        }
        
        try:
            torch.save(checkpoint, checkpoint_path)
            self.logger.info(f"âœ… å·²ä¿å­˜æª¢æŸ¥é»: {checkpoint_path}")
        except Exception as e:
            self.logger.error(f"ä¿å­˜æª¢æŸ¥é»å¤±æ•— {checkpoint_path}: {e}")
    
    def update_evm_parameters(self, alpha_evm: float):
        """æ›´æ–°EVMåƒæ•¸"""
        self.alpha_evm = alpha_evm
        self.physics_equations.update_evm_parameters(alpha_evm)
        self.logger.info(f"å·²æ›´æ–°alpha_evm: {alpha_evm}")