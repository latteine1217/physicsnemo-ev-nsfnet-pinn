"""
ç¥ç¶“ç¶²è·¯æ¨¡çµ„ - FCNetå…¨é€£æ¥ç¶²è·¯

åŸºæ–¼ev-NSFnet/net.pyï¼Œé©é…æ–°çš„æ¨¡çµ„åŒ–æ¶æ§‹
æ”¯æ´å¤šç¨®æ¿€æ´»å‡½æ•¸å’Œéˆæ´»çš„ç¶²è·¯é…ç½®
"""

import torch
import torch.nn as nn
from collections import OrderedDict
from typing import List, Optional, Union, Type, Callable

class FCNet(nn.Module):
    """
    å…¨é€£æ¥ç¥ç¶“ç¶²è·¯
    
    æ”¯æ´éˆæ´»çš„å±¤é…ç½®å’Œå¤šç¨®æ¿€æ´»å‡½æ•¸
    """
    
    def __init__(
        self,
        num_inputs: int = 3,
        num_outputs: int = 3,
        num_layers: int = 6,
        hidden_size: int = 80,
        hidden_sizes: Optional[List[int]] = None,
        activation: Union[str, Type[nn.Module], Callable] = "tanh",
        initialization: str = "xavier"
    ):
        """
        åˆå§‹åŒ–FCNet
        
        Args:
            num_inputs: è¼¸å…¥ç¶­åº¦
            num_outputs: è¼¸å‡ºç¶­åº¦  
            num_layers: éš±è—å±¤æ•¸é‡
            hidden_size: éš±è—å±¤ç¥ç¶“å…ƒæ•¸é‡ï¼ˆç•¶hidden_sizesç‚ºNoneæ™‚ä½¿ç”¨ï¼‰
            hidden_sizes: æ¯å±¤ç¥ç¶“å…ƒæ•¸é‡åˆ—è¡¨ï¼Œè‹¥æä¾›å‰‡å„ªå…ˆä½¿ç”¨
            activation: æ¿€æ´»å‡½æ•¸ï¼Œæ”¯æ´å­—ä¸²æˆ–é¡åˆ¥
            initialization: æ¬Šé‡åˆå§‹åŒ–æ–¹æ³•
        """
        super(FCNet, self).__init__()
        
        # å»ºç«‹å±¤å°ºå¯¸åˆ—è¡¨
        if hidden_sizes is not None:
            if len(hidden_sizes) != num_layers:
                raise ValueError(f"hidden_sizesé•·åº¦({len(hidden_sizes)})å¿…é ˆç­‰æ–¼num_layers({num_layers})")
            layers = [num_inputs] + hidden_sizes + [num_outputs]
        else:
            layers = [num_inputs] + [hidden_size] * num_layers + [num_outputs]
        
        # ç¶²è·¯åƒæ•¸
        self.depth = len(layers) - 1
        self.layers_sizes = layers
        self.activation_name = activation if isinstance(activation, str) else activation.__name__
        self.initialization = initialization
        
        # è§£ææ¿€æ´»å‡½æ•¸
        self.activation_fn = self._parse_activation(activation)
        
        # å»ºç«‹ç¶²è·¯å±¤
        self.layers = self._build_layers(layers)
        
        # åˆå§‹åŒ–æ¬Šé‡
        self._initialize_weights()
    
    def _parse_activation(self, activation: Union[str, Type[nn.Module], Callable]) -> Type[nn.Module]:
        """è§£ææ¿€æ´»å‡½æ•¸"""
        if isinstance(activation, str):
            activation_map = {
                "tanh": nn.Tanh,
                "relu": nn.ReLU,
                "sigmoid": nn.Sigmoid,
                "leaky_relu": nn.LeakyReLU,
                "elu": nn.ELU,
                "gelu": nn.GELU,
                "silu": nn.SiLU,
                "laaf": None  # LAAFå°‡å¾activationsæ¨¡çµ„å°å…¥
            }
            
            if activation.lower() == "laaf":
                # å»¶é²å°å…¥LAAFé¿å…å¾ªç’°ä¾è³´
                print("è­¦å‘Š: LAAFæ¿€æ´»å‡½æ•¸éœ€è¦å–®ç¨å°å…¥ï¼Œå›é€€åˆ°Tanh")
                return nn.Tanh
            
            if activation.lower() in activation_map:
                return activation_map[activation.lower()]
            else:
                raise ValueError(f"ä¸æ”¯æ´çš„æ¿€æ´»å‡½æ•¸: {activation}")
        
        elif callable(activation):
            return activation
        else:
            raise ValueError(f"æ¿€æ´»å‡½æ•¸å¿…é ˆæ˜¯å­—ä¸²æˆ–å¯èª¿ç”¨ç‰©ä»¶: {type(activation)}")
    
    def _build_layers(self, layer_sizes: List[int]) -> nn.Sequential:
        """å»ºç«‹ç¶²è·¯å±¤"""
        layer_list = []
        
        # å»ºç«‹éš±è—å±¤
        for i in range(self.depth - 1):
            # ç·šæ€§å±¤
            linear_layer = nn.Linear(layer_sizes[i], layer_sizes[i + 1])
            layer_list.append((f'layer_{i}', linear_layer))
            
            # æ¿€æ´»å‡½æ•¸
            try:
                # å˜—è©¦ä¸å¸¶åƒæ•¸åˆå§‹åŒ–
                activation_layer = self.activation_fn()
            except TypeError:
                try:
                    # å˜—è©¦å¸¶å±¤å°ºå¯¸åƒæ•¸åˆå§‹åŒ–ï¼ˆç”¨æ–¼LAAFç­‰éœ€è¦åƒæ•¸çš„æ¿€æ´»å‡½æ•¸ï¼‰
                    activation_layer = self.activation_fn(layer_sizes[i + 1])
                except TypeError:
                    # å¦‚æœéƒ½å¤±æ•—ï¼Œå›é€€åˆ°Tanh
                    print(f"è­¦å‘Š: æ¿€æ´»å‡½æ•¸åˆå§‹åŒ–å¤±æ•—ï¼Œå›é€€åˆ°Tanh")
                    activation_layer = nn.Tanh()
            
            layer_list.append((f'activation_{i}', activation_layer))
        
        # è¼¸å‡ºå±¤ï¼ˆç„¡æ¿€æ´»å‡½æ•¸ï¼‰
        output_layer = nn.Linear(layer_sizes[-2], layer_sizes[-1])
        layer_list.append((f'layer_{self.depth - 1}', output_layer))
        
        return nn.Sequential(OrderedDict(layer_list))
    
    def _initialize_weights(self) -> None:
        """
        æ¬Šé‡åˆå§‹åŒ–
        
        æ”¯æ´Xavierã€Heã€Normalç­‰åˆå§‹åŒ–æ–¹æ³•
        """
        if self.initialization.lower() == "xavier":
            # Xavieråˆå§‹åŒ–ï¼Œé©åˆtanhæ¿€æ´»å‡½æ•¸
            gain = nn.init.calculate_gain('tanh')
            for module in self.modules():
                if isinstance(module, nn.Linear):
                    nn.init.xavier_uniform_(module.weight, gain=gain)
                    nn.init.zeros_(module.bias)
        
        elif self.initialization.lower() == "he":
            # Heåˆå§‹åŒ–ï¼Œé©åˆReLUæ¿€æ´»å‡½æ•¸
            for module in self.modules():
                if isinstance(module, nn.Linear):
                    nn.init.kaiming_uniform_(module.weight, nonlinearity='relu')
                    nn.init.zeros_(module.bias)
        
        elif self.initialization.lower() == "normal":
            # æ­£æ…‹åˆ†å¸ƒåˆå§‹åŒ–
            for module in self.modules():
                if isinstance(module, nn.Linear):
                    nn.init.normal_(module.weight, mean=0.0, std=0.1)
                    nn.init.zeros_(module.bias)
        
        else:
            print(f"è­¦å‘Š: æœªçŸ¥çš„åˆå§‹åŒ–æ–¹æ³• {self.initialization}ï¼Œä½¿ç”¨é è¨­åˆå§‹åŒ–")
    
    def apply_layer_scaling(
        self, 
        first_layer_scale: float = 1.0, 
        last_layer_scale: float = 1.0
    ) -> None:
        """
        æ‡‰ç”¨é¦–æœ«å±¤ç¸®æ”¾
        
        Args:
            first_layer_scale: é¦–å±¤æ¬Šé‡ç¸®æ”¾ä¿‚æ•¸
            last_layer_scale: æœ«å±¤æ¬Šé‡ç¸®æ”¾ä¿‚æ•¸
        """
        layers = [module for module in self.modules() if isinstance(module, nn.Linear)]
        
        if len(layers) > 0 and first_layer_scale != 1.0:
            # ç¸®æ”¾é¦–å±¤
            with torch.no_grad():
                layers[0].weight.data *= first_layer_scale
        
        if len(layers) > 1 and last_layer_scale != 1.0:
            # ç¸®æ”¾æœ«å±¤
            with torch.no_grad():
                layers[-1].weight.data *= last_layer_scale
    
    def get_activations(self, x: torch.Tensor) -> List[torch.Tensor]:
        """
        ç²å–æ‰€æœ‰å±¤çš„æ¿€æ´»å€¼ï¼ˆç”¨æ–¼åˆ†æå’Œèª¿è©¦ï¼‰
        
        Args:
            x: è¼¸å…¥å¼µé‡
            
        Returns:
            List[torch.Tensor]: å„å±¤æ¿€æ´»å€¼åˆ—è¡¨
        """
        activations = []
        current = x
        
        for i, layer in enumerate(self.layers):
            current = layer(current)
            activations.append(current.clone())
        
        return activations
    
    def count_parameters(self) -> int:
        """è¨ˆç®—ç¶²è·¯åƒæ•¸ç¸½æ•¸"""
        return sum(p.numel() for p in self.parameters() if p.requires_grad)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘å‚³æ’­
        
        Args:
            x: è¼¸å…¥å¼µé‡ [batch_size, num_inputs]
            
        Returns:
            torch.Tensor: è¼¸å‡ºå¼µé‡ [batch_size, num_outputs]
        """
        return self.layers(x)
    
    def __str__(self) -> str:
        """ç¶²è·¯çµæ§‹æ‘˜è¦"""
        param_count = self.count_parameters()
        return f"""
FCNetæ¶æ§‹æ‘˜è¦:
- å±¤æ•¸: {self.depth} ({self.depth-1}éš±è—å±¤ + 1è¼¸å‡ºå±¤)
- å±¤å°ºå¯¸: {' â†’ '.join(map(str, self.layers_sizes))}
- æ¿€æ´»å‡½æ•¸: {self.activation_name}
- åˆå§‹åŒ–æ–¹æ³•: {self.initialization}
- åƒæ•¸ç¸½æ•¸: {param_count:,}
"""


def create_pinn_networks(config) -> tuple[FCNet, FCNet]:
    """
    æ ¹æ“šé…ç½®å‰µå»ºPINNä¸»ç¶²è·¯å’ŒEVMç¶²è·¯
    
    Args:
        config: é…ç½®å°è±¡ï¼ŒåŒ…å«networké…ç½®
        
    Returns:
        tuple[FCNet, FCNet]: (ä¸»ç¶²è·¯, EVMç¶²è·¯)
    """
    # ä¸»ç¶²è·¯ (æ±‚è§£u, v, p)
    main_net = FCNet(
        num_inputs=3,  # (x, y, t)
        num_outputs=3,  # (u, v, p)
        num_layers=config.network.main_net_layers,
        hidden_size=config.network.main_net_hidden_size,
        activation=config.network.main_net_activation,
        initialization=config.network.main_net_initialization
    )
    
    # æ‡‰ç”¨é¦–æœ«å±¤ç¸®æ”¾
    main_net.apply_layer_scaling(
        config.network.main_net_first_layer_scale,
        config.network.main_net_last_layer_scale
    )
    
    # EVMç¶²è·¯ (è¨ˆç®—entropy residual)
    evm_net = FCNet(
        num_inputs=3,  # (x, y, t)
        num_outputs=1,  # entropy residual
        num_layers=config.network.evm_net_layers,
        hidden_size=config.network.evm_net_hidden_size,
        activation=config.network.evm_net_activation,
        initialization=config.network.main_net_initialization
    )
    
    # æ‡‰ç”¨é¦–æœ«å±¤ç¸®æ”¾
    evm_net.apply_layer_scaling(
        config.network.evm_net_first_layer_scale,
        config.network.evm_net_last_layer_scale
    )
    
    print(f"âœ… ç¶²è·¯å‰µå»ºå®Œæˆ:")
    print(f"ğŸ“Š ä¸»ç¶²è·¯: {main_net.count_parameters():,} åƒæ•¸")
    print(f"ğŸ“Š EVMç¶²è·¯: {evm_net.count_parameters():,} åƒæ•¸")
    print(f"ğŸ“Š ç¸½åƒæ•¸: {main_net.count_parameters() + evm_net.count_parameters():,}")
    
    return main_net, evm_net