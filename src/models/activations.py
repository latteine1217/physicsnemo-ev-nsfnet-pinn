# SPDX-FileCopyrightText: Copyright (c) 2024 LDC-PINNs
# SPDX-License-Identifier: Apache-2.0
#
# ==============================================================================
# é€²éšæ¿€æ´»å‡½æ•¸æ¨¡çµ„ - PhysicsNeMoæ•´åˆç‰ˆ
# Advanced Activation Functions for PhysicsNeMo Framework
# ==============================================================================

import torch
import torch.nn as nn
import numpy as np
from typing import Dict, List, Optional, Union


class TSAActivation(nn.Module):
    """
    Trainable Sinusoidal Activation (TSA) for PhysicsNeMo
    å¯è¨“ç·´æ­£å¼¦æ¿€æ´»å‡½æ•¸ï¼Œç”¨æ–¼å¢å¼·PINNsçš„æ”¶æ–‚æ€§å’Œç²¾åº¦
    """
    
    def __init__(self, 
                 num_neurons: int,
                 freq_std: float = 1.0,
                 freq_mean: float = 0.0,
                 trainable_coeffs: bool = True,
                 initialization: str = "uniform"):
        """
        åˆå§‹åŒ–TSAæ¿€æ´»å‡½æ•¸
        
        Args:
            num_neurons: ç¥ç¶“å…ƒæ•¸é‡
            freq_std: é »ç‡åˆå§‹åŒ–æ¨™æº–å·®
            freq_mean: é »ç‡åˆå§‹åŒ–å¹³å‡å€¼
            trainable_coeffs: æ˜¯å¦è¨“ç·´æ­£å¼¦/é¤˜å¼¦ä¿‚æ•¸
            initialization: åˆå§‹åŒ–æ–¹æ³• ("uniform", "normal", "xavier")
        """
        super().__init__()
        
        self.num_neurons = num_neurons
        
        # åˆå§‹åŒ–é »ç‡åƒæ•¸
        if initialization == "uniform":
            freq_init = torch.rand(num_neurons) * 2 * freq_std - freq_std + freq_mean
        elif initialization == "normal":
            freq_init = torch.normal(freq_mean, freq_std, (num_neurons,))
        elif initialization == "xavier":
            freq_init = torch.randn(num_neurons) * np.sqrt(2.0 / num_neurons)
        else:
            freq_init = torch.ones(num_neurons) * freq_mean
        
        self.freq = nn.Parameter(freq_init)
        
        # æ­£å¼¦å’Œé¤˜å¼¦ä¿‚æ•¸
        if trainable_coeffs:
            self.c1 = nn.Parameter(torch.ones(1))  # æ­£å¼¦ä¿‚æ•¸
            self.c2 = nn.Parameter(torch.ones(1))  # é¤˜å¼¦ä¿‚æ•¸
        else:
            self.register_buffer('c1', torch.ones(1))
            self.register_buffer('c2', torch.ones(1))
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """TSAå‰å‘å‚³æ’­: c1*sin(freq*x) + c2*cos(freq*x)"""
        if x.size(-1) != self.num_neurons:
            raise ValueError(f"è¼¸å…¥ç¶­åº¦ {x.size(-1)} != é æœŸç¶­åº¦ {self.num_neurons}")
        
        freq_x = self.freq * x
        return self.c1 * torch.sin(freq_x) + self.c2 * torch.cos(freq_x)
    
    def get_stats(self) -> Dict[str, float]:
        """å–å¾—é »ç‡çµ±è¨ˆè³‡è¨Š"""
        return {
            'freq_mean': self.freq.mean().item(),
            'freq_std': self.freq.std().item(),
            'freq_min': self.freq.min().item(),
            'freq_max': self.freq.max().item(),
            'c1': self.c1.item(),
            'c2': self.c2.item()
        }


class LAAFActivation(nn.Module):
    """
    Locally Adaptive Activation Function (LAAF)
    å±€éƒ¨é©æ‡‰æ€§æ¿€æ´»å‡½æ•¸ï¼Œç”¨æ–¼å¤šå°ºåº¦ç‰©ç†å•é¡Œ
    """
    
    def __init__(self, 
                 num_neurons: int, 
                 activation_type: str = "tanh",
                 alpha_init: float = 1.0):
        """
        åˆå§‹åŒ–LAAFæ¿€æ´»å‡½æ•¸
        
        Args:
            num_neurons: ç¥ç¶“å…ƒæ•¸é‡
            activation_type: åŸºç¤æ¿€æ´»å‡½æ•¸é¡å‹ ("tanh", "sin", "swish")
            alpha_init: å¯å­¸ç¿’åƒæ•¸åˆå§‹å€¼
        """
        super().__init__()
        
        self.num_neurons = num_neurons
        self.activation_type = activation_type
        
        # å¯å­¸ç¿’çš„èª¿ç¯€åƒæ•¸
        self.alpha = nn.Parameter(torch.ones(num_neurons) * alpha_init)
        
        # é¸æ“‡åŸºç¤æ¿€æ´»å‡½æ•¸
        if activation_type == "tanh":
            self.base_activation = torch.tanh
        elif activation_type == "sin":
            self.base_activation = torch.sin
        elif activation_type == "swish":
            self.base_activation = lambda x: x * torch.sigmoid(x)
        else:
            raise ValueError(f"ä¸æ”¯æ´çš„æ¿€æ´»å‡½æ•¸é¡å‹: {activation_type}")
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """LAAFå‰å‘å‚³æ’­"""
        return self.base_activation(self.alpha * x)
    
    def get_stats(self) -> Dict:
        """å–å¾—LAAFçµ±è¨ˆè³‡è¨Š"""
        return {
            'alpha_mean': self.alpha.mean().item(),
            'alpha_std': self.alpha.std().item(),
            'alpha_min': self.alpha.min().item(),
            'alpha_max': self.alpha.max().item(),
            'activation_type': self.activation_type
        }


class AdaptiveSinusoidalActivation(nn.Module):
    """
    Adaptive Sinusoidal Activation (ASA)
    è‡ªé©æ‡‰æ­£å¼¦æ¿€æ´»å‡½æ•¸ï¼ŒçµåˆTSAå’ŒLAAFçš„å„ªé»
    """
    
    def __init__(self, 
                 num_neurons: int,
                 freq_init: float = 1.0,
                 phase_trainable: bool = True):
        """
        åˆå§‹åŒ–ASAæ¿€æ´»å‡½æ•¸
        
        Args:
            num_neurons: ç¥ç¶“å…ƒæ•¸é‡
            freq_init: é »ç‡åˆå§‹å€¼
            phase_trainable: æ˜¯å¦è¨“ç·´ç›¸ä½åƒæ•¸
        """
        super().__init__()
        
        self.num_neurons = num_neurons
        
        # å¯å­¸ç¿’çš„é »ç‡å’ŒæŒ¯å¹…
        self.freq = nn.Parameter(torch.ones(num_neurons) * freq_init)
        self.amplitude = nn.Parameter(torch.ones(num_neurons))
        
        # å¯é¸çš„ç›¸ä½åƒæ•¸
        if phase_trainable:
            self.phase = nn.Parameter(torch.zeros(num_neurons))
        else:
            self.register_buffer('phase', torch.zeros(num_neurons))
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """ASAå‰å‘å‚³æ’­"""
        return self.amplitude * torch.sin(self.freq * x + self.phase)


def compute_activation_regularization(model: nn.Module, 
                                    weight: float = 0.01,
                                    activation_types: Optional[List] = None) -> torch.Tensor:
    """
    è¨ˆç®—æ¿€æ´»å‡½æ•¸çš„æ­£å‰‡åŒ–æå¤±
    
    Args:
        model: åŒ…å«é€²éšæ¿€æ´»å‡½æ•¸çš„æ¨¡å‹
        weight: æ­£å‰‡åŒ–æ¬Šé‡
        activation_types: è¦æ­£å‰‡åŒ–çš„æ¿€æ´»å‡½æ•¸é¡å‹åˆ—è¡¨
    
    Returns:
        æ­£å‰‡åŒ–æå¤±
    """
    if activation_types is None:
        activation_types = [TSAActivation, LAAFActivation, AdaptiveSinusoidalActivation]
    
    device = next(model.parameters()).device
    total_reg = torch.tensor(0.0, device=device)
    
    for module in model.modules():
        if any(isinstance(module, act_type) for act_type in activation_types):
            if isinstance(module, TSAActivation):
                # TSAæ­£å‰‡åŒ–ï¼šæ§åˆ¶é »ç‡ç¯„åœ
                freq_reg = torch.mean(torch.abs(module.freq))
                total_reg += freq_reg
            
            elif isinstance(module, LAAFActivation):
                # LAAFæ­£å‰‡åŒ–ï¼šé¿å…åƒæ•¸éå¤§
                alpha_reg = torch.mean(torch.abs(module.alpha - 1.0))
                total_reg += alpha_reg
            
            elif isinstance(module, AdaptiveSinusoidalActivation):
                # ASAæ­£å‰‡åŒ–ï¼šé »ç‡å’ŒæŒ¯å¹…å¹³è¡¡
                freq_reg = torch.mean(torch.abs(module.freq - 1.0))
                amp_reg = torch.mean(torch.abs(module.amplitude - 1.0))
                total_reg += freq_reg + amp_reg
    
    return weight * total_reg


def get_activation_function(activation_config: Dict) -> nn.Module:
    """
    æ ¹æ“šé…ç½®å‰µå»ºæ¿€æ´»å‡½æ•¸
    
    Args:
        activation_config: æ¿€æ´»å‡½æ•¸é…ç½®å­—å…¸
    
    Returns:
        æ¿€æ´»å‡½æ•¸æ¨¡çµ„
    """
    activation_type = activation_config.get('type', 'SiLU').lower()
    num_neurons = activation_config.get('num_neurons', 512)
    
    if activation_type == 'tsa':
        return TSAActivation(
            num_neurons=num_neurons,
            freq_std=activation_config.get('freq_std', 1.0),
            trainable_coeffs=activation_config.get('trainable_coeffs', True),
            initialization=activation_config.get('initialization', 'uniform')
        )
    
    elif activation_type == 'laaf':
        return LAAFActivation(
            num_neurons=num_neurons,
            activation_type=activation_config.get('base_type', 'tanh'),
            alpha_init=activation_config.get('alpha_init', 1.0)
        )
    
    elif activation_type == 'asa':
        return AdaptiveSinusoidalActivation(
            num_neurons=num_neurons,
            freq_init=activation_config.get('freq_init', 1.0),
            phase_trainable=activation_config.get('phase_trainable', True)
        )
    
    elif activation_type == 'silu' or activation_type == 'swish':
        return nn.SiLU()
    
    elif activation_type == 'tanh':
        return nn.Tanh()
    
    elif activation_type == 'relu':
        return nn.ReLU()
    
    elif activation_type == 'gelu':
        return nn.GELU()
    
    else:
        raise ValueError(f"ä¸æ”¯æ´çš„æ¿€æ´»å‡½æ•¸é¡å‹: {activation_type}")


class ActivationMonitor:
    """æ¿€æ´»å‡½æ•¸ç›£æ§å™¨ï¼Œç”¨æ–¼è¿½è¹¤è¨“ç·´éç¨‹ä¸­æ¿€æ´»å‡½æ•¸çš„è®ŠåŒ–"""
    
    def __init__(self):
        self.history = []
    
    def log_stats(self, model: nn.Module, epoch: int):
        """è¨˜éŒ„æ¿€æ´»å‡½æ•¸çµ±è¨ˆ"""
        stats = {
            'epoch': epoch,
            'activations': {}
        }
        
        for name, module in model.named_modules():
            if hasattr(module, 'get_stats'):
                stats['activations'][name] = module.get_stats()
        
        self.history.append(stats)
    
    def get_latest_stats(self) -> Dict:
        """å–å¾—æœ€æ–°çš„çµ±è¨ˆè³‡è¨Š"""
        return self.history[-1] if self.history else {}
    
    def print_summary(self):
        """åˆ—å°æ¿€æ´»å‡½æ•¸çµ±è¨ˆæ‘˜è¦"""
        if not self.history:
            print("âŒ ç„¡æ¿€æ´»å‡½æ•¸çµ±è¨ˆè³‡æ–™")
            return
        
        latest = self.history[-1]
        print(f"ğŸ“Š æ¿€æ´»å‡½æ•¸çµ±è¨ˆ (Epoch {latest['epoch']})")
        print("=" * 50)
        
        for name, stats in latest['activations'].items():
            print(f"ğŸ”§ {name}:")
            for key, value in stats.items():
                if isinstance(value, float):
                    print(f"   {key}: {value:.4f}")
                else:
                    print(f"   {key}: {value}")
            print()