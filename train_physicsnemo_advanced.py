# SPDX-FileCopyrightText: Copyright (c) 2024 LDC-PINNs
# SPDX-License-Identifier: Apache-2.0
#
# ==============================================================================
# å®Œæ•´PhysicsNeMoæ¡†æ¶å¯¦ä½œ - æ•´åˆäººå·¥ç²˜æ»¯æ€§èˆ‡å¤šéšæ®µè¨“ç·´
# Complete PhysicsNeMo Framework with Artificial Viscosity & Multi-stage Training
# ==============================================================================

import os
import hydra
import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.nn as nn
from torch.optim import Adam, LBFGS
from omegaconf import DictConfig
from typing import Dict, Tuple, Optional, Any, List

# PhysicsNeMo imports
from physicsnemo.distributed import DistributedManager
from physicsnemo.launch.logging import PythonLogger
from physicsnemo.models.mlp.fully_connected import FullyConnected
from physicsnemo.sym.geometry.geometry_dataloader import GeometryDatapipe
from physicsnemo.sym.geometry.primitives_2d import Rectangle

# æœ¬å°ˆæ¡ˆæ¨¡çµ„
from src.models.activations import get_activation_function, compute_activation_regularization
from src.physics.equations import EntropyViscosityMethod
from src.physics.nemo_sym import NemoSymEquations


class EntropyResidualNetwork(nn.Module):
    """
    ç†µæ®˜å·®ç¶²è·¯ - ç”¨æ–¼è¨ˆç®—äººå·¥ç²˜æ»¯æ€§çš„å‰¯ç¶²è·¯
    åŸºæ–¼ev-NSFnetçš„æ¶æ§‹ï¼š4å±¤ Ã— 40ç¥ç¶“å…ƒ
    """
    
    def __init__(self, 
                 input_dim: int = 2,
                 hidden_layers: int = 4,
                 hidden_size: int = 40,
                 activation_config: Optional[Dict] = None):
        super().__init__()
        
        layers = []
        prev_size = input_dim
        
        # å»ºæ§‹éš±è—å±¤
        for i in range(hidden_layers):
            layers.append(nn.Linear(prev_size, hidden_size))
            
            # ä½¿ç”¨é€²éšæ¿€æ´»å‡½æ•¸æˆ–æ¨™æº–æ¿€æ´»å‡½æ•¸
            if activation_config and activation_config.get('enabled', False):
                activation_config['num_neurons'] = hidden_size
                activation = get_activation_function(activation_config)
            else:
                activation = nn.Tanh()
            
            layers.append(activation)
            prev_size = hidden_size
        
        # è¼¸å‡ºå±¤ (entropy residual)
        layers.append(nn.Linear(prev_size, 1))
        
        self.network = nn.Sequential(*layers)
        
        # Xavieråˆå§‹åŒ–
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            nn.init.xavier_normal_(module.weight)
            if module.bias is not None:
                nn.init.zeros_(module.bias)
    
    def forward(self, coords: torch.Tensor) -> torch.Tensor:
        return self.network(coords)


class AdvancedFullyConnectedNetwork(nn.Module):
    """
    é€²éšå…¨é€£æ¥ç¶²è·¯ - æ•´åˆTSA/LAAFæ¿€æ´»å‡½æ•¸
    åŸºæ–¼ev-NSFnetçš„æ¶æ§‹ï¼š6å±¤ Ã— 80ç¥ç¶“å…ƒï¼ˆä¸»ç¶²è·¯ï¼‰
    """
    
    def __init__(self,
                 input_dim: int = 2,
                 output_dim: int = 3,
                 hidden_layers: int = 6,
                 hidden_size: int = 80,
                 activation_config: Optional[Dict] = None):
        super().__init__()
        
        layers = []
        prev_size = input_dim
        
        # å»ºæ§‹éš±è—å±¤
        for i in range(hidden_layers):
            layers.append(nn.Linear(prev_size, hidden_size))
            
            # ä½¿ç”¨é€²éšæ¿€æ´»å‡½æ•¸
            if activation_config and activation_config.get('enabled', False):
                activation_config['num_neurons'] = hidden_size
                activation = get_activation_function(activation_config)
            else:
                activation = nn.SiLU()  # é è¨­ä½¿ç”¨SiLU
            
            layers.append(activation)
            prev_size = hidden_size
        
        # è¼¸å‡ºå±¤
        layers.append(nn.Linear(prev_size, output_dim))
        
        self.network = nn.Sequential(*layers)
        
        # Xavieråˆå§‹åŒ–
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            nn.init.xavier_normal_(module.weight)
            if module.bias is not None:
                nn.init.zeros_(module.bias)
    
    def forward(self, coords: torch.Tensor) -> torch.Tensor:
        return self.network(coords)


class MultiStageTrainingManager:
    """
    å¤šéšæ®µè¨“ç·´ç®¡ç†å™¨ - å¯¦ç¾ev-NSFnetçš„5éšæ®µè¨“ç·´ç­–ç•¥
    """
    
    def __init__(self, config: DictConfig):
        self.total_epochs = config.training.max_epochs
        self.stages = config.training.stages
        self.current_stage = 0
        
        # é è¨­5éšæ®µé…ç½®ï¼ˆå¦‚æœé…ç½®ä¸­æœªæŒ‡å®šï¼‰
        if not hasattr(config.training, 'stages') or not config.training.stages:
            self.stages = self._get_default_stages()
    
    def _get_default_stages(self) -> List[Dict]:
        """å–å¾—é è¨­çš„5éšæ®µè¨“ç·´é…ç½®"""
        epochs_per_stage = self.total_epochs // 5
        
        return [
            {
                'name': 'Stage_1_Initial',
                'epochs': epochs_per_stage,
                'alpha_evm': 0.03,
                'learning_rate': 1e-3,
                'optimizer': 'Adam',
                'use_lbfgs': False
            },
            {
                'name': 'Stage_2_Refinement',
                'epochs': epochs_per_stage,
                'alpha_evm': 0.01,
                'learning_rate': 8e-4,
                'optimizer': 'Adam',
                'use_lbfgs': False
            },
            {
                'name': 'Stage_3_Mixed',
                'epochs': epochs_per_stage,
                'alpha_evm': 0.003,
                'learning_rate': 5e-4,
                'optimizer': 'Mixed',  # 60% Adam + 40% L-BFGS
                'use_lbfgs': True,
                'lbfgs_ratio': 0.4
            },
            {
                'name': 'Stage_4_Precision',
                'epochs': epochs_per_stage,
                'alpha_evm': 0.001,
                'learning_rate': 3e-4,
                'optimizer': 'Mixed',
                'use_lbfgs': True,
                'lbfgs_ratio': 0.4
            },
            {
                'name': 'Stage_5_Final',
                'epochs': epochs_per_stage,
                'alpha_evm': 0.0002,
                'learning_rate': 1e-4,
                'optimizer': 'Mixed',
                'use_lbfgs': True,
                'lbfgs_ratio': 0.5
            }
        ]
    
    def get_current_stage_config(self, epoch: int) -> Dict:
        """å–å¾—ç•¶å‰epochå°æ‡‰çš„éšæ®µé…ç½®"""
        cumulative_epochs = 0
        
        for i, stage in enumerate(self.stages):
            cumulative_epochs += stage['epochs']
            if epoch < cumulative_epochs:
                stage_config = stage.copy()
                stage_config['stage_index'] = i
                stage_config['stage_start_epoch'] = cumulative_epochs - stage['epochs']
                stage_config['stage_end_epoch'] = cumulative_epochs
                stage_config['stage_progress'] = (epoch - stage_config['stage_start_epoch']) / stage['epochs']
                return stage_config
        
        # å¦‚æœè¶…éæ‰€æœ‰éšæ®µï¼Œè¿”å›æœ€å¾Œä¸€å€‹éšæ®µ
        last_stage = self.stages[-1].copy()
        last_stage['stage_index'] = len(self.stages) - 1
        return last_stage
    
    def should_switch_to_lbfgs(self, stage_config: Dict) -> bool:
        """åˆ¤æ–·æ˜¯å¦æ‡‰è©²åˆ‡æ›åˆ°L-BFGSå„ªåŒ–å™¨"""
        if not stage_config.get('use_lbfgs', False):
            return False
        
        lbfgs_threshold = 1.0 - stage_config.get('lbfgs_ratio', 0.4)
        return stage_config['stage_progress'] >= lbfgs_threshold
    
    def print_stage_transition(self, stage_config: Dict):
        """åˆ—å°éšæ®µè½‰æ›è³‡è¨Š"""
        print(f"\n{'='*60}")
        print(f"ğŸš€ é€²å…¥è¨“ç·´éšæ®µ {stage_config['stage_index'] + 1}: {stage_config['name']}")
        print(f"ğŸ“Š Alpha EVM: {stage_config['alpha_evm']}")
        print(f"ğŸ“ˆ å­¸ç¿’ç‡: {stage_config['learning_rate']:.2e}")
        print(f"ğŸ”§ å„ªåŒ–å™¨: {stage_config['optimizer']}")
        if stage_config.get('use_lbfgs'):
            print(f"âš¡ L-BFGSæ¯”ä¾‹: {stage_config.get('lbfgs_ratio', 0.4):.1%}")
        print(f"ğŸ“… Epochs: {stage_config['stage_start_epoch']} - {stage_config['stage_end_epoch']}")
        print(f"{'='*60}")


class DualNetworkPINNSolver:
    """
    é›™ç¶²è·¯PINNæ±‚è§£å™¨
    ä¸»ç¶²è·¯: [u, v, p] - 6å±¤Ã—80ç¥ç¶“å…ƒ
    å‰¯ç¶²è·¯: [entropy_residual] - 4å±¤Ã—40ç¥ç¶“å…ƒ
    """
    
    def __init__(self, config: DictConfig, device: torch.device):
        self.config = config
        self.device = device
        
        # å»ºç«‹ä¸»/å‰¯ç¶²è·¯ï¼šå„ªå…ˆä½¿ç”¨PhysicsNeMo FullyConnectedï¼Œå¤±æ•—å‰‡å›é€€åˆ°æœ¬åœ°ç¶²è·¯
        try:
            self.main_network = FullyConnected(
                in_features=2,
                out_features=3,
                num_layers=config.model.main_network.num_layers,
                layer_size=config.model.main_network.layer_size,
            ).to(device)
        except Exception:
            self.main_network = AdvancedFullyConnectedNetwork(
                input_dim=2,
                output_dim=3,
                hidden_layers=config.model.main_network.num_layers,
                hidden_size=config.model.main_network.layer_size,
                activation_config=config.model.get('advanced_activation', None)
            ).to(device)

        try:
            self.entropy_network = FullyConnected(
                in_features=2,
                out_features=1,
                num_layers=config.model.entropy_network.num_layers,
                layer_size=config.model.entropy_network.layer_size,
            ).to(device)
        except Exception:
            self.entropy_network = EntropyResidualNetwork(
                input_dim=2,
                hidden_layers=config.model.entropy_network.num_layers,
                hidden_size=config.model.entropy_network.layer_size,
                activation_config=config.model.get('advanced_activation', None)
            ).to(device)
        
        # ç‰©ç†æ–¹ç¨‹å¼ï¼šå„ªå…ˆä½¿ç”¨PhysicsNeMo-Symå°è£ï¼ˆå«Informer+EVMï¼‰ï¼Œé¿å…ç ´å£åŸæœ‰è³‡æ–™æµ
        self.physics_equations = NemoSymEquations(
            reynolds_number=config.physics.Re,
            alpha_evm=config.physics.alpha_evm,
            beta=config.physics.get('beta', 1.0)
        )
        
        # å¤šéšæ®µè¨“ç·´ç®¡ç†å™¨
        self.training_manager = MultiStageTrainingManager(config)
        
        # å„ªåŒ–å™¨ï¼ˆå°‡åœ¨è¨“ç·´éç¨‹ä¸­å‹•æ…‹å‰µå»ºï¼‰
        self.main_optimizer = None
        self.entropy_optimizer = None
        self.lbfgs_optimizer = None
        self.current_stage_config = None
    
    def create_optimizers(self, stage_config: Dict):
        """æ ¹æ“šéšæ®µé…ç½®å‰µå»ºå„ªåŒ–å™¨"""
        lr = stage_config['learning_rate']
        
        # ä¸»ç¶²è·¯å„ªåŒ–å™¨
        self.main_optimizer = Adam(
            self.main_network.parameters(),
            lr=lr,
            weight_decay=self.config.training.get('weight_decay', 0.0)
        )
        
        # å‰¯ç¶²è·¯å„ªåŒ–å™¨
        self.entropy_optimizer = Adam(
            self.entropy_network.parameters(),
            lr=lr,
            weight_decay=self.config.training.get('weight_decay', 0.0)
        )
        
        # L-BFGSå„ªåŒ–å™¨ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if stage_config.get('use_lbfgs', False):
            # åˆä½µæ‰€æœ‰åƒæ•¸ç”¨æ–¼L-BFGS
            all_params = list(self.main_network.parameters()) + list(self.entropy_network.parameters())
            self.lbfgs_optimizer = LBFGS(
                all_params,
                lr=lr * 0.8,  # L-BFGSä½¿ç”¨è¼ƒå°çš„å­¸ç¿’ç‡
                max_iter=20,
                tolerance_grad=1e-7,
                tolerance_change=1e-9,
                history_size=100,
                line_search_fn='strong_wolfe'
            )
    
    def compute_physics_loss(self, coords: torch.Tensor) -> Dict[str, torch.Tensor]:
        """è¨ˆç®—ç‰©ç†æå¤±"""
        x, y = coords[:, 0:1], coords[:, 1:2]
        x.requires_grad_(True)
        y.requires_grad_(True)
        
        # ä¸»ç¶²è·¯å‰å‘å‚³æ’­
        main_output = self.main_network(coords)
        u, v, p = main_output[:, 0:1], main_output[:, 1:2], main_output[:, 2:3]
        
        # å‰¯ç¶²è·¯å‰å‘å‚³æ’­
        e_raw = self.entropy_network(coords)
        
        # è¨ˆç®—ç‰©ç†æ–¹ç¨‹æ®˜å·®
        eq1, eq2, eq3, eq4 = self.physics_equations.compute_residuals(x, y, u, v, p, e_raw)
        
        # è¨ˆç®—æå¤±
        losses = {
            'momentum_x': torch.mean(eq1 ** 2),
            'momentum_y': torch.mean(eq2 ** 2),
            'continuity': torch.mean(eq3 ** 2),
            'entropy_residual': torch.mean(eq4 ** 2)
        }
        
        return losses
    
    def compute_boundary_loss(self, bc_data: Dict) -> torch.Tensor:
        """è¨ˆç®—é‚Šç•Œæ¢ä»¶æå¤±"""
        cavity_size = self.config.physics.cavity_size
        
        coords = bc_data['coords']
        y_vals = coords[:, 1:2]
        
        # åˆ†é›¢ä¸åŒé‚Šç•Œ
        mask_no_slip = torch.abs(y_vals + cavity_size/2) < 1e-6  # bottom wall
        mask_no_slip = mask_no_slip | (torch.abs(coords[:, 0:1] + cavity_size/2) < 1e-6)  # left wall
        mask_no_slip = mask_no_slip | (torch.abs(coords[:, 0:1] - cavity_size/2) < 1e-6)  # right wall
        
        mask_moving_lid = torch.abs(y_vals - cavity_size/2) < 1e-6  # top wall
        
        # æ¨¡å‹æ¨ç†
        output = self.main_network(coords)
        u, v = output[:, 0:1], output[:, 1:2]
        
        # ç„¡æ»‘ç§»é‚Šç•Œæ¢ä»¶
        no_slip_loss = torch.tensor(0.0, device=self.device)
        if mask_no_slip.any():
            no_slip_coords = coords[mask_no_slip.squeeze()]
            if no_slip_coords.numel() > 0:
                no_slip_output = self.main_network(no_slip_coords)
                no_slip_u, no_slip_v = no_slip_output[:, 0:1], no_slip_output[:, 1:2]
                no_slip_loss = torch.mean(no_slip_u**2 + no_slip_v**2)
        
        # ç§»å‹•é ‚å£é‚Šç•Œæ¢ä»¶
        moving_lid_loss = torch.tensor(0.0, device=self.device)
        if mask_moving_lid.any():
            lid_coords = coords[mask_moving_lid.squeeze()]
            if lid_coords.numel() > 0:
                lid_output = self.main_network(lid_coords)
                lid_u, lid_v = lid_output[:, 0:1], lid_output[:, 1:2]
                
                # u = 1 (ç§»å‹•é€Ÿåº¦), v = 0
                lid_u_target = 1.0
                lid_v_target = 0.0
                moving_lid_loss = torch.mean((lid_u - lid_u_target)**2 + (lid_v - lid_v_target)**2)
        
        return no_slip_loss + moving_lid_loss
    
    def training_step(self, interior_data: Dict, boundary_data: Dict, 
                     stage_config: Dict, use_lbfgs: bool = False) -> Dict[str, float]:
        """å–®æ­¥è¨“ç·´"""
        
        def closure():
            """L-BFGS closureå‡½æ•¸"""
            if use_lbfgs and self.lbfgs_optimizer:
                self.lbfgs_optimizer.zero_grad()
            else:
                self.main_optimizer.zero_grad()
                self.entropy_optimizer.zero_grad()
            
            # è¨ˆç®—ç‰©ç†æå¤±
            physics_losses = self.compute_physics_loss(interior_data['coords'])
            
            # è¨ˆç®—é‚Šç•Œæå¤±
            boundary_loss = self.compute_boundary_loss(boundary_data)
            
            # æ¬Šé‡é…ç½®
            weights = self.config.loss_weights
            
            # ç¸½æå¤±
            total_loss = (
                weights.momentum_x * physics_losses['momentum_x'] +
                weights.momentum_y * physics_losses['momentum_y'] +
                weights.continuity * physics_losses['continuity'] +
                stage_config['alpha_evm'] * physics_losses['entropy_residual'] +
                weights.boundary * boundary_loss
            )
            
            # æ¿€æ´»å‡½æ•¸æ­£å‰‡åŒ–
            if self.config.model.get('advanced_activation', {}).get('enabled', False):
                activation_reg = compute_activation_regularization(self.main_network) + \
                               compute_activation_regularization(self.entropy_network)
                total_loss += 0.01 * activation_reg
            
            total_loss.backward()
            
            return total_loss
        
        # åŸ·è¡Œå„ªåŒ–æ­¥é©Ÿ
        if use_lbfgs and self.lbfgs_optimizer:
            loss = self.lbfgs_optimizer.step(closure)
        else:
            loss = closure()
            self.main_optimizer.step()
            self.entropy_optimizer.step()
        
        # è¿”å›æå¤±è³‡è¨Š
        return {
            'total_loss': loss.item(),
            'optimizer': 'L-BFGS' if use_lbfgs else 'Adam'
        }
    
    def update_stage(self, new_stage_config: Dict):
        """æ›´æ–°è¨“ç·´éšæ®µ"""
        if self.current_stage_config != new_stage_config:
            self.current_stage_config = new_stage_config
            
            # æ›´æ–°ç‰©ç†æ–¹ç¨‹åƒæ•¸
            self.physics_equations.update_evm_parameters(new_stage_config['alpha_evm'])
            
            # é‡æ–°å‰µå»ºå„ªåŒ–å™¨
            self.create_optimizers(new_stage_config)
            
            # åˆ—å°éšæ®µè½‰æ›è³‡è¨Š
            self.training_manager.print_stage_transition(new_stage_config)


def create_data_loaders(config: DictConfig, device: torch.device):
    """å‰µå»ºæ•¸æ“šè¼‰å…¥å™¨"""
    # å‰µå»ºå¹¾ä½•å°è±¡
    bounds = config.geometry.domain.bounds
    geometry = Rectangle(
        (bounds[0][0], bounds[0][1]),  # lower left
        (bounds[1][0], bounds[1][1])   # upper right
    )
    
    # é‚Šç•Œæ¢ä»¶æ•¸æ“šè¼‰å…¥å™¨
    bc_dataloader = GeometryDatapipe(
        geom_objects=[geometry],
        batch_size=1,
        num_points=config.geometry.sampling.boundary_points,
        sample_type="surface",
        device=device,
        num_workers=config.geometry.sampling.num_workers,
        requested_vars=["x", "y"]
    )
    
    # å…§éƒ¨é»æ•¸æ“šè¼‰å…¥å™¨
    interior_dataloader = GeometryDatapipe(
        geom_objects=[geometry],
        batch_size=1,
        num_points=config.geometry.sampling.interior_points,
        sample_type="volume",
        device=device,
        num_workers=config.geometry.sampling.num_workers,
        requested_vars=["x", "y", "sdf"]
    )
    
    return bc_dataloader, interior_dataloader


def plot_results(solver: DualNetworkPINNSolver, config: DictConfig, 
                epoch: int, loss_dict: Dict, output_dir: str):
    """ç¹ªè£½çµæœ"""
    if not config.outputs.save_plots or epoch % config.training.plot_freq != 0:
        return
    
    # å‰µå»ºæ¨ç†ç¶²æ ¼
    resolution = config.inference.grid_resolution
    domain = config.inference.domain
    
    x = np.linspace(domain[0][0], domain[0][1], resolution)
    y = np.linspace(domain[1][0], domain[1][1], resolution)
    xx, yy = np.meshgrid(x, y, indexing="xy")
    
    coords = torch.from_numpy(
        np.column_stack([xx.ravel(), yy.ravel()])
    ).float().to(solver.device)
    
    with torch.no_grad():
        # ä¸»ç¶²è·¯æ¨ç†
        main_output = solver.main_network(coords)
        u_pred = main_output[:, 0].cpu().numpy().reshape(resolution, resolution)
        v_pred = main_output[:, 1].cpu().numpy().reshape(resolution, resolution)
        p_pred = main_output[:, 2].cpu().numpy().reshape(resolution, resolution)
        
        # å‰¯ç¶²è·¯æ¨ç†
        e_pred = solver.entropy_network(coords).cpu().numpy().reshape(resolution, resolution)
    
    # å‰µå»ºåœ–åƒ
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    
    # é€Ÿåº¦å ´
    im1 = axes[0, 0].imshow(u_pred, origin="lower", extent=domain[0] + domain[1])
    axes[0, 0].set_title(f"u velocity (epoch {epoch})")
    fig.colorbar(im1, ax=axes[0, 0])
    
    im2 = axes[0, 1].imshow(v_pred, origin="lower", extent=domain[0] + domain[1])
    axes[0, 1].set_title(f"v velocity (epoch {epoch})")
    fig.colorbar(im2, ax=axes[0, 1])
    
    # å£“åŠ›å ´
    im3 = axes[0, 2].imshow(p_pred, origin="lower", extent=domain[0] + domain[1])
    axes[0, 2].set_title(f"Pressure (epoch {epoch})")
    fig.colorbar(im3, ax=axes[0, 2])
    
    # é€Ÿåº¦å¹…åº¦
    u_magnitude = np.sqrt(u_pred**2 + v_pred**2)
    im4 = axes[1, 0].imshow(u_magnitude, origin="lower", extent=domain[0] + domain[1])
    axes[1, 0].set_title(f"Velocity Magnitude (epoch {epoch})")
    fig.colorbar(im4, ax=axes[1, 0])
    
    # ç†µæ®˜å·®
    im5 = axes[1, 1].imshow(e_pred, origin="lower", extent=domain[0] + domain[1])
    axes[1, 1].set_title(f"Entropy Residual (epoch {epoch})")
    fig.colorbar(im5, ax=axes[1, 1])
    
    # æµç·šåœ–
    axes[1, 2].streamplot(xx, yy, u_pred, v_pred, density=2, color='k', linewidth=0.8)
    axes[1, 2].set_title(f"Streamlines (epoch {epoch})")
    axes[1, 2].set_xlim(domain[0])
    axes[1, 2].set_ylim(domain[1])
    
    # æ·»åŠ æ•´é«”æ¨™é¡Œ
    stage_info = solver.current_stage_config['name'] if solver.current_stage_config else "Training"
    fig.suptitle(f"LDC Flow (Re={config.physics.Re}) - {stage_info} - Loss: {loss_dict['total_loss']:.2e}", 
                fontsize=16)
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"ldc_results_epoch_{epoch:06d}.png"), 
                dpi=150, bbox_inches='tight')
    plt.close()


@hydra.main(version_base="1.3", config_path="configs", config_name="ldc_pinn_advanced")
def advanced_ldc_trainer(cfg: DictConfig) -> None:
    """é€²éšLDC-PINNè¨“ç·´å™¨ - å®Œæ•´PhysicsNeMoæ¡†æ¶"""
    
    # åˆå§‹åŒ–åˆ†å¸ƒå¼ç®¡ç†å™¨
    DistributedManager.initialize()
    dist = DistributedManager()
    
    # è¨­ç½®æ—¥èªŒ
    log = PythonLogger(name="advanced-ldc-physicsnemo")
    if cfg.logging.file_logging:
        log.file_logging()
    
    if dist.rank == 0:
        log.info("ğŸš€ å•Ÿå‹•é€²éšPhysicsNeMo LDC-PINNè¨“ç·´")
        log.info(f"ğŸ“Š è¨­å‚™: {dist.device}")
        log.info(f"ğŸ”§ Reynoldsæ•¸: {cfg.physics.Re}")
        log.info(f"ğŸ¯ ç¸½epochs: {cfg.training.max_epochs}")
        log.info(f"ğŸŒŸ ä½¿ç”¨é€²éšæ¿€æ´»å‡½æ•¸: {cfg.model.get('advanced_activation', {}).get('enabled', False)}")
        log.info(f"âš¡ å¤šéšæ®µè¨“ç·´: {len(cfg.training.get('stages', [])) > 0}")
    
    # å‰µå»ºè¼¸å‡ºç›®éŒ„
    output_dir = cfg.outputs.save_dir
    os.makedirs(output_dir, exist_ok=True)
    
    # å‰µå»ºæ±‚è§£å™¨
    solver = DualNetworkPINNSolver(cfg, dist.device)
    
    # å‰µå»ºæ•¸æ“šè¼‰å…¥å™¨
    bc_dataloader, interior_dataloader = create_data_loaders(cfg, dist.device)
    
    if dist.rank == 0:
        log.info("âœ… åˆå§‹åŒ–å®Œæˆï¼Œé–‹å§‹å¤šéšæ®µè¨“ç·´")
    
    # è¨“ç·´å¾ªç’°
    for epoch in range(cfg.training.max_epochs):
        # å–å¾—ç•¶å‰éšæ®µé…ç½®
        stage_config = solver.training_manager.get_current_stage_config(epoch)
        solver.update_stage(stage_config)
        
        # åˆ¤æ–·æ˜¯å¦ä½¿ç”¨L-BFGS
        use_lbfgs = solver.training_manager.should_switch_to_lbfgs(stage_config)
        
        # åŸ·è¡Œè¨“ç·´æ­¥é©Ÿ
        for bc_data, interior_data in zip(bc_dataloader, interior_dataloader):
            # æº–å‚™æ•¸æ“š
            bc_coords = torch.cat([bc_data[0]["x"].reshape(-1, 1), 
                                 bc_data[0]["y"].reshape(-1, 1)], dim=1)
            interior_coords = torch.cat([interior_data[0]["x"].reshape(-1, 1), 
                                       interior_data[0]["y"].reshape(-1, 1)], dim=1)
            
            boundary_data = {'coords': bc_coords}
            interior_data = {'coords': interior_coords}
            
            # åŸ·è¡Œè¨“ç·´æ­¥é©Ÿ
            loss_dict = solver.training_step(interior_data, boundary_data, stage_config, use_lbfgs)
            
            break  # æ¯å€‹epochåªè™•ç†ä¸€å€‹batch
        
        # æ—¥èªŒè¼¸å‡º
        if dist.rank == 0 and epoch % cfg.training.log_freq == 0:
            optimizer_info = f"({loss_dict['optimizer']})" if 'optimizer' in loss_dict else ""
            log.info(f"Epoch {epoch:6d}: Loss = {loss_dict['total_loss']:.2e} "
                    f"Stage = {stage_config['name']} {optimizer_info}")
        
        # ç¹ªè£½çµæœ
        if dist.rank == 0:
            plot_results(solver, cfg, epoch, loss_dict, output_dir)
    
    if dist.rank == 0:
        log.info("ğŸ‰ é€²éšå¤šéšæ®µè¨“ç·´å®Œæˆ!")


if __name__ == "__main__":
    advanced_ldc_trainer()
