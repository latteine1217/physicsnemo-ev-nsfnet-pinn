"""
PINNè¨“ç·´è…³æœ¬

åŸºæ–¼PhysicsNeMoæ¡†æ¶çš„PINNè¨“ç·´ï¼Œæ”¯æ´ï¼š
- å¤šéšæ®µè¨“ç·´
- åˆ†æ•£å¼è¨“ç·´
- æª¢æŸ¥é»ä¿å­˜/è¼‰å…¥
- è‡ªå‹•æ··åˆå„ªåŒ–ï¼ˆAdam + L-BFGSï¼‰
"""

import os
import sys
import argparse
import torch
import torch.distributed as dist
import numpy as np
from typing import Tuple

# æ·»åŠ srcè·¯å¾‘åˆ°Pythonè·¯å¾‘
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))

from src.solvers.pinn_solver import PINNSolver
from src.config.config_manager import ConfigManager
from src.utils.logger import LoggerFactory


class CavityDataGenerator:
    """
    Lid-driven cavityæ•¸æ“šç”Ÿæˆå™¨
    
    ç”ŸæˆPINNè¨“ç·´æ‰€éœ€çš„é‚Šç•Œæ¢ä»¶å’Œæ–¹ç¨‹é»
    """
    
    def __init__(self, N_f: int = 20000, N_b: int = 1000):
        self.N_f = N_f  # æ–¹ç¨‹é»æ•¸é‡
        self.N_b = N_b  # é‚Šç•Œé»æ•¸é‡
        self.x_min = -1.0
        self.x_max = 1.0
        self.y_min = -1.0
        self.y_max = 1.0
    
    def generate_boundary_data(self) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
        """
        ç”Ÿæˆé‚Šç•Œæ¢ä»¶æ•¸æ“š
        
        Returns:
            (x_b, y_b, u_b, v_b): é‚Šç•Œé»åº§æ¨™å’Œé‚Šç•Œæ¢ä»¶å€¼
        """
        Nx = 513
        Ny = 513
        r_const = 10
        
        # ä¸Šé‚Šç•Œçš„æ»‘å‹•è“‹å­é€Ÿåº¦åˆ†ä½ˆ
        upper_x = np.linspace(self.x_min, self.x_max, num=Nx)
        u_upper = 1 - np.cosh(r_const * (upper_x - 0.0)) / np.cosh(r_const * 1.0)
        
        # é‚Šç•Œé»åº§æ¨™: ä¸‹ã€ä¸Šã€å·¦ã€å³
        x_b = np.concatenate([
            np.linspace(self.x_min, self.x_max, num=Nx),  # ä¸‹é‚Šç•Œ
            np.linspace(self.x_min, self.x_max, num=Nx),  # ä¸Šé‚Šç•Œ
            self.x_min * np.ones([Ny]),                   # å·¦é‚Šç•Œ
            self.x_max * np.ones([Ny])                    # å³é‚Šç•Œ
        ], axis=0).reshape([-1, 1])
        
        y_b = np.concatenate([
            self.y_min * np.ones([Nx]),                   # ä¸‹é‚Šç•Œ
            self.y_max * np.ones([Nx]),                   # ä¸Šé‚Šç•Œ  
            np.linspace(self.y_min, self.y_max, num=Ny), # å·¦é‚Šç•Œ
            np.linspace(self.y_min, self.y_max, num=Ny)  # å³é‚Šç•Œ
        ], axis=0).reshape([-1, 1])
        
        # é‚Šç•Œæ¢ä»¶å€¼
        u_b = np.concatenate([
            np.zeros([Nx]),     # ä¸‹é‚Šç•Œ: u=0
            u_upper,            # ä¸Šé‚Šç•Œ: æ»‘å‹•è“‹å­
            np.zeros([Ny]),     # å·¦é‚Šç•Œ: u=0
            np.zeros([Ny])      # å³é‚Šç•Œ: u=0
        ], axis=0).reshape([-1, 1])
        
        v_b = np.zeros([x_b.shape[0]]).reshape([-1, 1])  # æ‰€æœ‰é‚Šç•Œ: v=0
        
        print(f"ç”Ÿæˆé‚Šç•Œé»æ•¸é‡: {x_b.shape[0]}")
        return x_b, y_b, u_b, v_b
    
    def generate_training_points(self) -> Tuple[np.ndarray, np.ndarray]:
        """
        ç”Ÿæˆè¨“ç·´æ–¹ç¨‹é»ï¼ˆä½¿ç”¨Latin Hypercube Samplingï¼‰
        
        Returns:
            (x_f, y_f): æ–¹ç¨‹é»åº§æ¨™
        """
        # ç°¡åŒ–ç‰ˆLHSæ¡æ¨£
        np.random.seed(42)  # ç¢ºä¿å¯é‡ç¾æ€§
        
        x_f = np.random.uniform(self.x_min, self.x_max, (self.N_f, 1))
        y_f = np.random.uniform(self.y_min, self.y_max, (self.N_f, 1))
        
        print(f"ç”Ÿæˆæ–¹ç¨‹é»æ•¸é‡: {self.N_f}")
        return x_f, y_f


def setup_distributed_training():
    """è¨­å®šåˆ†æ•£å¼è¨“ç·´ç’°å¢ƒ"""
    if 'RANK' in os.environ:
        # ä½¿ç”¨torchrunå•Ÿå‹•çš„åˆ†æ•£å¼è¨“ç·´
        rank = int(os.environ['RANK'])
        local_rank = int(os.environ['LOCAL_RANK'])
        world_size = int(os.environ['WORLD_SIZE'])
        
        # åˆå§‹åŒ–process group
        dist.init_process_group(backend='nccl', rank=rank, world_size=world_size)
        
        print(f"åˆ†æ•£å¼è¨“ç·´åˆå§‹åŒ–: rank={rank}, local_rank={local_rank}, world_size={world_size}")
        return True
    else:
        # å–®GPU/CPUè¨“ç·´
        os.environ['RANK'] = '0'
        os.environ['LOCAL_RANK'] = '0'
        os.environ['WORLD_SIZE'] = '1'
        return False


def create_scheduler(optimizer, config, num_epochs: int):
    """
    å‰µå»ºå­¸ç¿’ç‡èª¿åº¦å™¨
    
    Args:
        optimizer: å„ªåŒ–å™¨
        config: é…ç½®å°è±¡
        num_epochs: ç¸½epochæ•¸
        
    Returns:
        å­¸ç¿’ç‡èª¿åº¦å™¨
    """
    scheduler_type = getattr(config.training, 'scheduler_type', 'constant').lower()
    
    if scheduler_type == 'cosine':
        eta_min = getattr(config.training, 'eta_min', 1e-6)
        return torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer, T_max=num_epochs, eta_min=eta_min
        )
    elif scheduler_type == 'multistep':
        milestones = getattr(config.training, 'milestones', [num_epochs // 2])
        gamma = getattr(config.training, 'gamma', 0.1)
        return torch.optim.lr_scheduler.MultiStepLR(
            optimizer, milestones=milestones, gamma=gamma
        )
    else:
        # å¸¸æ•¸å­¸ç¿’ç‡
        return torch.optim.lr_scheduler.ConstantLR(optimizer, factor=1.0)


def train_single_stage(solver: PINNSolver, num_epochs: int, stage_name: str,
                      save_dir: str, start_epoch: int = 0) -> float:
    """
    å–®éšæ®µè¨“ç·´
    
    Args:
        solver: PINNæ±‚è§£å™¨
        num_epochs: è¨“ç·´epochæ•¸
        stage_name: éšæ®µåç¨±
        save_dir: ä¿å­˜ç›®éŒ„
        start_epoch: èµ·å§‹epoch
        
    Returns:
        æœ€çµ‚æå¤±å€¼
    """
    solver.current_stage = stage_name
    logger = solver.logger
    
    logger.info(f"=== é–‹å§‹{stage_name}éšæ®µè¨“ç·´ ===")
    logger.info(f"è¨“ç·´epochs: {num_epochs}, èµ·å§‹epoch: {start_epoch}")
    
    # å‰µå»ºå­¸ç¿’ç‡èª¿åº¦å™¨
    scheduler = create_scheduler(solver.opt, solver.config, num_epochs)
    
    best_loss = float('inf')
    
    for epoch in range(start_epoch, num_epochs):
        # è¨“ç·´ä¸€å€‹epoch
        loss_value, losses = solver.train_epoch()
        
        # å­¸ç¿’ç‡èª¿åº¦
        scheduler.step()
        
        # è¨˜éŒ„æœ€ä½³æå¤±
        if loss_value < best_loss:
            best_loss = loss_value
        
        # è¨˜éŒ„TensorBoard
        solver.log_tensorboard(epoch, loss_value, losses)
        
        # æ‰“å°è¨“ç·´æ—¥èªŒ
        if epoch % 100 == 0 or epoch == num_epochs - 1:
            current_lr = solver.opt.param_groups[0]['lr']
            logger.info(f"Epoch {epoch+1}/{num_epochs} | "
                       f"Loss: {loss_value:.3e} | "
                       f"Best: {best_loss:.3e} | "
                       f"LR: {current_lr:.2e}")
            
            # è©³ç´°æå¤±ä¿¡æ¯
            if epoch % 500 == 0:
                logger.info(f"  æ–¹ç¨‹æå¤±: {losses[0]:.3e}")
                logger.info(f"  é‚Šç•Œæå¤±: {losses[1]:.3e}")
                logger.info(f"  NS-Xæå¤±: {losses[3]:.3e}")
                logger.info(f"  NS-Yæå¤±: {losses[4]:.3e}")
                logger.info(f"  é€£çºŒæ€§æå¤±: {losses[5]:.3e}")
                logger.info(f"  Entropyæå¤±: {losses[6]:.3e}")
        
        # ä¿å­˜æª¢æŸ¥é»
        if (epoch + 1) % solver.checkpoint_freq == 0:
            checkpoint_dir = os.path.join(save_dir, stage_name)
            solver.save_checkpoint(epoch, checkpoint_dir)
    
    logger.info(f"=== {stage_name}éšæ®µå®Œæˆ | æœ€ä½³æå¤±: {best_loss:.3e} ===")
    return best_loss


def main():
    parser = argparse.ArgumentParser(description='PINN Training Script')
    parser.add_argument('--config', type=str, default='configs/production.yaml',
                       help='é…ç½®æª”æ¡ˆè·¯å¾‘')
    parser.add_argument('--checkpoint', type=str, default=None,
                       help='æª¢æŸ¥é»è·¯å¾‘ï¼ˆå¯é¸ï¼‰')
    parser.add_argument('--save_dir', type=str, default='results',
                       help='çµæœä¿å­˜ç›®éŒ„')
    
    args = parser.parse_args()
    
    # è¨­å®šåˆ†æ•£å¼è¨“ç·´
    is_distributed = setup_distributed_training()
    rank = int(os.environ.get('RANK', 0))
    
    # åˆå§‹åŒ–PINNæ±‚è§£å™¨
    solver = PINNSolver(args.config)
    
    # ç”Ÿæˆè¨“ç·´æ•¸æ“š
    data_generator = CavityDataGenerator(
        N_f=solver.config.training.N_f,
        N_b=getattr(solver.config.training, 'N_b', 1000)
    )
    
    # é‚Šç•Œæ¢ä»¶æ•¸æ“š
    x_b, y_b, u_b, v_b = data_generator.generate_boundary_data()
    
    # æ–¹ç¨‹é»æ•¸æ“š
    x_f, y_f = data_generator.generate_training_points()
    
    # è½‰æ›ç‚ºtorchå¼µé‡ä¸¦è¼‰å…¥åˆ°æ±‚è§£å™¨
    solver.load_training_data(
        x_f=torch.tensor(x_f, dtype=torch.float32),
        y_f=torch.tensor(y_f, dtype=torch.float32),
        x_b=torch.tensor(x_b, dtype=torch.float32),
        y_b=torch.tensor(y_b, dtype=torch.float32),
        u_b=torch.tensor(u_b, dtype=torch.float32),
        v_b=torch.tensor(v_b, dtype=torch.float32)
    )
    
    # è¨­å®šå„ªåŒ–å™¨
    learning_rate = getattr(solver.config.training, 'learning_rate', 1e-3)
    weight_decay = getattr(solver.config.training, 'weight_decay', 0.0)
    solver.setup_optimizer(learning_rate, weight_decay)
    
    # å‰µå»ºä¿å­˜ç›®éŒ„
    save_dir = os.path.join(args.save_dir, f"Re{solver.Re}")
    os.makedirs(save_dir, exist_ok=True)
    
    # è¨“ç·´éšæ®µé…ç½®
    training_stages = getattr(solver.config.training, 'training_stages', None)
    
    if training_stages:
        # å¤šéšæ®µè¨“ç·´
        for i, (alpha_evm, epochs, lr, scheduler_type) in enumerate(training_stages):
            stage_name = f"Stage{i+1}"
            
            # æ›´æ–°EVMåƒæ•¸
            solver.update_evm_parameters(alpha_evm)
            
            # æ›´æ–°å­¸ç¿’ç‡
            for group in solver.opt.param_groups:
                group['lr'] = lr
            
            # åŸ·è¡Œéšæ®µè¨“ç·´
            best_loss = train_single_stage(
                solver, epochs, stage_name, save_dir
            )
            
            if rank == 0:
                solver.logger.info(f"{stage_name}å®Œæˆ: alpha_evm={alpha_evm}, "
                                  f"lr={lr}, loss={best_loss:.3e}")
    else:
        # å–®éšæ®µè¨“ç·´
        num_epochs = getattr(solver.config.training, 'num_epochs', 10000)
        train_single_stage(solver, num_epochs, "Training", save_dir)
    
    # æœ€çµ‚ä¿å­˜
    if rank == 0:
        final_checkpoint_dir = os.path.join(save_dir, "final")
        solver.save_checkpoint(num_epochs - 1, final_checkpoint_dir)
        solver.logger.info("ğŸ‰ è¨“ç·´å®Œæˆï¼")
    
    # æ¸…ç†åˆ†æ•£å¼è¨“ç·´
    if is_distributed:
        dist.destroy_process_group()


if __name__ == "__main__":
    main()