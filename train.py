#!/usr/bin/env python3
"""
ä¸»è¨“ç·´è…³æœ¬ - ä½¿ç”¨æ¨¡çµ„åŒ–æ¶æ§‹

åŸºæ–¼ev-NSFnet/train.pyï¼Œæ¡ç”¨æ¨¡çµ„åŒ–è¨­è¨ˆ
æ”¯æ´åˆ†ä½ˆå¼è¨“ç·´ã€é…ç½®ç®¡ç†ã€ç›£ç£å­¸ç¿’ç­‰åŠŸèƒ½
"""

import os
import sys
import time
import argparse
import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

# æ·»åŠ srcè·¯å¾‘
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '.'))

from src.config.config_manager import ConfigManager
from src.data.cavity_data import DataLoader
from src.solvers.pinn_solver import PINNSolver
from src.utils.device_utils import setup_device, get_cuda_info
from src.utils.logger import LoggerFactory

# å•Ÿç”¨CUDAå„ªåŒ–
torch.backends.cudnn.benchmark = True


def parse_args():
    """è§£æå‘½ä»¤è¡Œåƒæ•¸"""
    parser = argparse.ArgumentParser(description='PINN Training with Configuration Management')
    parser.add_argument('--config', type=str, default='configs/production.yaml',
                       help='é…ç½®æ–‡ä»¶è·¯å¾‘ (default: configs/production.yaml)')
    parser.add_argument('--resume', type=str, default=None,
                       help='å¾æª¢æŸ¥é»æ¢å¾©è¨“ç·´')
    parser.add_argument('--dry-run', action='store_true',
                       help='åªé¡¯ç¤ºé…ç½®ä¸åŸ·è¡Œè¨“ç·´')
    return parser.parse_args()


def setup_distributed():
    """è¨­ç½®åˆ†ä½ˆå¼è¨“ç·´ç’°å¢ƒ"""
    # æª¢æŸ¥åˆ†å¸ƒå¼ç’°å¢ƒè®Šæ•¸
    if 'RANK' not in os.environ:
        print("ğŸ’» å–®GPUæ¨¡å¼")
        os.environ['RANK'] = '0'
        os.environ['LOCAL_RANK'] = '0'
        os.environ['WORLD_SIZE'] = '1'
        return False  # éåˆ†å¸ƒå¼æ¨¡å¼
    
    # åˆå§‹åŒ–åˆ†å¸ƒå¼é€²ç¨‹çµ„
    try:
        if not dist.is_initialized():
            # åªæœ‰åœ¨ rank 0 æ™‚æ‰é¡¯ç¤ºåˆå§‹åŒ–ä¿¡æ¯
            rank = int(os.environ.get('RANK', 0))
            if rank == 0:
                print("ğŸ”— åˆå§‹åŒ–åˆ†å¸ƒå¼è¨“ç·´...")
                
            dist.init_process_group(backend='nccl')
            
            rank = dist.get_rank()
            world_size = dist.get_world_size()
            local_rank = int(os.environ['LOCAL_RANK'])
            
            # åªæœ‰ä¸»é€²ç¨‹é¡¯ç¤ºåˆ†å¸ƒå¼ä¿¡æ¯
            if rank == 0:
                print(f"ğŸ“¡ åˆ†å¸ƒå¼è¨“ç·´è¨­ç½®å®Œæˆ: {world_size} GPUs")
                print(f"   - Backend: NCCL")
                print(f"   - æ¯å€‹é€²ç¨‹è² è²¬ GPU {local_rank}")
            
            # ä½¿ç”¨çµ±ä¸€è¨­å‚™ç®¡ç†å‡½æ•¸
            device = setup_device(local_rank)
                
        return True  # åˆ†å¸ƒå¼æ¨¡å¼
        
    except Exception as e:
        rank = int(os.environ.get('RANK', 0))
        if rank == 0:
            print(f"âŒ åˆ†å¸ƒå¼åˆå§‹åŒ–å¤±æ•—: {e}")
            print("ğŸ’» é€€å›å–®GPUæ¨¡å¼")
        os.environ['RANK'] = '0'
        os.environ['LOCAL_RANK'] = '0'
        os.environ['WORLD_SIZE'] = '1'
        return False


def cleanup_distributed():
    """æ¸…ç†åˆ†å¸ƒå¼è¨“ç·´ç’°å¢ƒ"""
    if dist.is_initialized():
        dist.destroy_process_group()


def display_supervision_setup(config_manager, rank=0):
    """é¡¯ç¤ºç›£ç£è¨­ç½®å’Œç›£ç£é»ä½ç½®"""
    if rank != 0:  # åªåœ¨ä¸»é€²ç¨‹é¡¯ç¤º
        return
        
    supervision_config = getattr(config_manager.config, 'supervision', None)
    if not supervision_config:
        return
    
    print("ğŸ¯ ç›£ç£æ•¸æ“šé…ç½®:")
    print(f"   å•Ÿç”¨ç‹€æ…‹: {'âœ… å•Ÿç”¨' if supervision_config.enabled else 'âŒ é—œé–‰'}")
    print(f"   ç›£ç£é»æ•¸: {supervision_config.data_points}")
    print(f"   æ•¸æ“šæ¬Šé‡: {supervision_config.weight}")
    print(f"   éš¨æ©Ÿç¨®å­: {supervision_config.random_seed}")
    print(f"   æ•¸æ“šè·¯å¾‘: {supervision_config.data_path}")
    
    if supervision_config.enabled and supervision_config.data_points > 0:
        print("\n" + "="*50)
        print("ğŸ” ç›£ç£é»ä½ç½®è©³ç´°ä¿¡æ¯ï¼š")
        print("="*50)
        
        # æª¢æŸ¥æ•¸æ“šæ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(supervision_config.data_path):
            print(f"âš ï¸  æ•¸æ“šæ–‡ä»¶ä¸å­˜åœ¨: {supervision_config.data_path}")
            return
            
        try:
            loader = DataLoader()
            loader.print_supervision_locations(
                supervision_config.data_path,
                supervision_config.data_points, 
                supervision_config.random_seed
            )
        except Exception as e:
            print(f"âš ï¸  è¼‰å…¥ç›£ç£é»ä½ç½®æ™‚å‡ºéŒ¯: {e}")


def load_training_data(config, dataloader, pinn_solver, rank=0):
    """è¼‰å…¥è¨“ç·´è³‡æ–™ä¸¦åˆ†é…çµ¦å„å€‹é€²ç¨‹"""
    if rank == 0:
        print("ğŸ“ è¼‰å…¥è¨“ç·´è³‡æ–™...")
    
    # è¨­ç½®é‚Šç•Œè³‡æ–™
    boundary_data = dataloader.loading_boundary_data()
    xb_cpu = torch.as_tensor(boundary_data[0], dtype=torch.float32).contiguous()
    yb_cpu = torch.as_tensor(boundary_data[1], dtype=torch.float32).contiguous()
    ub_cpu = torch.as_tensor(boundary_data[2], dtype=torch.float32).contiguous()
    vb_cpu = torch.as_tensor(boundary_data[3], dtype=torch.float32).contiguous()
    
    # åˆ†ä½ˆå¼è³‡æ–™åˆ†é…ï¼ˆé‚Šç•Œè³‡æ–™ï¼‰
    world_size = int(os.environ.get('WORLD_SIZE', '1'))
    total_b = xb_cpu.shape[0]
    
    if total_b < world_size:
        if rank < total_b:
            b_start, b_end = rank, rank + 1
        else:
            b_start, b_end = 0, 0
    else:
        per = total_b // world_size
        b_start = rank * per
        b_end = b_start + per if rank < world_size - 1 else total_b
    
    # åˆ†ä½ˆå¼åŒæ­¥é‚Šç•Œç´¢å¼•
    if dist.is_initialized():
        idx = [b_start, b_end]
        dist.broadcast_object_list(idx, src=0)
        b_start, b_end = idx
    
    # ç§»è‡³è¨­å‚™
    xb = xb_cpu[b_start:b_end].to(pinn_solver.device)
    yb = yb_cpu[b_start:b_end].to(pinn_solver.device)
    ub = ub_cpu[b_start:b_end].to(pinn_solver.device)
    vb = vb_cpu[b_start:b_end].to(pinn_solver.device)
    
    pinn_solver.set_boundary_data(X=(xb, yb, ub, vb))
    
    # è¨­ç½®æ–¹ç¨‹å¼è¨“ç·´è³‡æ–™
    eq_data = dataloader.loading_training_data()
    xf_cpu = torch.as_tensor(eq_data[0], dtype=torch.float32).contiguous()
    yf_cpu = torch.as_tensor(eq_data[1], dtype=torch.float32).contiguous()
    
    # åˆ†ä½ˆå¼è³‡æ–™åˆ†é…ï¼ˆæ–¹ç¨‹å¼è³‡æ–™ï¼‰
    total_f = xf_cpu.shape[0]
    
    if total_f < world_size:
        if rank < total_f:
            f_start, f_end = rank, rank + 1
        else:
            f_start, f_end = 0, 1
    else:
        per_f = total_f // world_size
        f_start = rank * per_f
        f_end = f_start + per_f if rank < world_size - 1 else total_f
    
    # åˆ†ä½ˆå¼åŒæ­¥æ–¹ç¨‹å¼ç´¢å¼•
    if dist.is_initialized():
        idxf = [f_start, f_end]
        dist.broadcast_object_list(idxf, src=0)
        f_start, f_end = idxf
    
    # ç§»è‡³è¨­å‚™ï¼ˆéœ€è¦æ¢¯åº¦ï¼‰
    xf = xf_cpu[f_start:f_end].to(pinn_solver.device).contiguous().requires_grad_(True)
    yf = yf_cpu[f_start:f_end].to(pinn_solver.device).contiguous().requires_grad_(True)
    
    pinn_solver.set_eq_training_data(X=(xf, yf))
    
    return True


def load_supervision_data(config, dataloader, pinn_solver, rank=0):
    """è¼‰å…¥ç›£ç£è³‡æ–™"""
    supervision_config = getattr(config, 'supervision', None)
    
    if not supervision_config or not supervision_config.enabled or supervision_config.data_points <= 0:
        if rank == 0:
            print("ğŸ“Š æœªå•Ÿç”¨ç›£ç£è³‡æ–™æˆ–è³‡æ–™é»æ•¸é‡ç‚º0")
        return False
    
    if rank == 0:
        print(f"ğŸ“Š è¼‰å…¥ç›£ç£è³‡æ–™: {supervision_config.data_points} å€‹è³‡æ–™é»...")
    
    try:
        # è¼‰å…¥ç›£ç£è³‡æ–™
        x_sup, y_sup, u_sup, v_sup, p_sup = dataloader.loading_supervision_data(
            supervision_config.data_path, 
            supervision_config.data_points,
            supervision_config.random_seed
        )
        
        # è½‰æ›ç‚ºtensorä¸¦ç§»åˆ°GPU
        if x_sup.shape[0] > 0:  # ç¢ºä¿æœ‰è³‡æ–™é»
            x_sup_tensor = torch.as_tensor(x_sup, dtype=torch.float32).to(pinn_solver.device)
            y_sup_tensor = torch.as_tensor(y_sup, dtype=torch.float32).to(pinn_solver.device)
            u_sup_tensor = torch.as_tensor(u_sup, dtype=torch.float32).to(pinn_solver.device)
            v_sup_tensor = torch.as_tensor(v_sup, dtype=torch.float32).to(pinn_solver.device)
            p_sup_tensor = torch.as_tensor(p_sup, dtype=torch.float32).to(pinn_solver.device)
            
            # è¨­ç½®ç›£ç£è³‡æ–™åˆ°PINN
            pinn_solver.set_supervision_data(
                x_sup_tensor, y_sup_tensor, 
                u_sup_tensor, v_sup_tensor, p_sup_tensor
            )
            
            if rank == 0:
                print(f"âœ… ç›£ç£è³‡æ–™è¼‰å…¥å®Œæˆï¼Œç›£ç£é»åº§æ¨™: ({x_sup[0,0]:.4f}, {y_sup[0,0]:.4f})")
            
            return True
        
    except Exception as e:
        if rank == 0:
            print(f"âŒ ç›£ç£è³‡æ–™è¼‰å…¥å¤±æ•—: {e}")
    
    return False


def execute_training_stages(config, pinn_solver, is_distributed, start_epoch=0, rank=0):
    """åŸ·è¡Œåˆ†éšæ®µè¨“ç·´"""
    # ä½¿ç”¨é…ç½®ä¸­çš„è¨“ç·´éšæ®µ
    training_stages = []
    for i, stage in enumerate(config.training.training_stages):
        alpha, epochs, lr = stage[0], stage[1], stage[2]
        sched = stage[3] if len(stage) > 3 else 'Constant'
        stage_name = f"Stage {i+1}"
        training_stages.append((alpha, epochs, lr, sched, stage_name))
    
    total_epochs = sum([stage[1] for stage in training_stages])
    
    if not is_distributed or rank == 0:
        print(f"ğŸš€ é–‹å§‹å®Œæ•´è¨“ç·´ï¼šç¸½å…± {total_epochs:,} epochsï¼Œåˆ† {len(training_stages)} å€‹éšæ®µ")
        print("=" * 60)
    
    # åŸ·è¡Œåˆ†éšæ®µè¨“ç·´
    current_epoch = start_epoch
    
    for stage_idx, (alpha_evm, num_epochs, learning_rate, sched_name, stage_name) in enumerate(training_stages):
        # è·³éå·²å®Œæˆçš„epochs
        if current_epoch >= num_epochs:
            if rank == 0:
                print(f"â­ï¸ è·³é {stage_name} (å·²å®Œæˆ)")
            current_epoch -= num_epochs
            continue
        
        epochs_to_run = num_epochs - current_epoch
        
        if not is_distributed or rank == 0:
            print(f"ğŸ”„ {stage_name}: alpha_evm={alpha_evm}, epochs={epochs_to_run}/{num_epochs}, lr={learning_rate:.2e}")
        
        # è¨­ç½®éšæ®µåƒæ•¸
        pinn_solver.current_stage = stage_name
        pinn_solver.set_alpha_evm(alpha_evm)
        
        # é‡å»ºå„ªåŒ–å™¨
        weight_decay = getattr(config.training, 'weight_decay', 0.0)
        if hasattr(config.training, 'weight_decay_stages') and config.training.weight_decay_stages:
            weight_decay = config.training.weight_decay_stages[stage_idx]
        
        pinn_solver.build_optimizer(learning_rate, weight_decay)
        
        if rank == 0:
            print(f"   - Optimizer: AdamW (lr={learning_rate:.2e}, wd={weight_decay})")
        
        # åŸ·è¡Œè©²éšæ®µçš„è¨“ç·´
        try:
            pinn_solver.train_stage(
                epochs=epochs_to_run,
                stage_name=stage_name,
                scheduler=sched_name
            )
        except Exception as e:
            if rank == 0:
                print(f"âŒ è¨“ç·´éšæ®µ {stage_name} ç™¼ç”ŸéŒ¯èª¤: {e}")
            raise
        
        current_epoch = 0  # ä¸‹ä¸€éšæ®µå¾0é–‹å§‹


def main():
    """ä¸»è¨“ç·´å‡½æ•¸"""
    args = parse_args()
    
    # è¨­ç½®åˆ†ä½ˆå¼ç’°å¢ƒ
    is_distributed = setup_distributed()
    
    # ç²å–ç•¶å‰é€²ç¨‹çš„rank
    rank = int(os.environ.get('RANK', 0))
    
    try:
        # è¼‰å…¥é…ç½®
        if rank == 0:
            print(f"ğŸ“‚ è¼‰å…¥é…ç½®æ–‡ä»¶: {args.config}")
        
        config_manager = ConfigManager.from_file(args.config)
        config = config_manager.config
        
        # åªåœ¨ä¸»é€²ç¨‹é¡¯ç¤ºé…ç½®ä¿¡æ¯
        if rank == 0:
            warnings = config_manager.validate_config()
            if warnings:
                print("âš ï¸  é…ç½®è­¦å‘Š:")
                for warning in warnings:
                    print(f"   - {warning}")
            
            config_manager.print_config()
            display_supervision_setup(config_manager, rank)
            
            if args.dry_run:
                print("ğŸƒ Dry runæ¨¡å¼ï¼Œä¸åŸ·è¡Œè¨“ç·´")
                return
        
        # Dry runæª¢æŸ¥
        if args.dry_run:
            return
        
        # å•Ÿç”¨ç•°å¸¸æª¢æ¸¬ï¼ˆèª¿è©¦ç”¨ï¼‰
        torch.autograd.set_detect_anomaly(False)
        
        # å‰µå»ºPINNæ±‚è§£å™¨
        if rank == 0:
            print("ğŸš€ å‰µå»ºPINNæ±‚è§£å™¨...")
        
        pinn_solver = PINNSolver(config)
        
        # å‰µå»ºè³‡æ–™è¼‰å…¥å™¨
        if rank == 0:
            print("ğŸ“ å‰µå»ºè³‡æ–™è¼‰å…¥å™¨...")
        
        dataloader = DataLoader(
            path='./data/',
            N_f=config.training.N_f,
            N_b=1000,
            sort_by_boundary_distance=getattr(config.training, 'sort_by_boundary_distance', True)
        )
        
        # è¼‰å…¥è¨“ç·´è³‡æ–™
        load_training_data(config, dataloader, pinn_solver, rank)
        
        # è¼‰å…¥ç›£ç£è³‡æ–™
        load_supervision_data(config, dataloader, pinn_solver, rank)
        
        # è¼‰å…¥è©•ä¼°è³‡æ–™
        filename = f'./data/cavity_Re{config.physics.Re}_256_Uniform.mat'
        if os.path.exists(filename):
            eval_data = dataloader.loading_evaluate_data(filename)
            pinn_solver.set_evaluation_data(*eval_data)
            if rank == 0:
                print(f"âœ… è¼‰å…¥è©•ä¼°è³‡æ–™: {filename}")
        elif rank == 0:
            print(f"âš ï¸  è©•ä¼°è³‡æ–™ä¸å­˜åœ¨: {filename}")
        
        # æ¢å¾©è¨“ç·´ç‹€æ…‹
        start_epoch = 0
        if args.resume:
            if rank == 0:
                print(f"ğŸ”„ æ­£åœ¨å¾æª¢æŸ¥é»æ¢å¾©: {args.resume}")
            start_epoch = pinn_solver.load_checkpoint(args.resume)
            if rank == 0:
                if start_epoch > 0:
                    print(f"âœ… æˆåŠŸæ¢å¾©ï¼Œå°‡å¾ epoch {start_epoch} é–‹å§‹")
                else:
                    print("âš ï¸ ç„¡æ³•è¼‰å…¥æª¢æŸ¥é»ï¼Œå°‡å¾é ­é–‹å§‹è¨“ç·´")
        
        # åŸ·è¡Œåˆ†éšæ®µè¨“ç·´
        execute_training_stages(config, pinn_solver, is_distributed, start_epoch, rank)
        
        if rank == 0:
            print("ğŸ‰ è¨“ç·´å®Œæˆï¼")
        
    except Exception as e:
        if rank == 0:
            print(f"âŒ è¨“ç·´éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
            import traceback
            traceback.print_exc()
        raise
    
    finally:
        # æ¸…ç†åˆ†ä½ˆå¼ç’°å¢ƒ
        if is_distributed:
            cleanup_distributed()


if __name__ == "__main__":
    main()